{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Burden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from SCMP import SCMP\n",
    "import DataGeneration as data\n",
    "from CommonFunctions import *\n",
    "from SocialMeasures import Burden\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "PATH = \"./Results/burden\"\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit dataset\n",
    "X, Y = data.load_credit_default_data()\n",
    "X, Y = X[:3000], Y[:3000]\n",
    "X, Y, Xval, Yval, Xtest, Ytest = data.split_validation_test(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = len(X[0])\n",
    "scale = 1 # 1 / x_dim\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "lambda_range = np.logspace(start=-2, stop=0.3, num=50)\n",
    "print(lambda_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"lambdas\": [],\n",
    "    \"accuracies\": [],\n",
    "    \"burdens\": []\n",
    "}\n",
    "\n",
    "print(f\"---------- Training with lambda={0} ----------\")\n",
    "model_name = f\"burden_{0:.3f}\"\n",
    "burden = Burden(0, x_dim, SCMP.linear_score, quad_cost_cvxpy_not_batched, quad_cost_torch, cost_const_kwargs={\"scale\": scale})  # Create burden from outside since reg=0.\n",
    "benchmark_model = SCMP(x_dim, batch_size, cost_fn=\"quad\", cost_const_kwargs={\"scale\": scale}, \n",
    "                       social_measure_dict={\"burden\": burden}, strategic=True)\n",
    "benchmark_model.fit(X, Y, Xval, Yval, opt_class=torch.optim.Adam, opt_kwargs={\"lr\": 5e-2}, epochs=epochs,\n",
    "                    verbose=\"batches\", path=PATH, model_name=model_name)\n",
    "\n",
    "benchmark_model = SCMP(x_dim, batch_size, cost_fn=\"quad\", cost_const_kwargs={\"scale\": scale}, \n",
    "                       social_measure_dict={\"burden\": burden}, strategic=True)\n",
    "benchmark_model.load_model(PATH, model_name)\n",
    "\n",
    "results[\"lambdas\"].append(0)\n",
    "accuracy = benchmark_model.evaluate(Xtest, Ytest, strategic_data=True)\n",
    "results[\"accuracies\"].append(accuracy)\n",
    "burden = benchmark_model.social_measure_dict[\"burden\"].calc_burden(Xtest, Ytest, benchmark_model.w, benchmark_model.b, requires_grad=False).item()\n",
    "results[\"burdens\"].append(burden)\n",
    "pd.DataFrame(results).to_csv(f\"{PATH}/results.csv\")\n",
    "\n",
    "for lamb in lambda_range:\n",
    "    print(f\"---------- Training with lambda={lamb} ----------\")\n",
    "    model_name = f\"burden_{lamb:.3f}\"\n",
    "    model = SCMP(x_dim, batch_size, cost_fn=\"quad\", cost_const_kwargs={\"scale\": scale}, burden_reg=lamb, strategic=True)\n",
    "    model.fit(X, Y, Xval, Yval, opt_class=torch.optim.Adam, opt_kwargs={\"lr\": 5e-2},\n",
    "                        epochs=epochs, verbose=\"batches\", path=PATH, model_name=model_name)\n",
    "    \n",
    "    model = SCMP(x_dim, batch_size, cost_fn=\"quad\", cost_const_kwargs={\"scale\": scale}, burden_reg=lamb, strategic=True)\n",
    "    model.load_model(PATH, model_name)\n",
    "    \n",
    "    results[\"lambdas\"].append(lamb)\n",
    "    accuracy = model.evaluate(Xtest, Ytest, strategic_data=True)\n",
    "    results[\"accuracies\"].append(accuracy)\n",
    "    burden = model.social_measure_dict[\"burden\"].calc_burden(Xtest, Ytest, model.w, model.b, requires_grad=False).item()\n",
    "    results[\"burdens\"].append(burden)\n",
    "    pd.DataFrame(results).to_csv(f\"{PATH}/results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"Results/burden/results.csv\")\n",
    "accuracies = results[\"accuracies\"].to_list()\n",
    "burdens = results[\"burdens\"].to_list()\n",
    "benchmark = accuracies[0]\n",
    "# accuracies, burdens = accuracies[1:], burdens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(24, 6))\n",
    "\n",
    "_ = ax.set_xlabel(\"social burden (decreasing)\")\n",
    "_ = ax.set_xlim([2.4, 0])\n",
    "_ = ax.set_ylabel(\"accuracy\")\n",
    "_ = ax.set_title(\"Social burden tradeoff\")\n",
    "_ = ax.axhline(y=benchmark, linewidth=3, linestyle=\"--\", color=\"gray\", label=\"Benchmark\")\n",
    "_ = ax.plot(burdens, accuracies, 'o',  label=\"SERM\")\n",
    "_ = ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
