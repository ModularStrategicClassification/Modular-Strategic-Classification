{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from SCMP import SCMP\n",
    "import DataGeneration as data\n",
    "from CommonFunctions import *\n",
    "from SocialMeasures import Utility\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "PATH = \"./Results/utility\"\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit dataset\n",
    "X, Y = data.load_credit_default_data()\n",
    "X, Y = X[:3000], Y[:3000]\n",
    "X, Y, Xval, Yval, Xtest, Ytest = data.split_validation_test(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = len(X[0])\n",
    "scale = 1 # 1 / x_dim\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "lambda_range = np.arange(0.05, 1.201, 0.05)\n",
    "print(lambda_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"lambdas\": [],\n",
    "    \"accuracies\": [],\n",
    "    \"utilities\": []\n",
    "}\n",
    "\n",
    "print(f\"---------- Training with lambda={0} ----------\")\n",
    "model_name = f\"utility_{0:.3f}\"\n",
    "utility = Utility(0, quad_cost_torch, cost_const_kwargs={\"scale\": scale})  # Create utility from outside since reg=0.\n",
    "benchmark_model = SCMP(x_dim, batch_size, cost_fn=\"quad\", cost_const_kwargs={\"scale\": scale}, \n",
    "                       social_measure_dict={\"utility\": utility}, strategic=True)\n",
    "benchmark_model.fit(X, Y, Xval, Yval, opt_class=torch.optim.Adam, opt_kwargs={\"lr\": 5e-2}, epochs=epochs,\n",
    "                    verbose=\"batches\", path=PATH, model_name=model_name)\n",
    "\n",
    "benchmark_model = SCMP(x_dim, batch_size, cost_fn=\"quad\", cost_const_kwargs={\"scale\": scale}, \n",
    "                       social_measure_dict={\"utility\": utility}, strategic=True)\n",
    "benchmark_model.load_model(PATH, model_name)\n",
    "\n",
    "results[\"lambdas\"].append(0)\n",
    "accuracy = benchmark_model.evaluate(Xtest, Ytest, strategic_data=True)\n",
    "results[\"accuracies\"].append(accuracy)\n",
    "Xtest_opt, Ytest_pred = benchmark_model.forward(Xtest, requires_grad=False)\n",
    "utility = benchmark_model.social_measure_dict[\"utility\"].calc_utility(Xtest, Xtest_opt, Ytest_pred, requires_grad=False).item()\n",
    "results[\"utilities\"].append(utility)\n",
    "pd.DataFrame(results).to_csv(f\"{PATH}/results.csv\")\n",
    "\n",
    "for lamb in lambda_range:\n",
    "    print(f\"---------- Training with lambda={lamb} ----------\")\n",
    "    model_name = f\"utility_{lamb:.3f}\"\n",
    "    model = SCMP(x_dim, batch_size, cost_fn=\"quad\", cost_const_kwargs={\"scale\": scale}, utility_reg=lamb, strategic=True)\n",
    "    model.fit(X, Y, Xval, Yval, opt_class=torch.optim.Adam, opt_kwargs={\"lr\": 5e-2}, epochs=epochs,\n",
    "              verbose=\"batches\", path=PATH, model_name=model_name)\n",
    "    \n",
    "    model = SCMP(x_dim, batch_size, cost_fn=\"quad\", cost_const_kwargs={\"scale\": scale}, utility_reg=lamb, strategic=True)\n",
    "    model.load_model(PATH, model_name)\n",
    "    \n",
    "    # Calculate and save results.\n",
    "    results[\"lambdas\"].append(lamb)\n",
    "    accuracy = model.evaluate(Xtest, Ytest, strategic_data=True)\n",
    "    results[\"accuracies\"].append(accuracy)\n",
    "    Xtest_opt, Ytest_pred = model.forward(Xtest, requires_grad=False)\n",
    "    utility = model.social_measure_dict[\"utility\"].calc_utility(Xtest, Xtest_opt, Ytest_pred, requires_grad=False).item()\n",
    "    results[\"utilities\"].append(utility)\n",
    "    pd.DataFrame(results).to_csv(f\"{PATH}/results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"Results/utility/results.csv\")\n",
    "accuracies = results[\"accuracies\"].to_list()\n",
    "utilities = results[\"utilities\"].to_list()\n",
    "benchmark = accuracies[0]\n",
    "# accuracies, utilities = accuracies[1:], utilities[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(24, 6))\n",
    "\n",
    "_ = ax.set_xlabel(\"utility\")\n",
    "_ = ax.set_xlim([0, 1])\n",
    "_ = ax.set_ylabel(\"accuracy\")\n",
    "_ = ax.set_title(\"Expected utility tradeoff\")\n",
    "_ = ax.axhline(y=benchmark, linewidth=3, linestyle=\"--\", color=\"gray\", label=\"Benchmark\")\n",
    "_ = ax.plot(utilities, accuracies, 'o',  label=\"SERM\")\n",
    "_ = ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
