{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla vs Hardt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "\n",
    "from lib.StrategicModel import StrategicModel\n",
    "import DataGeneration as data\n",
    "#import Presentation\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "PATH = \"./Results/vanilla_vs_hardt_orig\"\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data.load_spam_data()\n",
    "X, Y, Xval, Yval, Xtest, Ytest = data.split_validation_test(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = len(X[0])\n",
    "epochs = 16\n",
    "batch_size = 128\n",
    "\n",
    "v = torch.tensor([-1,-1,-1,-1,-1,-1,-1,1,1,0.1,1,0.1,0.1,1,0.1])\n",
    "\n",
    "small_eps = 0.04\n",
    "epsilons = [small_eps, 0.06, 0.08, 0.1, 0.15, 0.2, 0.3, 0.4, 0.6, 0.8, 0.95, 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Training Hardt et al's model (strategic with eps=0.04) ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.96494 | error: 0.46094\n",
      "  Ended batch 002 / 034 | loss: 0.99688 | error: 0.48438\n",
      "  Ended batch 003 / 034 | loss: 1.00848 | error: 0.52344\n",
      "  Ended batch 004 / 034 | loss: 1.01666 | error: 0.52344\n",
      "  Ended batch 005 / 034 | loss: 0.99586 | error: 0.47656\n",
      "  Ended batch 006 / 034 | loss: 0.91422 | error: 0.46875\n",
      "  Ended batch 007 / 034 | loss: 1.08640 | error: 0.58594\n",
      "  Ended batch 008 / 034 | loss: 0.91806 | error: 0.31250\n",
      "  Ended batch 009 / 034 | loss: 0.85547 | error: 0.46094\n",
      "  Ended batch 010 / 034 | loss: 0.84307 | error: 0.42188\n",
      "  Ended batch 011 / 034 | loss: 0.88342 | error: 0.47656\n",
      "  Ended batch 012 / 034 | loss: 1.20697 | error: 0.61719\n",
      "  Ended batch 013 / 034 | loss: 0.86470 | error: 0.45312\n",
      "  Ended batch 014 / 034 | loss: 0.85290 | error: 0.42188\n",
      "  Ended batch 015 / 034 | loss: 0.85567 | error: 0.45312\n",
      "  Ended batch 016 / 034 | loss: 0.77038 | error: 0.38281\n",
      "  Ended batch 017 / 034 | loss: 0.78940 | error: 0.41406\n",
      "  Ended batch 018 / 034 | loss: 0.79044 | error: 0.38281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\cvxpy\\problems\\problem.py:1055: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ended batch 019 / 034 | loss: 0.74027 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.94829 | error: 0.45312\n",
      "  Ended batch 021 / 034 | loss: 0.91169 | error: 0.45312\n",
      "  Ended batch 022 / 034 | loss: 0.70683 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.70030 | error: 0.34375\n",
      "  Ended batch 024 / 034 | loss: 0.75500 | error: 0.37500\n",
      "  Ended batch 025 / 034 | loss: 0.80582 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.84008 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.66407 | error: 0.32031\n",
      "  Ended batch 028 / 034 | loss: 0.78769 | error: 0.38281\n",
      "  Ended batch 029 / 034 | loss: 0.59008 | error: 0.29688\n",
      "  Ended batch 030 / 034 | loss: 0.68286 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.75378 | error: 0.36719\n",
      "  Ended batch 032 / 034 | loss: 0.83983 | error: 0.42188\n",
      "  Ended batch 033 / 034 | loss: 0.71213 | error: 0.32031\n",
      "  Ended batch 034 / 034 | loss: 0.82168 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 087 sec | loss: 1.43306 | error: 0.48168\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.71014 | error: 0.32812\n",
      "  Ended batch 002 / 034 | loss: 0.68812 | error: 0.34375\n",
      "  Ended batch 003 / 034 | loss: 0.84963 | error: 0.42188\n",
      "  Ended batch 004 / 034 | loss: 0.74797 | error: 0.37500\n",
      "  Ended batch 005 / 034 | loss: 0.71079 | error: 0.36719\n",
      "  Ended batch 006 / 034 | loss: 0.60290 | error: 0.28906\n",
      "  Ended batch 007 / 034 | loss: 0.94481 | error: 0.49219\n",
      "  Ended batch 008 / 034 | loss: 0.62368 | error: 0.29688\n",
      "  Ended batch 009 / 034 | loss: 0.80506 | error: 0.40625\n",
      "  Ended batch 010 / 034 | loss: 0.70240 | error: 0.35156\n",
      "  Ended batch 011 / 034 | loss: 0.71410 | error: 0.36719\n",
      "  Ended batch 012 / 034 | loss: 0.97903 | error: 0.50781\n",
      "  Ended batch 013 / 034 | loss: 0.82469 | error: 0.42188\n",
      "  Ended batch 014 / 034 | loss: 0.84280 | error: 0.39062\n",
      "  Ended batch 015 / 034 | loss: 0.71334 | error: 0.35156\n",
      "  Ended batch 016 / 034 | loss: 0.75410 | error: 0.38281\n",
      "  Ended batch 017 / 034 | loss: 0.75663 | error: 0.39844\n",
      "  Ended batch 018 / 034 | loss: 0.74189 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.72406 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.87572 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.82388 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.72487 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.77476 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.81467 | error: 0.42969\n",
      "  Ended batch 025 / 034 | loss: 0.86980 | error: 0.45312\n",
      "  Ended batch 026 / 034 | loss: 0.83749 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.81631 | error: 0.42969\n",
      "  Ended batch 028 / 034 | loss: 0.85782 | error: 0.44531\n",
      "  Ended batch 029 / 034 | loss: 0.78449 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.72524 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.76233 | error: 0.38281\n",
      "  Ended batch 032 / 034 | loss: 0.89629 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.79128 | error: 0.40625\n",
      "  Ended batch 034 / 034 | loss: 0.97334 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 096 sec | loss: 0.89009 | error: 0.33129\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80799 | error: 0.41406\n",
      "  Ended batch 002 / 034 | loss: 0.81389 | error: 0.42969\n",
      "  Ended batch 003 / 034 | loss: 0.79005 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.72124 | error: 0.38281\n",
      "  Ended batch 005 / 034 | loss: 0.75947 | error: 0.39062\n",
      "  Ended batch 006 / 034 | loss: 0.70042 | error: 0.36719\n",
      "  Ended batch 007 / 034 | loss: 0.96865 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.78152 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.83253 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.73690 | error: 0.35938\n",
      "  Ended batch 011 / 034 | loss: 0.73490 | error: 0.37500\n",
      "  Ended batch 012 / 034 | loss: 1.02097 | error: 0.56250\n",
      "  Ended batch 013 / 034 | loss: 0.78434 | error: 0.41406\n",
      "  Ended batch 014 / 034 | loss: 0.81976 | error: 0.39844\n",
      "  Ended batch 015 / 034 | loss: 0.81764 | error: 0.44531\n",
      "  Ended batch 016 / 034 | loss: 0.75620 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.78410 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.76357 | error: 0.39062\n",
      "  Ended batch 019 / 034 | loss: 0.73693 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.88027 | error: 0.45312\n",
      "  Ended batch 021 / 034 | loss: 0.86517 | error: 0.46094\n",
      "  Ended batch 022 / 034 | loss: 0.72839 | error: 0.35938\n",
      "  Ended batch 023 / 034 | loss: 0.80574 | error: 0.42969\n",
      "  Ended batch 024 / 034 | loss: 0.87584 | error: 0.47656\n",
      "  Ended batch 025 / 034 | loss: 0.86298 | error: 0.46094\n",
      "  Ended batch 026 / 034 | loss: 0.82500 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.81310 | error: 0.42188\n",
      "  Ended batch 028 / 034 | loss: 0.84260 | error: 0.44531\n",
      "  Ended batch 029 / 034 | loss: 0.78089 | error: 0.42969\n",
      "  Ended batch 030 / 034 | loss: 0.74504 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.78010 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.94273 | error: 0.50000\n",
      "  Ended batch 033 / 034 | loss: 0.81215 | error: 0.42188\n",
      "  Ended batch 034 / 034 | loss: 0.96687 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 085 sec | loss: 0.74237 | error: 0.28116\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.82652 | error: 0.42969\n",
      "  Ended batch 002 / 034 | loss: 0.81662 | error: 0.43750\n",
      "  Ended batch 003 / 034 | loss: 0.80280 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.71840 | error: 0.29688\n",
      "  Ended batch 005 / 034 | loss: 0.77116 | error: 0.31250\n",
      "  Ended batch 006 / 034 | loss: 0.71170 | error: 0.30469\n",
      "  Ended batch 007 / 034 | loss: 0.98730 | error: 0.55469\n",
      "  Ended batch 008 / 034 | loss: 0.78063 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.80283 | error: 0.40625\n",
      "  Ended batch 010 / 034 | loss: 0.73789 | error: 0.31250\n",
      "  Ended batch 011 / 034 | loss: 0.75107 | error: 0.32031\n",
      "  Ended batch 012 / 034 | loss: 0.99360 | error: 0.57031\n",
      "  Ended batch 013 / 034 | loss: 0.78544 | error: 0.41406\n",
      "  Ended batch 014 / 034 | loss: 0.81457 | error: 0.39844\n",
      "  Ended batch 015 / 034 | loss: 0.83754 | error: 0.45312\n",
      "  Ended batch 016 / 034 | loss: 0.80697 | error: 0.42969\n",
      "  Ended batch 017 / 034 | loss: 0.79117 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.75421 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.73019 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.90337 | error: 0.45312\n",
      "  Ended batch 021 / 034 | loss: 0.85935 | error: 0.45312\n",
      "  Ended batch 022 / 034 | loss: 0.72854 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.76676 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.83072 | error: 0.44531\n",
      "  Ended batch 025 / 034 | loss: 0.87638 | error: 0.46094\n",
      "  Ended batch 026 / 034 | loss: 0.82889 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.80714 | error: 0.42969\n",
      "  Ended batch 028 / 034 | loss: 0.84928 | error: 0.44531\n",
      "  Ended batch 029 / 034 | loss: 0.76407 | error: 0.41406\n",
      "  Ended batch 030 / 034 | loss: 0.73295 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.77663 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.95876 | error: 0.50000\n",
      "  Ended batch 033 / 034 | loss: 0.80013 | error: 0.41406\n",
      "  Ended batch 034 / 034 | loss: 0.96461 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 081 sec | loss: 0.84620 | error: 0.32413\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.81715 | error: 0.41406\n",
      "  Ended batch 002 / 034 | loss: 0.80812 | error: 0.34375\n",
      "  Ended batch 003 / 034 | loss: 0.78127 | error: 0.33594\n",
      "  Ended batch 004 / 034 | loss: 0.70067 | error: 0.24219\n",
      "  Ended batch 005 / 034 | loss: 0.77019 | error: 0.34375\n",
      "  Ended batch 006 / 034 | loss: 0.70466 | error: 0.36719\n",
      "  Ended batch 007 / 034 | loss: 1.05862 | error: 0.56250\n",
      "  Ended batch 008 / 034 | loss: 0.80502 | error: 0.42969\n",
      "  Ended batch 009 / 034 | loss: 0.82630 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.73522 | error: 0.30469\n",
      "  Ended batch 011 / 034 | loss: 0.73392 | error: 0.28906\n",
      "  Ended batch 012 / 034 | loss: 0.93675 | error: 0.53906\n",
      "  Ended batch 013 / 034 | loss: 0.80000 | error: 0.39844\n",
      "  Ended batch 014 / 034 | loss: 0.82583 | error: 0.39062\n",
      "  Ended batch 015 / 034 | loss: 0.83293 | error: 0.44531\n",
      "  Ended batch 016 / 034 | loss: 0.84542 | error: 0.43750\n",
      "  Ended batch 017 / 034 | loss: 0.80437 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.76516 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.73493 | error: 0.28906\n",
      "  Ended batch 020 / 034 | loss: 0.83569 | error: 0.42969\n",
      "  Ended batch 021 / 034 | loss: 0.84566 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.71098 | error: 0.35938\n",
      "  Ended batch 023 / 034 | loss: 0.79004 | error: 0.40625\n",
      "  Ended batch 024 / 034 | loss: 0.82193 | error: 0.43750\n",
      "  Ended batch 025 / 034 | loss: 0.83689 | error: 0.43750\n",
      "  Ended batch 026 / 034 | loss: 0.83609 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.74078 | error: 0.35938\n",
      "  Ended batch 028 / 034 | loss: 0.85460 | error: 0.44531\n",
      "  Ended batch 029 / 034 | loss: 0.79550 | error: 0.42969\n",
      "  Ended batch 030 / 034 | loss: 0.72406 | error: 0.37500\n",
      "  Ended batch 031 / 034 | loss: 0.78955 | error: 0.40625\n",
      "  Ended batch 032 / 034 | loss: 0.90490 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.76286 | error: 0.37500\n",
      "  Ended batch 034 / 034 | loss: 0.94931 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 084 sec | loss: 1.09374 | error: 0.44913\n",
      "Starting epoch 006 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.81815 | error: 0.32812\n",
      "  Ended batch 002 / 034 | loss: 0.79993 | error: 0.34375\n",
      "  Ended batch 003 / 034 | loss: 0.80088 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.73627 | error: 0.39062\n",
      "  Ended batch 005 / 034 | loss: 0.78164 | error: 0.40625\n",
      "  Ended batch 006 / 034 | loss: 0.71327 | error: 0.37500\n",
      "  Ended batch 007 / 034 | loss: 1.04641 | error: 0.55469\n",
      "  Ended batch 008 / 034 | loss: 0.78629 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.80647 | error: 0.43750\n",
      "  Ended batch 010 / 034 | loss: 0.74321 | error: 0.29688\n",
      "  Ended batch 011 / 034 | loss: 0.74502 | error: 0.33594\n",
      "  Ended batch 012 / 034 | loss: 1.01662 | error: 0.57031\n",
      "  Ended batch 013 / 034 | loss: 0.80587 | error: 0.42188\n",
      "  Ended batch 014 / 034 | loss: 0.82250 | error: 0.39062\n",
      "  Ended batch 015 / 034 | loss: 0.78143 | error: 0.41406\n",
      "  Ended batch 016 / 034 | loss: 0.75561 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.80009 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.75066 | error: 0.39062\n",
      "  Ended batch 019 / 034 | loss: 0.72123 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.86871 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.84328 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.73992 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.76577 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.90725 | error: 0.46875\n",
      "  Ended batch 025 / 034 | loss: 0.88888 | error: 0.46094\n",
      "  Ended batch 026 / 034 | loss: 0.84241 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.78244 | error: 0.35938\n",
      "  Ended batch 028 / 034 | loss: 0.86176 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.79375 | error: 0.42969\n",
      "  Ended batch 030 / 034 | loss: 0.74313 | error: 0.38281\n",
      "  Ended batch 031 / 034 | loss: 0.81632 | error: 0.42188\n",
      "  Ended batch 032 / 034 | loss: 0.89675 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.79678 | error: 0.35156\n",
      "  Ended batch 034 / 034 | loss: 0.93325 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 006 / 016 | time: 072 sec | loss: 1.10171 | error: 0.46150\n",
      "Starting epoch 007 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80908 | error: 0.30469\n",
      "  Ended batch 002 / 034 | loss: 0.82790 | error: 0.43750\n",
      "  Ended batch 003 / 034 | loss: 0.84542 | error: 0.44531\n",
      "  Ended batch 004 / 034 | loss: 0.85384 | error: 0.44531\n",
      "  Ended batch 005 / 034 | loss: 0.78535 | error: 0.40625\n",
      "  Ended batch 006 / 034 | loss: 0.71761 | error: 0.32812\n",
      "  Ended batch 007 / 034 | loss: 0.92292 | error: 0.53125\n",
      "  Ended batch 008 / 034 | loss: 0.72912 | error: 0.31250\n",
      "  Ended batch 009 / 034 | loss: 0.80480 | error: 0.42969\n",
      "  Ended batch 010 / 034 | loss: 0.76448 | error: 0.39062\n",
      "  Ended batch 011 / 034 | loss: 0.76753 | error: 0.40625\n",
      "  Ended batch 012 / 034 | loss: 1.16785 | error: 0.59375\n",
      "  Ended batch 013 / 034 | loss: 0.80443 | error: 0.42188\n",
      "  Ended batch 014 / 034 | loss: 0.85136 | error: 0.35156\n",
      "  Ended batch 015 / 034 | loss: 0.74429 | error: 0.32031\n",
      "  Ended batch 016 / 034 | loss: 0.76222 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.79982 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.77596 | error: 0.39844\n",
      "  Ended batch 019 / 034 | loss: 0.72225 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.85000 | error: 0.42969\n",
      "  Ended batch 021 / 034 | loss: 0.82265 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.72665 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.76845 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.83491 | error: 0.43750\n",
      "  Ended batch 025 / 034 | loss: 0.86042 | error: 0.44531\n",
      "  Ended batch 026 / 034 | loss: 0.82173 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.75199 | error: 0.37500\n",
      "  Ended batch 028 / 034 | loss: 0.84716 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.78196 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.70013 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.77579 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.90325 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.77318 | error: 0.37500\n",
      "  Ended batch 034 / 034 | loss: 0.94690 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 007 / 016 | time: 082 sec | loss: 1.15552 | error: 0.46475\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 9.769119397799175 minutes (586.1471638679504 seconds).\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "------------------------- 0.04 -------------------------\n",
      "---------- Training non-strategically with epsilon=0.04 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98923 | error: 0.43750\n",
      "  Ended batch 002 / 034 | loss: 0.83193 | error: 0.39844\n",
      "  Ended batch 003 / 034 | loss: 0.75601 | error: 0.38281\n",
      "  Ended batch 004 / 034 | loss: 0.62267 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.53573 | error: 0.26562\n",
      "  Ended batch 006 / 034 | loss: 0.43920 | error: 0.17188\n",
      "  Ended batch 007 / 034 | loss: 0.55319 | error: 0.24219\n",
      "  Ended batch 008 / 034 | loss: 0.43821 | error: 0.17188\n",
      "  Ended batch 009 / 034 | loss: 0.48477 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.44141 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.34122 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.39537 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.46204 | error: 0.17969\n",
      "  Ended batch 014 / 034 | loss: 0.61706 | error: 0.21094\n",
      "  Ended batch 015 / 034 | loss: 0.40735 | error: 0.15625\n",
      "  Ended batch 016 / 034 | loss: 0.44818 | error: 0.19531\n",
      "  Ended batch 017 / 034 | loss: 0.42789 | error: 0.17969\n",
      "  Ended batch 018 / 034 | loss: 0.45081 | error: 0.14062\n",
      "  Ended batch 019 / 034 | loss: 0.40077 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.67522 | error: 0.20312\n",
      "  Ended batch 021 / 034 | loss: 0.44442 | error: 0.17188\n",
      "  Ended batch 022 / 034 | loss: 0.49886 | error: 0.20312\n",
      "  Ended batch 023 / 034 | loss: 0.43399 | error: 0.16406\n",
      "  Ended batch 024 / 034 | loss: 0.42774 | error: 0.17188\n",
      "  Ended batch 025 / 034 | loss: 0.49798 | error: 0.17969\n",
      "  Ended batch 026 / 034 | loss: 0.48518 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34755 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.36209 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30258 | error: 0.14062\n",
      "  Ended batch 030 / 034 | loss: 0.27834 | error: 0.09375\n",
      "  Ended batch 031 / 034 | loss: 0.31573 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.46953 | error: 0.21094\n",
      "  Ended batch 033 / 034 | loss: 0.44510 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.35380 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 000 sec | loss: 0.45466 | error: 0.21103\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/non_strategic_0.04_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.40293 | error: 0.16406\n",
      "  Ended batch 002 / 034 | loss: 0.39698 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39748 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.33817 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31394 | error: 0.13281\n",
      "  Ended batch 006 / 034 | loss: 0.26311 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.40746 | error: 0.21094\n",
      "  Ended batch 008 / 034 | loss: 0.28619 | error: 0.12500\n",
      "  Ended batch 009 / 034 | loss: 0.42815 | error: 0.21875\n",
      "  Ended batch 010 / 034 | loss: 0.37308 | error: 0.14062\n",
      "  Ended batch 011 / 034 | loss: 0.24522 | error: 0.10938\n",
      "  Ended batch 012 / 034 | loss: 0.35200 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.38305 | error: 0.16406\n",
      "  Ended batch 014 / 034 | loss: 0.48793 | error: 0.18750\n",
      "  Ended batch 015 / 034 | loss: 0.29911 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37009 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39094 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.39047 | error: 0.16406\n",
      "  Ended batch 019 / 034 | loss: 0.33128 | error: 0.14844\n",
      "  Ended batch 020 / 034 | loss: 0.62716 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.39453 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.42599 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.40997 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38743 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.49943 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.46154 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34475 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.35799 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30337 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29895 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31889 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44511 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40606 | error: 0.16406\n",
      "  Ended batch 034 / 034 | loss: 0.41604 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 000 sec | loss: 0.44767 | error: 0.21363\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37960 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.38054 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39798 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.32809 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31529 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26527 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.39605 | error: 0.20312\n",
      "  Ended batch 008 / 034 | loss: 0.29298 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42259 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.37166 | error: 0.12500\n",
      "  Ended batch 011 / 034 | loss: 0.24852 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.34251 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37322 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47350 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31297 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.36932 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39082 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38728 | error: 0.15625\n",
      "  Ended batch 019 / 034 | loss: 0.33095 | error: 0.13281\n",
      "  Ended batch 020 / 034 | loss: 0.63589 | error: 0.24219\n",
      "  Ended batch 021 / 034 | loss: 0.39093 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41775 | error: 0.18750\n",
      "  Ended batch 023 / 034 | loss: 0.40417 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38116 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48604 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.45087 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34605 | error: 0.13281\n",
      "  Ended batch 028 / 034 | loss: 0.36658 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30254 | error: 0.10938\n",
      "  Ended batch 030 / 034 | loss: 0.30504 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31964 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44560 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40693 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.43542 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 000 sec | loss: 0.44874 | error: 0.21168\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37193 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37114 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39974 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31597 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31782 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26514 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.39098 | error: 0.18750\n",
      "  Ended batch 008 / 034 | loss: 0.28986 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42376 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.36756 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24499 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34379 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37043 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47858 | error: 0.20312\n",
      "  Ended batch 015 / 034 | loss: 0.30862 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37446 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38862 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38593 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32783 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64187 | error: 0.25000\n",
      "  Ended batch 021 / 034 | loss: 0.38988 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41519 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39861 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37473 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48543 | error: 0.21094\n",
      "  Ended batch 026 / 034 | loss: 0.44854 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34043 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36239 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.29949 | error: 0.11719\n",
      "  Ended batch 030 / 034 | loss: 0.29681 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31391 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45120 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40857 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42124 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 000 sec | loss: 0.44502 | error: 0.21103\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37063 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37067 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39355 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31063 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31461 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26898 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.38740 | error: 0.19531\n",
      "  Ended batch 008 / 034 | loss: 0.28760 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42075 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.36434 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24144 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34192 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.36529 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47503 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31060 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37830 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38680 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38255 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32599 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64071 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.38804 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41321 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39709 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37160 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48514 | error: 0.20312\n",
      "  Ended batch 026 / 034 | loss: 0.44696 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34037 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36098 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30023 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29524 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31327 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45363 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40813 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42413 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 000 sec | loss: 0.44367 | error: 0.21168\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 1.3260080814361572 seconds.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.04_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.04_model.pt.\n",
      "---------- Training strategically with epsilon=0.04 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.96494 | error: 0.46094\n",
      "  Ended batch 002 / 034 | loss: 0.99688 | error: 0.48438\n",
      "  Ended batch 003 / 034 | loss: 1.00848 | error: 0.52344\n",
      "  Ended batch 004 / 034 | loss: 1.01666 | error: 0.52344\n",
      "  Ended batch 005 / 034 | loss: 0.99586 | error: 0.47656\n",
      "  Ended batch 006 / 034 | loss: 0.91422 | error: 0.46875\n",
      "  Ended batch 007 / 034 | loss: 1.08640 | error: 0.58594\n",
      "  Ended batch 008 / 034 | loss: 0.91806 | error: 0.31250\n",
      "  Ended batch 009 / 034 | loss: 0.85547 | error: 0.46094\n",
      "  Ended batch 010 / 034 | loss: 0.84307 | error: 0.42188\n",
      "  Ended batch 011 / 034 | loss: 0.88342 | error: 0.47656\n",
      "  Ended batch 012 / 034 | loss: 1.20697 | error: 0.61719\n",
      "  Ended batch 013 / 034 | loss: 0.86470 | error: 0.45312\n",
      "  Ended batch 014 / 034 | loss: 0.85290 | error: 0.42188\n",
      "  Ended batch 015 / 034 | loss: 0.85567 | error: 0.45312\n",
      "  Ended batch 016 / 034 | loss: 0.77038 | error: 0.38281\n",
      "  Ended batch 017 / 034 | loss: 0.78940 | error: 0.41406\n",
      "  Ended batch 018 / 034 | loss: 0.79044 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.74027 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.94829 | error: 0.45312\n",
      "  Ended batch 021 / 034 | loss: 0.91169 | error: 0.45312\n",
      "  Ended batch 022 / 034 | loss: 0.70683 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.70030 | error: 0.34375\n",
      "  Ended batch 024 / 034 | loss: 0.75500 | error: 0.37500\n",
      "  Ended batch 025 / 034 | loss: 0.80582 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.84008 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.66407 | error: 0.32031\n",
      "  Ended batch 028 / 034 | loss: 0.78769 | error: 0.38281\n",
      "  Ended batch 029 / 034 | loss: 0.59008 | error: 0.29688\n",
      "  Ended batch 030 / 034 | loss: 0.68286 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.75378 | error: 0.36719\n",
      "  Ended batch 032 / 034 | loss: 0.83983 | error: 0.42188\n",
      "  Ended batch 033 / 034 | loss: 0.71213 | error: 0.32031\n",
      "  Ended batch 034 / 034 | loss: 0.82168 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 081 sec | loss: 1.43306 | error: 0.48168\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.04_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.71014 | error: 0.32812\n",
      "  Ended batch 002 / 034 | loss: 0.68812 | error: 0.34375\n",
      "  Ended batch 003 / 034 | loss: 0.84963 | error: 0.42188\n",
      "  Ended batch 004 / 034 | loss: 0.74797 | error: 0.37500\n",
      "  Ended batch 005 / 034 | loss: 0.71079 | error: 0.36719\n",
      "  Ended batch 006 / 034 | loss: 0.60290 | error: 0.28906\n",
      "  Ended batch 007 / 034 | loss: 0.94481 | error: 0.49219\n",
      "  Ended batch 008 / 034 | loss: 0.62368 | error: 0.29688\n",
      "  Ended batch 009 / 034 | loss: 0.80506 | error: 0.40625\n",
      "  Ended batch 010 / 034 | loss: 0.70240 | error: 0.35156\n",
      "  Ended batch 011 / 034 | loss: 0.71410 | error: 0.36719\n",
      "  Ended batch 012 / 034 | loss: 0.97903 | error: 0.50781\n",
      "  Ended batch 013 / 034 | loss: 0.82469 | error: 0.42188\n",
      "  Ended batch 014 / 034 | loss: 0.84280 | error: 0.39062\n",
      "  Ended batch 015 / 034 | loss: 0.71334 | error: 0.35156\n",
      "  Ended batch 016 / 034 | loss: 0.75410 | error: 0.38281\n",
      "  Ended batch 017 / 034 | loss: 0.75663 | error: 0.39844\n",
      "  Ended batch 018 / 034 | loss: 0.74189 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.72406 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.87572 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.82388 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.72487 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.77476 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.81467 | error: 0.42969\n",
      "  Ended batch 025 / 034 | loss: 0.86980 | error: 0.45312\n",
      "  Ended batch 026 / 034 | loss: 0.83749 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.81631 | error: 0.42969\n",
      "  Ended batch 028 / 034 | loss: 0.85782 | error: 0.44531\n",
      "  Ended batch 029 / 034 | loss: 0.78449 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.72524 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.76233 | error: 0.38281\n",
      "  Ended batch 032 / 034 | loss: 0.89629 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.79128 | error: 0.40625\n",
      "  Ended batch 034 / 034 | loss: 0.97334 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 080 sec | loss: 0.89009 | error: 0.33129\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.04_model.pt.\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80799 | error: 0.41406\n",
      "  Ended batch 002 / 034 | loss: 0.81389 | error: 0.42969\n",
      "  Ended batch 003 / 034 | loss: 0.79005 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.72124 | error: 0.38281\n",
      "  Ended batch 005 / 034 | loss: 0.75947 | error: 0.39062\n",
      "  Ended batch 006 / 034 | loss: 0.70042 | error: 0.36719\n",
      "  Ended batch 007 / 034 | loss: 0.96865 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.78152 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.83253 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.73690 | error: 0.35938\n",
      "  Ended batch 011 / 034 | loss: 0.73490 | error: 0.37500\n",
      "  Ended batch 012 / 034 | loss: 1.02097 | error: 0.56250\n",
      "  Ended batch 013 / 034 | loss: 0.78434 | error: 0.41406\n",
      "  Ended batch 014 / 034 | loss: 0.81976 | error: 0.39844\n",
      "  Ended batch 015 / 034 | loss: 0.81764 | error: 0.44531\n",
      "  Ended batch 016 / 034 | loss: 0.75620 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.78410 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.76357 | error: 0.39062\n",
      "  Ended batch 019 / 034 | loss: 0.73693 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.88027 | error: 0.45312\n",
      "  Ended batch 021 / 034 | loss: 0.86517 | error: 0.46094\n",
      "  Ended batch 022 / 034 | loss: 0.72839 | error: 0.35938\n",
      "  Ended batch 023 / 034 | loss: 0.80574 | error: 0.42969\n",
      "  Ended batch 024 / 034 | loss: 0.87584 | error: 0.47656\n",
      "  Ended batch 025 / 034 | loss: 0.86298 | error: 0.46094\n",
      "  Ended batch 026 / 034 | loss: 0.82500 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.81310 | error: 0.42188\n",
      "  Ended batch 028 / 034 | loss: 0.84260 | error: 0.44531\n",
      "  Ended batch 029 / 034 | loss: 0.78089 | error: 0.42969\n",
      "  Ended batch 030 / 034 | loss: 0.74504 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.78010 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.94273 | error: 0.50000\n",
      "  Ended batch 033 / 034 | loss: 0.81215 | error: 0.42188\n",
      "  Ended batch 034 / 034 | loss: 0.96687 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 064 sec | loss: 0.74237 | error: 0.28116\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.04_model.pt.\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.82652 | error: 0.42969\n",
      "  Ended batch 002 / 034 | loss: 0.81662 | error: 0.43750\n",
      "  Ended batch 003 / 034 | loss: 0.80280 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.71840 | error: 0.29688\n",
      "  Ended batch 005 / 034 | loss: 0.77116 | error: 0.31250\n",
      "  Ended batch 006 / 034 | loss: 0.71170 | error: 0.30469\n",
      "  Ended batch 007 / 034 | loss: 0.98730 | error: 0.55469\n",
      "  Ended batch 008 / 034 | loss: 0.78063 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.80283 | error: 0.40625\n",
      "  Ended batch 010 / 034 | loss: 0.73789 | error: 0.31250\n",
      "  Ended batch 011 / 034 | loss: 0.75107 | error: 0.32031\n",
      "  Ended batch 012 / 034 | loss: 0.99360 | error: 0.57031\n",
      "  Ended batch 013 / 034 | loss: 0.78544 | error: 0.41406\n",
      "  Ended batch 014 / 034 | loss: 0.81457 | error: 0.39844\n",
      "  Ended batch 015 / 034 | loss: 0.83754 | error: 0.45312\n",
      "  Ended batch 016 / 034 | loss: 0.80697 | error: 0.42969\n",
      "  Ended batch 017 / 034 | loss: 0.79117 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.75421 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.73019 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.90337 | error: 0.45312\n",
      "  Ended batch 021 / 034 | loss: 0.85935 | error: 0.45312\n",
      "  Ended batch 022 / 034 | loss: 0.72854 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.76676 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.83072 | error: 0.44531\n",
      "  Ended batch 025 / 034 | loss: 0.87638 | error: 0.46094\n",
      "  Ended batch 026 / 034 | loss: 0.82889 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.80714 | error: 0.42969\n",
      "  Ended batch 028 / 034 | loss: 0.84928 | error: 0.44531\n",
      "  Ended batch 029 / 034 | loss: 0.76407 | error: 0.41406\n",
      "  Ended batch 030 / 034 | loss: 0.73295 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.77663 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.95876 | error: 0.50000\n",
      "  Ended batch 033 / 034 | loss: 0.80013 | error: 0.41406\n",
      "  Ended batch 034 / 034 | loss: 0.96461 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 067 sec | loss: 0.84620 | error: 0.32413\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.81715 | error: 0.41406\n",
      "  Ended batch 002 / 034 | loss: 0.80812 | error: 0.34375\n",
      "  Ended batch 003 / 034 | loss: 0.78127 | error: 0.33594\n",
      "  Ended batch 004 / 034 | loss: 0.70067 | error: 0.24219\n",
      "  Ended batch 005 / 034 | loss: 0.77019 | error: 0.34375\n",
      "  Ended batch 006 / 034 | loss: 0.70466 | error: 0.36719\n",
      "  Ended batch 007 / 034 | loss: 1.05862 | error: 0.56250\n",
      "  Ended batch 008 / 034 | loss: 0.80502 | error: 0.42969\n",
      "  Ended batch 009 / 034 | loss: 0.82630 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.73522 | error: 0.30469\n",
      "  Ended batch 011 / 034 | loss: 0.73392 | error: 0.28906\n",
      "  Ended batch 012 / 034 | loss: 0.93675 | error: 0.53906\n",
      "  Ended batch 013 / 034 | loss: 0.80000 | error: 0.39844\n",
      "  Ended batch 014 / 034 | loss: 0.82583 | error: 0.39062\n",
      "  Ended batch 015 / 034 | loss: 0.83293 | error: 0.44531\n",
      "  Ended batch 016 / 034 | loss: 0.84542 | error: 0.43750\n",
      "  Ended batch 017 / 034 | loss: 0.80437 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.76516 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.73493 | error: 0.28906\n",
      "  Ended batch 020 / 034 | loss: 0.83569 | error: 0.42969\n",
      "  Ended batch 021 / 034 | loss: 0.84566 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.71098 | error: 0.35938\n",
      "  Ended batch 023 / 034 | loss: 0.79004 | error: 0.40625\n",
      "  Ended batch 024 / 034 | loss: 0.82193 | error: 0.43750\n",
      "  Ended batch 025 / 034 | loss: 0.83689 | error: 0.43750\n",
      "  Ended batch 026 / 034 | loss: 0.83609 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.74078 | error: 0.35938\n",
      "  Ended batch 028 / 034 | loss: 0.85460 | error: 0.44531\n",
      "  Ended batch 029 / 034 | loss: 0.79550 | error: 0.42969\n",
      "  Ended batch 030 / 034 | loss: 0.72406 | error: 0.37500\n",
      "  Ended batch 031 / 034 | loss: 0.78955 | error: 0.40625\n",
      "  Ended batch 032 / 034 | loss: 0.90490 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.76286 | error: 0.37500\n",
      "  Ended batch 034 / 034 | loss: 0.94931 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 068 sec | loss: 1.09374 | error: 0.44913\n",
      "Starting epoch 006 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.81815 | error: 0.32812\n",
      "  Ended batch 002 / 034 | loss: 0.79993 | error: 0.34375\n",
      "  Ended batch 003 / 034 | loss: 0.80088 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.73627 | error: 0.39062\n",
      "  Ended batch 005 / 034 | loss: 0.78164 | error: 0.40625\n",
      "  Ended batch 006 / 034 | loss: 0.71327 | error: 0.37500\n",
      "  Ended batch 007 / 034 | loss: 1.04641 | error: 0.55469\n",
      "  Ended batch 008 / 034 | loss: 0.78629 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.80647 | error: 0.43750\n",
      "  Ended batch 010 / 034 | loss: 0.74321 | error: 0.29688\n",
      "  Ended batch 011 / 034 | loss: 0.74502 | error: 0.33594\n",
      "  Ended batch 012 / 034 | loss: 1.01662 | error: 0.57031\n",
      "  Ended batch 013 / 034 | loss: 0.80587 | error: 0.42188\n",
      "  Ended batch 014 / 034 | loss: 0.82250 | error: 0.39062\n",
      "  Ended batch 015 / 034 | loss: 0.78143 | error: 0.41406\n",
      "  Ended batch 016 / 034 | loss: 0.75561 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.80009 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.75066 | error: 0.39062\n",
      "  Ended batch 019 / 034 | loss: 0.72123 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.86871 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.84328 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.73992 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.76577 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.90725 | error: 0.46875\n",
      "  Ended batch 025 / 034 | loss: 0.88888 | error: 0.46094\n",
      "  Ended batch 026 / 034 | loss: 0.84241 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.78244 | error: 0.35938\n",
      "  Ended batch 028 / 034 | loss: 0.86176 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.79375 | error: 0.42969\n",
      "  Ended batch 030 / 034 | loss: 0.74313 | error: 0.38281\n",
      "  Ended batch 031 / 034 | loss: 0.81632 | error: 0.42188\n",
      "  Ended batch 032 / 034 | loss: 0.89675 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.79678 | error: 0.35156\n",
      "  Ended batch 034 / 034 | loss: 0.93325 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 006 / 016 | time: 066 sec | loss: 1.10171 | error: 0.46150\n",
      "Starting epoch 007 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80908 | error: 0.30469\n",
      "  Ended batch 002 / 034 | loss: 0.82790 | error: 0.43750\n",
      "  Ended batch 003 / 034 | loss: 0.84542 | error: 0.44531\n",
      "  Ended batch 004 / 034 | loss: 0.85384 | error: 0.44531\n",
      "  Ended batch 005 / 034 | loss: 0.78535 | error: 0.40625\n",
      "  Ended batch 006 / 034 | loss: 0.71761 | error: 0.32812\n",
      "  Ended batch 007 / 034 | loss: 0.92292 | error: 0.53125\n",
      "  Ended batch 008 / 034 | loss: 0.72912 | error: 0.31250\n",
      "  Ended batch 009 / 034 | loss: 0.80480 | error: 0.42969\n",
      "  Ended batch 010 / 034 | loss: 0.76448 | error: 0.39062\n",
      "  Ended batch 011 / 034 | loss: 0.76753 | error: 0.40625\n",
      "  Ended batch 012 / 034 | loss: 1.16785 | error: 0.59375\n",
      "  Ended batch 013 / 034 | loss: 0.80443 | error: 0.42188\n",
      "  Ended batch 014 / 034 | loss: 0.85136 | error: 0.35156\n",
      "  Ended batch 015 / 034 | loss: 0.74429 | error: 0.32031\n",
      "  Ended batch 016 / 034 | loss: 0.76222 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.79982 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.77596 | error: 0.39844\n",
      "  Ended batch 019 / 034 | loss: 0.72225 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.85000 | error: 0.42969\n",
      "  Ended batch 021 / 034 | loss: 0.82265 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.72665 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.76845 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.83491 | error: 0.43750\n",
      "  Ended batch 025 / 034 | loss: 0.86042 | error: 0.44531\n",
      "  Ended batch 026 / 034 | loss: 0.82173 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.75199 | error: 0.37500\n",
      "  Ended batch 028 / 034 | loss: 0.84716 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.78196 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.70013 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.77579 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.90325 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.77318 | error: 0.37500\n",
      "  Ended batch 034 / 034 | loss: 0.94690 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 007 / 016 | time: 062 sec | loss: 1.15552 | error: 0.46475\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 8.128273193041483 minutes (487.696391582489 seconds).\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.04_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.04_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "---------- Calculating results ----------\n",
      "Evaluating using epsilon: 0.04\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0903, -0.2800, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "         -0.0746,  0.0482, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919,  0.1656,\n",
      "          0.3650,  0.3795, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "          0.0652, -0.1662, -0.0495,  0.3771, -0.0304, -0.0473, -0.0763]])\n",
      "Evaluating using epsilon: 0.04\n",
      "Evaluating on X_opt=\n",
      "tensor([[-1.2484e-01, -9.4696e-02, -1.1800e-01, -1.2797e-01, -3.9515e-02,\n",
      "         -9.8223e-02, -1.1691e-01, -6.9089e-02, -2.1326e-02, -5.2895e-03,\n",
      "         -1.5114e-01, -2.1046e-01, -8.9142e-02, -6.2027e-02, -1.1477e-01],\n",
      "        [-4.9494e-01,  2.2466e+00, -5.5744e-01, -5.8412e-01,  1.1835e-03,\n",
      "         -4.0783e-01, -3.9972e-01,  8.4724e-01,  1.0197e+00, -2.7724e-01,\n",
      "         -8.7131e-01, -4.3817e-01, -2.5163e-01, -1.7151e-01, -5.4879e-01],\n",
      "        [-1.2223e-01,  2.4440e-01, -1.3685e-01, -1.4754e-01, -3.7769e-02,\n",
      "         -1.1150e-01, -1.2904e-01, -4.2235e-02,  1.4429e-01, -2.4553e-01,\n",
      "         -1.4423e-01,  3.4713e-01, -5.1806e-02, -6.1710e-02, -1.3339e-01]])\n",
      "Evaluating using epsilon: 0.04\n",
      "Evaluating on X_opt=\n",
      "tensor([[-1.2484e-01, -9.4696e-02, -1.1800e-01, -1.2797e-01, -3.9515e-02,\n",
      "         -9.8223e-02, -1.1691e-01, -6.9089e-02, -2.1326e-02, -5.2895e-03,\n",
      "         -1.5114e-01, -2.1046e-01, -8.9142e-02, -6.2027e-02, -1.1477e-01],\n",
      "        [-4.9494e-01,  2.2466e+00, -5.5744e-01, -5.8412e-01,  1.1835e-03,\n",
      "         -4.0783e-01, -3.9972e-01,  8.4724e-01,  1.0197e+00, -2.7724e-01,\n",
      "         -8.7131e-01, -4.3817e-01, -2.5163e-01, -1.7151e-01, -5.4879e-01],\n",
      "        [-1.2223e-01,  2.4440e-01, -1.3685e-01, -1.4754e-01, -3.7769e-02,\n",
      "         -1.1150e-01, -1.2904e-01, -4.2235e-02,  1.4429e-01, -2.4553e-01,\n",
      "         -1.4423e-01,  3.4713e-01, -5.1806e-02, -6.1710e-02, -1.3339e-01]])\n",
      "Evaluating using epsilon: 0.04\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1243,  0.4847, -0.3604, -0.3537, -0.3788, -0.2676, -0.1593, -0.2330,\n",
      "         -0.1589, -0.0901, -0.5219, -0.1950, -0.0587,  0.1821, -0.1865],\n",
      "        [-0.1019,  0.6646, -0.3348, -0.3295, -0.3483, -0.2498, -0.1532,  0.0670,\n",
      "          0.2883,  0.2537, -0.4824, -0.1946, -0.0602,  0.1608, -0.1765],\n",
      "        [-0.1009,  0.6436, -0.3271, -0.3222, -0.3391, -0.2444, -0.1514, -0.2202,\n",
      "         -0.0091, -0.2881, -0.4327,  0.3729, -0.0163,  0.1594, -0.1734]])\n",
      "------------------------- 0.06 -------------------------\n",
      "---------- Training non-strategically with epsilon=0.06 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98923 | error: 0.43750\n",
      "  Ended batch 002 / 034 | loss: 0.83193 | error: 0.39844\n",
      "  Ended batch 003 / 034 | loss: 0.75601 | error: 0.38281\n",
      "  Ended batch 004 / 034 | loss: 0.62267 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.53573 | error: 0.26562\n",
      "  Ended batch 006 / 034 | loss: 0.43920 | error: 0.17188\n",
      "  Ended batch 007 / 034 | loss: 0.55319 | error: 0.24219\n",
      "  Ended batch 008 / 034 | loss: 0.43821 | error: 0.17188\n",
      "  Ended batch 009 / 034 | loss: 0.48477 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.44141 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.34122 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.39537 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.46204 | error: 0.17969\n",
      "  Ended batch 014 / 034 | loss: 0.61706 | error: 0.21094\n",
      "  Ended batch 015 / 034 | loss: 0.40735 | error: 0.15625\n",
      "  Ended batch 016 / 034 | loss: 0.44818 | error: 0.19531\n",
      "  Ended batch 017 / 034 | loss: 0.42789 | error: 0.17969\n",
      "  Ended batch 018 / 034 | loss: 0.45081 | error: 0.14062\n",
      "  Ended batch 019 / 034 | loss: 0.40077 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.67522 | error: 0.20312\n",
      "  Ended batch 021 / 034 | loss: 0.44442 | error: 0.17188\n",
      "  Ended batch 022 / 034 | loss: 0.49886 | error: 0.20312\n",
      "  Ended batch 023 / 034 | loss: 0.43399 | error: 0.16406\n",
      "  Ended batch 024 / 034 | loss: 0.42774 | error: 0.17188\n",
      "  Ended batch 025 / 034 | loss: 0.49798 | error: 0.17969\n",
      "  Ended batch 026 / 034 | loss: 0.48518 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34755 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.36209 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30258 | error: 0.14062\n",
      "  Ended batch 030 / 034 | loss: 0.27834 | error: 0.09375\n",
      "  Ended batch 031 / 034 | loss: 0.31573 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.46953 | error: 0.21094\n",
      "  Ended batch 033 / 034 | loss: 0.44510 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.35380 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 000 sec | loss: 0.45466 | error: 0.21103\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/non_strategic_0.06_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.40293 | error: 0.16406\n",
      "  Ended batch 002 / 034 | loss: 0.39698 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39748 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.33817 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31394 | error: 0.13281\n",
      "  Ended batch 006 / 034 | loss: 0.26311 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.40746 | error: 0.21094\n",
      "  Ended batch 008 / 034 | loss: 0.28619 | error: 0.12500\n",
      "  Ended batch 009 / 034 | loss: 0.42815 | error: 0.21875\n",
      "  Ended batch 010 / 034 | loss: 0.37308 | error: 0.14062\n",
      "  Ended batch 011 / 034 | loss: 0.24522 | error: 0.10938\n",
      "  Ended batch 012 / 034 | loss: 0.35200 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.38305 | error: 0.16406\n",
      "  Ended batch 014 / 034 | loss: 0.48793 | error: 0.18750\n",
      "  Ended batch 015 / 034 | loss: 0.29911 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37009 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39094 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.39047 | error: 0.16406\n",
      "  Ended batch 019 / 034 | loss: 0.33128 | error: 0.14844\n",
      "  Ended batch 020 / 034 | loss: 0.62716 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.39453 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.42599 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.40997 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38743 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.49943 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.46154 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34475 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.35799 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30337 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29895 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31889 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44511 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40606 | error: 0.16406\n",
      "  Ended batch 034 / 034 | loss: 0.41604 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 000 sec | loss: 0.44767 | error: 0.21363\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37960 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.38054 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39798 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.32809 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31529 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26527 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.39605 | error: 0.20312\n",
      "  Ended batch 008 / 034 | loss: 0.29298 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42259 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.37166 | error: 0.12500\n",
      "  Ended batch 011 / 034 | loss: 0.24852 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.34251 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37322 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47350 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31297 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.36932 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39082 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38728 | error: 0.15625\n",
      "  Ended batch 019 / 034 | loss: 0.33095 | error: 0.13281\n",
      "  Ended batch 020 / 034 | loss: 0.63589 | error: 0.24219\n",
      "  Ended batch 021 / 034 | loss: 0.39093 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41775 | error: 0.18750\n",
      "  Ended batch 023 / 034 | loss: 0.40417 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38116 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48604 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.45087 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34605 | error: 0.13281\n",
      "  Ended batch 028 / 034 | loss: 0.36658 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30254 | error: 0.10938\n",
      "  Ended batch 030 / 034 | loss: 0.30504 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31964 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44560 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40693 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.43542 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 000 sec | loss: 0.44874 | error: 0.21168\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37193 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37114 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39974 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31597 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31782 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26514 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.39098 | error: 0.18750\n",
      "  Ended batch 008 / 034 | loss: 0.28986 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42376 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.36756 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24499 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34379 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37043 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47858 | error: 0.20312\n",
      "  Ended batch 015 / 034 | loss: 0.30862 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37446 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38862 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38593 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32783 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64187 | error: 0.25000\n",
      "  Ended batch 021 / 034 | loss: 0.38988 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41519 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39861 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37473 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48543 | error: 0.21094\n",
      "  Ended batch 026 / 034 | loss: 0.44854 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34043 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36239 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.29949 | error: 0.11719\n",
      "  Ended batch 030 / 034 | loss: 0.29681 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31391 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45120 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40857 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42124 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 000 sec | loss: 0.44502 | error: 0.21103\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37063 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37067 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39355 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31063 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31461 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26898 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.38740 | error: 0.19531\n",
      "  Ended batch 008 / 034 | loss: 0.28760 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42075 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.36434 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24144 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34192 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.36529 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47503 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31060 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37830 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38680 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38255 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32599 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64071 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.38804 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41321 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39709 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37160 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48514 | error: 0.20312\n",
      "  Ended batch 026 / 034 | loss: 0.44696 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34037 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36098 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30023 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29524 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31327 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45363 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40813 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42413 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 000 sec | loss: 0.44367 | error: 0.21168\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 0.5610013008117676 seconds.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.06_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.06_model.pt.\n",
      "---------- Training strategically with epsilon=0.06 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.92847 | error: 0.46094\n",
      "  Ended batch 002 / 034 | loss: 1.34152 | error: 0.48438\n",
      "  Ended batch 003 / 034 | loss: 1.28097 | error: 0.52344\n",
      "  Ended batch 004 / 034 | loss: 0.97005 | error: 0.51562\n",
      "  Ended batch 005 / 034 | loss: 0.94224 | error: 0.47656\n",
      "  Ended batch 006 / 034 | loss: 0.93971 | error: 0.46875\n",
      "  Ended batch 007 / 034 | loss: 1.12104 | error: 0.58594\n",
      "  Ended batch 008 / 034 | loss: 0.89679 | error: 0.46875\n",
      "  Ended batch 009 / 034 | loss: 0.89169 | error: 0.43750\n",
      "  Ended batch 010 / 034 | loss: 0.80658 | error: 0.30469\n",
      "  Ended batch 011 / 034 | loss: 0.78563 | error: 0.39062\n",
      "  Ended batch 012 / 034 | loss: 1.11573 | error: 0.60156\n",
      "  Ended batch 013 / 034 | loss: 0.83501 | error: 0.45312\n",
      "  Ended batch 014 / 034 | loss: 0.83011 | error: 0.38281\n",
      "  Ended batch 015 / 034 | loss: 0.78588 | error: 0.40625\n",
      "  Ended batch 016 / 034 | loss: 0.79189 | error: 0.40625\n",
      "  Ended batch 017 / 034 | loss: 0.81899 | error: 0.43750\n",
      "  Ended batch 018 / 034 | loss: 0.78206 | error: 0.39844\n",
      "  Ended batch 019 / 034 | loss: 0.71491 | error: 0.36719\n",
      "  Ended batch 020 / 034 | loss: 0.84900 | error: 0.42188\n",
      "  Ended batch 021 / 034 | loss: 0.85366 | error: 0.43750\n",
      "  Ended batch 022 / 034 | loss: 0.64369 | error: 0.30469\n",
      "  Ended batch 023 / 034 | loss: 0.77550 | error: 0.39062\n",
      "  Ended batch 024 / 034 | loss: 0.80522 | error: 0.40625\n",
      "  Ended batch 025 / 034 | loss: 0.80854 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.79660 | error: 0.38281\n",
      "  Ended batch 027 / 034 | loss: 0.66395 | error: 0.31250\n",
      "  Ended batch 028 / 034 | loss: 0.82326 | error: 0.41406\n",
      "  Ended batch 029 / 034 | loss: 0.70524 | error: 0.35938\n",
      "  Ended batch 030 / 034 | loss: 0.71668 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.68135 | error: 0.33594\n",
      "  Ended batch 032 / 034 | loss: 0.76357 | error: 0.38281\n",
      "  Ended batch 033 / 034 | loss: 0.71152 | error: 0.34375\n",
      "  Ended batch 034 / 034 | loss: 0.99280 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 058 sec | loss: 1.31826 | error: 0.47321\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.06_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.71425 | error: 0.35156\n",
      "  Ended batch 002 / 034 | loss: 0.69565 | error: 0.35156\n",
      "  Ended batch 003 / 034 | loss: 0.79100 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.63215 | error: 0.32812\n",
      "  Ended batch 005 / 034 | loss: 0.65186 | error: 0.33594\n",
      "  Ended batch 006 / 034 | loss: 0.60418 | error: 0.31250\n",
      "  Ended batch 007 / 034 | loss: 0.98006 | error: 0.50781\n",
      "  Ended batch 008 / 034 | loss: 0.68757 | error: 0.35156\n",
      "  Ended batch 009 / 034 | loss: 0.75382 | error: 0.37500\n",
      "  Ended batch 010 / 034 | loss: 0.66493 | error: 0.31250\n",
      "  Ended batch 011 / 034 | loss: 0.65221 | error: 0.32812\n",
      "  Ended batch 012 / 034 | loss: 0.95273 | error: 0.49219\n",
      "  Ended batch 013 / 034 | loss: 0.82443 | error: 0.42188\n",
      "  Ended batch 014 / 034 | loss: 0.75721 | error: 0.35938\n",
      "  Ended batch 015 / 034 | loss: 0.76382 | error: 0.39844\n",
      "  Ended batch 016 / 034 | loss: 0.69088 | error: 0.34375\n",
      "  Ended batch 017 / 034 | loss: 0.69684 | error: 0.36719\n",
      "  Ended batch 018 / 034 | loss: 0.71025 | error: 0.36719\n",
      "  Ended batch 019 / 034 | loss: 0.70333 | error: 0.36719\n",
      "  Ended batch 020 / 034 | loss: 0.87551 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.80240 | error: 0.42188\n",
      "  Ended batch 022 / 034 | loss: 0.71773 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.76250 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.76907 | error: 0.40625\n",
      "  Ended batch 025 / 034 | loss: 0.86309 | error: 0.44531\n",
      "  Ended batch 026 / 034 | loss: 0.82668 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.74170 | error: 0.38281\n",
      "  Ended batch 028 / 034 | loss: 0.80379 | error: 0.41406\n",
      "  Ended batch 029 / 034 | loss: 0.70513 | error: 0.36719\n",
      "  Ended batch 030 / 034 | loss: 0.71897 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.75747 | error: 0.39062\n",
      "  Ended batch 032 / 034 | loss: 0.92146 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.81623 | error: 0.41406\n",
      "  Ended batch 034 / 034 | loss: 0.96495 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 067 sec | loss: 0.85697 | error: 0.31501\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.06_model.pt.\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80948 | error: 0.41406\n",
      "  Ended batch 002 / 034 | loss: 0.76236 | error: 0.37500\n",
      "  Ended batch 003 / 034 | loss: 0.77999 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.66343 | error: 0.31250\n",
      "  Ended batch 005 / 034 | loss: 0.75550 | error: 0.39062\n",
      "  Ended batch 006 / 034 | loss: 0.65857 | error: 0.34375\n",
      "  Ended batch 007 / 034 | loss: 1.04502 | error: 0.54688\n",
      "  Ended batch 008 / 034 | loss: 0.82166 | error: 0.43750\n",
      "  Ended batch 009 / 034 | loss: 0.81962 | error: 0.44531\n",
      "  Ended batch 010 / 034 | loss: 0.72478 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.71461 | error: 0.25781\n",
      "  Ended batch 012 / 034 | loss: 0.85504 | error: 0.50000\n",
      "  Ended batch 013 / 034 | loss: 0.74738 | error: 0.25000\n",
      "  Ended batch 014 / 034 | loss: 0.84346 | error: 0.31250\n",
      "  Ended batch 015 / 034 | loss: 0.78534 | error: 0.42188\n",
      "  Ended batch 016 / 034 | loss: 0.77665 | error: 0.40625\n",
      "  Ended batch 017 / 034 | loss: 0.85752 | error: 0.44531\n",
      "  Ended batch 018 / 034 | loss: 0.74634 | error: 0.39062\n",
      "  Ended batch 019 / 034 | loss: 0.73111 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.84277 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.83247 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.70238 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.79759 | error: 0.41406\n",
      "  Ended batch 024 / 034 | loss: 0.87085 | error: 0.46094\n",
      "  Ended batch 025 / 034 | loss: 0.82725 | error: 0.43750\n",
      "  Ended batch 026 / 034 | loss: 0.81216 | error: 0.37500\n",
      "  Ended batch 027 / 034 | loss: 0.72249 | error: 0.26562\n",
      "  Ended batch 028 / 034 | loss: 0.79407 | error: 0.32031\n",
      "  Ended batch 029 / 034 | loss: 0.74759 | error: 0.40625\n",
      "  Ended batch 030 / 034 | loss: 0.71801 | error: 0.36719\n",
      "  Ended batch 031 / 034 | loss: 0.77673 | error: 0.40625\n",
      "  Ended batch 032 / 034 | loss: 0.93517 | error: 0.48438\n",
      "  Ended batch 033 / 034 | loss: 0.77097 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.95777 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 069 sec | loss: 0.85504 | error: 0.32022\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.82060 | error: 0.41406\n",
      "  Ended batch 002 / 034 | loss: 0.78460 | error: 0.39844\n",
      "  Ended batch 003 / 034 | loss: 0.79048 | error: 0.42188\n",
      "  Ended batch 004 / 034 | loss: 0.70524 | error: 0.37500\n",
      "  Ended batch 005 / 034 | loss: 0.75933 | error: 0.39844\n",
      "  Ended batch 006 / 034 | loss: 0.69869 | error: 0.37500\n",
      "  Ended batch 007 / 034 | loss: 0.99442 | error: 0.53906\n",
      "  Ended batch 008 / 034 | loss: 0.79606 | error: 0.42969\n",
      "  Ended batch 009 / 034 | loss: 0.83351 | error: 0.46094\n",
      "  Ended batch 010 / 034 | loss: 0.72528 | error: 0.31250\n",
      "  Ended batch 011 / 034 | loss: 0.70892 | error: 0.22656\n",
      "  Ended batch 012 / 034 | loss: 0.84761 | error: 0.48438\n",
      "  Ended batch 013 / 034 | loss: 0.75939 | error: 0.28906\n",
      "  Ended batch 014 / 034 | loss: 0.84572 | error: 0.28125\n",
      "  Ended batch 015 / 034 | loss: 0.77897 | error: 0.42188\n",
      "  Ended batch 016 / 034 | loss: 0.75004 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.80460 | error: 0.43750\n",
      "  Ended batch 018 / 034 | loss: 0.74354 | error: 0.39062\n",
      "  Ended batch 019 / 034 | loss: 0.71565 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.85186 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.83242 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.70801 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.75693 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.83578 | error: 0.44531\n",
      "  Ended batch 025 / 034 | loss: 0.85512 | error: 0.45312\n",
      "  Ended batch 026 / 034 | loss: 0.81472 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.80310 | error: 0.42188\n",
      "  Ended batch 028 / 034 | loss: 0.83285 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.76909 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.71153 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.76421 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.94407 | error: 0.49219\n",
      "  Ended batch 033 / 034 | loss: 0.81738 | error: 0.42188\n",
      "  Ended batch 034 / 034 | loss: 0.95061 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 066 sec | loss: 0.88975 | error: 0.34561\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.81484 | error: 0.34375\n",
      "  Ended batch 002 / 034 | loss: 0.74709 | error: 0.24219\n",
      "  Ended batch 003 / 034 | loss: 0.77862 | error: 0.37500\n",
      "  Ended batch 004 / 034 | loss: 0.72359 | error: 0.36719\n",
      "  Ended batch 005 / 034 | loss: 0.77054 | error: 0.40625\n",
      "  Ended batch 006 / 034 | loss: 0.71209 | error: 0.38281\n",
      "  Ended batch 007 / 034 | loss: 1.00536 | error: 0.54688\n",
      "  Ended batch 008 / 034 | loss: 0.79594 | error: 0.42969\n",
      "  Ended batch 009 / 034 | loss: 0.80772 | error: 0.44531\n",
      "  Ended batch 010 / 034 | loss: 0.72543 | error: 0.30469\n",
      "  Ended batch 011 / 034 | loss: 0.70237 | error: 0.23438\n",
      "  Ended batch 012 / 034 | loss: 0.84969 | error: 0.50000\n",
      "  Ended batch 013 / 034 | loss: 0.77230 | error: 0.36719\n",
      "  Ended batch 014 / 034 | loss: 0.81598 | error: 0.36719\n",
      "  Ended batch 015 / 034 | loss: 0.80635 | error: 0.43750\n",
      "  Ended batch 016 / 034 | loss: 0.75849 | error: 0.39844\n",
      "  Ended batch 017 / 034 | loss: 0.82328 | error: 0.43750\n",
      "  Ended batch 018 / 034 | loss: 0.72709 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.72700 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.84583 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.82353 | error: 0.43750\n",
      "  Ended batch 022 / 034 | loss: 0.70454 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.76489 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.83688 | error: 0.44531\n",
      "  Ended batch 025 / 034 | loss: 0.82584 | error: 0.43750\n",
      "  Ended batch 026 / 034 | loss: 0.80738 | error: 0.40625\n",
      "  Ended batch 027 / 034 | loss: 0.73515 | error: 0.35156\n",
      "  Ended batch 028 / 034 | loss: 0.80287 | error: 0.41406\n",
      "  Ended batch 029 / 034 | loss: 0.77018 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.70548 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.76180 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.93532 | error: 0.48438\n",
      "  Ended batch 033 / 034 | loss: 0.77229 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.95497 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 072 sec | loss: 0.93481 | error: 0.36058\n",
      "Starting epoch 006 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.82446 | error: 0.42188\n",
      "  Ended batch 002 / 034 | loss: 0.77469 | error: 0.34375\n",
      "  Ended batch 003 / 034 | loss: 0.78101 | error: 0.37500\n",
      "  Ended batch 004 / 034 | loss: 0.69014 | error: 0.28125\n",
      "  Ended batch 005 / 034 | loss: 0.73826 | error: 0.34375\n",
      "  Ended batch 006 / 034 | loss: 0.68594 | error: 0.35938\n",
      "  Ended batch 007 / 034 | loss: 1.01255 | error: 0.53906\n",
      "  Ended batch 008 / 034 | loss: 0.80203 | error: 0.42969\n",
      "  Ended batch 009 / 034 | loss: 0.84273 | error: 0.46094\n",
      "  Ended batch 010 / 034 | loss: 0.71785 | error: 0.35156\n",
      "  Ended batch 011 / 034 | loss: 0.70414 | error: 0.27344\n",
      "  Ended batch 012 / 034 | loss: 0.85073 | error: 0.50000\n",
      "  Ended batch 013 / 034 | loss: 0.76483 | error: 0.32031\n",
      "  Ended batch 014 / 034 | loss: 0.85280 | error: 0.29688\n",
      "  Ended batch 015 / 034 | loss: 0.76962 | error: 0.40625\n",
      "  Ended batch 016 / 034 | loss: 0.74904 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.81142 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.75095 | error: 0.39062\n",
      "  Ended batch 019 / 034 | loss: 0.72028 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.83685 | error: 0.42969\n",
      "  Ended batch 021 / 034 | loss: 0.80000 | error: 0.42188\n",
      "  Ended batch 022 / 034 | loss: 0.69873 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.77671 | error: 0.40625\n",
      "  Ended batch 024 / 034 | loss: 0.84480 | error: 0.44531\n",
      "  Ended batch 025 / 034 | loss: 0.82835 | error: 0.43750\n",
      "  Ended batch 026 / 034 | loss: 0.82965 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.74622 | error: 0.36719\n",
      "  Ended batch 028 / 034 | loss: 0.81067 | error: 0.42188\n",
      "  Ended batch 029 / 034 | loss: 0.75607 | error: 0.41406\n",
      "  Ended batch 030 / 034 | loss: 0.72459 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.77485 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.93842 | error: 0.49219\n",
      "  Ended batch 033 / 034 | loss: 0.83397 | error: 0.42969\n",
      "  Ended batch 034 / 034 | loss: 0.95117 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 006 / 016 | time: 071 sec | loss: 0.92313 | error: 0.35798\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 6.706931976477305 minutes (402.4159185886383 seconds).\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.06_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.06_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "---------- Calculating results ----------\n",
      "Evaluating using epsilon: 0.06\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0903, -0.2800, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "         -0.0746,  0.0482, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919,  0.1656,\n",
      "          0.3650,  0.3795, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "          0.0652, -0.1662, -0.0495,  0.3771, -0.0304, -0.0473, -0.0763]])\n",
      "Evaluating using epsilon: 0.06\n",
      "Evaluating on X_opt=\n",
      "tensor([[-1.1071e-01, -1.7059e-01, -1.0206e-01, -1.1143e-01, -4.0991e-02,\n",
      "         -8.6991e-02, -1.0665e-01, -9.1804e-02, -4.3146e-02,  1.6595e-02,\n",
      "         -1.2501e-01, -2.0220e-01, -8.3247e-02, -5.8056e-02, -9.9029e-02],\n",
      "        [-4.7587e-01,  2.1441e+00, -5.3591e-01, -5.6178e-01, -8.0975e-04,\n",
      "         -3.9267e-01, -3.8587e-01,  8.1658e-01,  9.9026e-01, -2.4769e-01,\n",
      "         -8.3604e-01, -4.2702e-01, -2.4367e-01, -1.6614e-01, -5.2754e-01],\n",
      "        [-9.8100e-02,  1.1480e-01, -1.0962e-01, -1.1928e-01, -4.0290e-02,\n",
      "         -9.2320e-02, -1.1152e-01, -8.1026e-02,  1.0702e-01, -2.0816e-01,\n",
      "         -9.9613e-02,  3.6124e-01, -4.1739e-02, -5.4927e-02, -1.0650e-01]])\n",
      "Evaluating using epsilon: 0.06\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1047, -0.1955, -0.0950, -0.1031, -0.0470, -0.0803, -0.1016, -0.1027,\n",
      "         -0.0549,  0.0323, -0.1087, -0.1960, -0.0824, -0.0529, -0.0868],\n",
      "        [-0.4382,  2.1363, -0.4868, -0.4852, -0.1426, -0.3148, -0.3424,  0.7255,\n",
      "          0.8715, -0.0261, -0.6360, -0.3376, -0.2722, -0.0677, -0.3461],\n",
      "        [-0.0907,  0.0852, -0.1009, -0.1088, -0.0484, -0.0838, -0.1053, -0.0946,\n",
      "          0.0923, -0.1879, -0.0789,  0.3692, -0.0410, -0.0481, -0.0908]])\n",
      "Evaluating using epsilon: 0.06\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1213,  0.4182, -0.3359, -0.3306, -0.3496, -0.2505, -0.1535, -0.2236,\n",
      "         -0.1516, -0.0781, -0.4841, -0.1946, -0.0601,  0.1617, -0.1769],\n",
      "        [-0.0989,  0.5981, -0.3104, -0.3064, -0.3191, -0.2326, -0.1473,  0.0765,\n",
      "          0.2956,  0.2658, -0.4446, -0.1942, -0.0616,  0.1404, -0.1669],\n",
      "        [-0.0980,  0.5772, -0.3027, -0.2991, -0.3099, -0.2272, -0.1455, -0.2108,\n",
      "         -0.0018, -0.2761, -0.3949,  0.3733, -0.0177,  0.1390, -0.1639]])\n",
      "------------------------- 0.08 -------------------------\n",
      "---------- Training non-strategically with epsilon=0.08 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98923 | error: 0.43750\n",
      "  Ended batch 002 / 034 | loss: 0.83193 | error: 0.39844\n",
      "  Ended batch 003 / 034 | loss: 0.75601 | error: 0.38281\n",
      "  Ended batch 004 / 034 | loss: 0.62267 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.53573 | error: 0.26562\n",
      "  Ended batch 006 / 034 | loss: 0.43920 | error: 0.17188\n",
      "  Ended batch 007 / 034 | loss: 0.55319 | error: 0.24219\n",
      "  Ended batch 008 / 034 | loss: 0.43821 | error: 0.17188\n",
      "  Ended batch 009 / 034 | loss: 0.48477 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.44141 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.34122 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.39537 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.46204 | error: 0.17969\n",
      "  Ended batch 014 / 034 | loss: 0.61706 | error: 0.21094\n",
      "  Ended batch 015 / 034 | loss: 0.40735 | error: 0.15625\n",
      "  Ended batch 016 / 034 | loss: 0.44818 | error: 0.19531\n",
      "  Ended batch 017 / 034 | loss: 0.42789 | error: 0.17969\n",
      "  Ended batch 018 / 034 | loss: 0.45081 | error: 0.14062\n",
      "  Ended batch 019 / 034 | loss: 0.40077 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.67522 | error: 0.20312\n",
      "  Ended batch 021 / 034 | loss: 0.44442 | error: 0.17188\n",
      "  Ended batch 022 / 034 | loss: 0.49886 | error: 0.20312\n",
      "  Ended batch 023 / 034 | loss: 0.43399 | error: 0.16406\n",
      "  Ended batch 024 / 034 | loss: 0.42774 | error: 0.17188\n",
      "  Ended batch 025 / 034 | loss: 0.49798 | error: 0.17969\n",
      "  Ended batch 026 / 034 | loss: 0.48518 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34755 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.36209 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30258 | error: 0.14062\n",
      "  Ended batch 030 / 034 | loss: 0.27834 | error: 0.09375\n",
      "  Ended batch 031 / 034 | loss: 0.31573 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.46953 | error: 0.21094\n",
      "  Ended batch 033 / 034 | loss: 0.44510 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.35380 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 000 sec | loss: 0.45466 | error: 0.21103\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/non_strategic_0.08_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.40293 | error: 0.16406\n",
      "  Ended batch 002 / 034 | loss: 0.39698 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39748 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.33817 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31394 | error: 0.13281\n",
      "  Ended batch 006 / 034 | loss: 0.26311 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.40746 | error: 0.21094\n",
      "  Ended batch 008 / 034 | loss: 0.28619 | error: 0.12500\n",
      "  Ended batch 009 / 034 | loss: 0.42815 | error: 0.21875\n",
      "  Ended batch 010 / 034 | loss: 0.37308 | error: 0.14062\n",
      "  Ended batch 011 / 034 | loss: 0.24522 | error: 0.10938\n",
      "  Ended batch 012 / 034 | loss: 0.35200 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.38305 | error: 0.16406\n",
      "  Ended batch 014 / 034 | loss: 0.48793 | error: 0.18750\n",
      "  Ended batch 015 / 034 | loss: 0.29911 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37009 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39094 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.39047 | error: 0.16406\n",
      "  Ended batch 019 / 034 | loss: 0.33128 | error: 0.14844\n",
      "  Ended batch 020 / 034 | loss: 0.62716 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.39453 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.42599 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.40997 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38743 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.49943 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.46154 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34475 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.35799 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30337 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29895 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31889 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44511 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40606 | error: 0.16406\n",
      "  Ended batch 034 / 034 | loss: 0.41604 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 000 sec | loss: 0.44767 | error: 0.21363\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37960 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.38054 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39798 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.32809 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31529 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26527 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.39605 | error: 0.20312\n",
      "  Ended batch 008 / 034 | loss: 0.29298 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42259 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.37166 | error: 0.12500\n",
      "  Ended batch 011 / 034 | loss: 0.24852 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.34251 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37322 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47350 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31297 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.36932 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39082 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38728 | error: 0.15625\n",
      "  Ended batch 019 / 034 | loss: 0.33095 | error: 0.13281\n",
      "  Ended batch 020 / 034 | loss: 0.63589 | error: 0.24219\n",
      "  Ended batch 021 / 034 | loss: 0.39093 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41775 | error: 0.18750\n",
      "  Ended batch 023 / 034 | loss: 0.40417 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38116 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48604 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.45087 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34605 | error: 0.13281\n",
      "  Ended batch 028 / 034 | loss: 0.36658 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30254 | error: 0.10938\n",
      "  Ended batch 030 / 034 | loss: 0.30504 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31964 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44560 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40693 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.43542 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 000 sec | loss: 0.44874 | error: 0.21168\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37193 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37114 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39974 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31597 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31782 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26514 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.39098 | error: 0.18750\n",
      "  Ended batch 008 / 034 | loss: 0.28986 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42376 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.36756 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24499 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34379 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37043 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47858 | error: 0.20312\n",
      "  Ended batch 015 / 034 | loss: 0.30862 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37446 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38862 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38593 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32783 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64187 | error: 0.25000\n",
      "  Ended batch 021 / 034 | loss: 0.38988 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41519 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39861 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37473 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48543 | error: 0.21094\n",
      "  Ended batch 026 / 034 | loss: 0.44854 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34043 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36239 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.29949 | error: 0.11719\n",
      "  Ended batch 030 / 034 | loss: 0.29681 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31391 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45120 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40857 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42124 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 000 sec | loss: 0.44502 | error: 0.21103\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37063 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37067 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39355 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31063 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31461 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26898 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.38740 | error: 0.19531\n",
      "  Ended batch 008 / 034 | loss: 0.28760 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42075 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.36434 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24144 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34192 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.36529 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47503 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31060 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37830 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38680 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38255 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32599 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64071 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.38804 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41321 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39709 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37160 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48514 | error: 0.20312\n",
      "  Ended batch 026 / 034 | loss: 0.44696 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34037 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36098 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30023 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29524 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31327 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45363 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40813 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42413 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 000 sec | loss: 0.44367 | error: 0.21168\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 0.7290005683898926 seconds.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.08_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.08_model.pt.\n",
      "---------- Training strategically with epsilon=0.08 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.93775 | error: 0.46094\n",
      "  Ended batch 002 / 034 | loss: 1.27382 | error: 0.48438\n",
      "  Ended batch 003 / 034 | loss: 1.23696 | error: 0.52344\n",
      "  Ended batch 004 / 034 | loss: 0.95727 | error: 0.50781\n",
      "  Ended batch 005 / 034 | loss: 0.92777 | error: 0.44531\n",
      "  Ended batch 006 / 034 | loss: 0.87086 | error: 0.46094\n",
      "  Ended batch 007 / 034 | loss: 1.28292 | error: 0.58594\n",
      "  Ended batch 008 / 034 | loss: 1.01561 | error: 0.49219\n",
      "  Ended batch 009 / 034 | loss: 0.98033 | error: 0.53906\n",
      "  Ended batch 010 / 034 | loss: 0.81507 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.78348 | error: 0.18750\n",
      "  Ended batch 012 / 034 | loss: 0.96367 | error: 0.57031\n",
      "  Ended batch 013 / 034 | loss: 0.80140 | error: 0.40625\n",
      "  Ended batch 014 / 034 | loss: 0.82636 | error: 0.38281\n",
      "  Ended batch 015 / 034 | loss: 0.82137 | error: 0.44531\n",
      "  Ended batch 016 / 034 | loss: 0.81593 | error: 0.42969\n",
      "  Ended batch 017 / 034 | loss: 0.83623 | error: 0.43750\n",
      "  Ended batch 018 / 034 | loss: 0.77173 | error: 0.39844\n",
      "  Ended batch 019 / 034 | loss: 0.74409 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.86852 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.84814 | error: 0.43750\n",
      "  Ended batch 022 / 034 | loss: 0.66718 | error: 0.32812\n",
      "  Ended batch 023 / 034 | loss: 0.77439 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.79370 | error: 0.41406\n",
      "  Ended batch 025 / 034 | loss: 0.83824 | error: 0.42969\n",
      "  Ended batch 026 / 034 | loss: 0.84278 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.71283 | error: 0.35156\n",
      "  Ended batch 028 / 034 | loss: 0.83968 | error: 0.42969\n",
      "  Ended batch 029 / 034 | loss: 0.72137 | error: 0.37500\n",
      "  Ended batch 030 / 034 | loss: 0.68866 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.67553 | error: 0.33594\n",
      "  Ended batch 032 / 034 | loss: 0.80134 | error: 0.41406\n",
      "  Ended batch 033 / 034 | loss: 0.73017 | error: 0.36719\n",
      "  Ended batch 034 / 034 | loss: 0.97425 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 075 sec | loss: 1.29795 | error: 0.47712\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.08_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.74502 | error: 0.37500\n",
      "  Ended batch 002 / 034 | loss: 0.71343 | error: 0.36719\n",
      "  Ended batch 003 / 034 | loss: 0.79854 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.65613 | error: 0.34375\n",
      "  Ended batch 005 / 034 | loss: 0.70297 | error: 0.36719\n",
      "  Ended batch 006 / 034 | loss: 0.59017 | error: 0.30469\n",
      "  Ended batch 007 / 034 | loss: 0.96134 | error: 0.50000\n",
      "  Ended batch 008 / 034 | loss: 0.71441 | error: 0.37500\n",
      "  Ended batch 009 / 034 | loss: 0.76971 | error: 0.40625\n",
      "  Ended batch 010 / 034 | loss: 0.65411 | error: 0.31250\n",
      "  Ended batch 011 / 034 | loss: 0.64320 | error: 0.32031\n",
      "  Ended batch 012 / 034 | loss: 0.89875 | error: 0.48438\n",
      "  Ended batch 013 / 034 | loss: 0.77153 | error: 0.39844\n",
      "  Ended batch 014 / 034 | loss: 0.76763 | error: 0.37500\n",
      "  Ended batch 015 / 034 | loss: 0.78123 | error: 0.41406\n",
      "  Ended batch 016 / 034 | loss: 0.70560 | error: 0.36719\n",
      "  Ended batch 017 / 034 | loss: 0.70754 | error: 0.37500\n",
      "  Ended batch 018 / 034 | loss: 0.68704 | error: 0.35156\n",
      "  Ended batch 019 / 034 | loss: 0.68694 | error: 0.35938\n",
      "  Ended batch 020 / 034 | loss: 0.88902 | error: 0.44531\n",
      "  Ended batch 021 / 034 | loss: 0.81905 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.70087 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.75823 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.76727 | error: 0.40625\n",
      "  Ended batch 025 / 034 | loss: 0.79701 | error: 0.41406\n",
      "  Ended batch 026 / 034 | loss: 0.83002 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.76139 | error: 0.40625\n",
      "  Ended batch 028 / 034 | loss: 0.81918 | error: 0.42969\n",
      "  Ended batch 029 / 034 | loss: 0.67813 | error: 0.35156\n",
      "  Ended batch 030 / 034 | loss: 0.70363 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.76184 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.90186 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.79929 | error: 0.40625\n",
      "  Ended batch 034 / 034 | loss: 0.95355 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 074 sec | loss: 0.95818 | error: 0.35863\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.08_model.pt.\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80680 | error: 0.41406\n",
      "  Ended batch 002 / 034 | loss: 0.73472 | error: 0.36719\n",
      "  Ended batch 003 / 034 | loss: 0.77607 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.67751 | error: 0.34375\n",
      "  Ended batch 005 / 034 | loss: 0.71998 | error: 0.36719\n",
      "  Ended batch 006 / 034 | loss: 0.63555 | error: 0.32812\n",
      "  Ended batch 007 / 034 | loss: 0.98444 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.79011 | error: 0.42188\n",
      "  Ended batch 009 / 034 | loss: 0.80551 | error: 0.43750\n",
      "  Ended batch 010 / 034 | loss: 0.71271 | error: 0.35156\n",
      "  Ended batch 011 / 034 | loss: 0.70061 | error: 0.34375\n",
      "  Ended batch 012 / 034 | loss: 0.87945 | error: 0.50000\n",
      "  Ended batch 013 / 034 | loss: 0.77871 | error: 0.41406\n",
      "  Ended batch 014 / 034 | loss: 0.81086 | error: 0.39844\n",
      "  Ended batch 015 / 034 | loss: 0.76983 | error: 0.41406\n",
      "  Ended batch 016 / 034 | loss: 0.74210 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.79619 | error: 0.43750\n",
      "  Ended batch 018 / 034 | loss: 0.72222 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.70997 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.85520 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.81647 | error: 0.43750\n",
      "  Ended batch 022 / 034 | loss: 0.70267 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.75276 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.81933 | error: 0.44531\n",
      "  Ended batch 025 / 034 | loss: 0.85379 | error: 0.46094\n",
      "  Ended batch 026 / 034 | loss: 0.81513 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.77368 | error: 0.41406\n",
      "  Ended batch 028 / 034 | loss: 0.81910 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.70531 | error: 0.33594\n",
      "  Ended batch 030 / 034 | loss: 0.71174 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.76387 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.88811 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.81916 | error: 0.42188\n",
      "  Ended batch 034 / 034 | loss: 0.95203 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 070 sec | loss: 0.76449 | error: 0.27269\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.08_model.pt.\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.81100 | error: 0.42188\n",
      "  Ended batch 002 / 034 | loss: 0.76779 | error: 0.35938\n",
      "  Ended batch 003 / 034 | loss: 0.76953 | error: 0.36719\n",
      "  Ended batch 004 / 034 | loss: 0.66214 | error: 0.25000\n",
      "  Ended batch 005 / 034 | loss: 0.73332 | error: 0.33594\n",
      "  Ended batch 006 / 034 | loss: 0.67435 | error: 0.35156\n",
      "  Ended batch 007 / 034 | loss: 0.97042 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.78369 | error: 0.42188\n",
      "  Ended batch 009 / 034 | loss: 0.82366 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.71403 | error: 0.35938\n",
      "  Ended batch 011 / 034 | loss: 0.68808 | error: 0.28906\n",
      "  Ended batch 012 / 034 | loss: 0.84450 | error: 0.50000\n",
      "  Ended batch 013 / 034 | loss: 0.74790 | error: 0.28125\n",
      "  Ended batch 014 / 034 | loss: 0.84010 | error: 0.28125\n",
      "  Ended batch 015 / 034 | loss: 0.76249 | error: 0.39844\n",
      "  Ended batch 016 / 034 | loss: 0.74289 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.80500 | error: 0.43750\n",
      "  Ended batch 018 / 034 | loss: 0.71954 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.71067 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.86012 | error: 0.44531\n",
      "  Ended batch 021 / 034 | loss: 0.79282 | error: 0.42188\n",
      "  Ended batch 022 / 034 | loss: 0.70637 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.75192 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.83071 | error: 0.44531\n",
      "  Ended batch 025 / 034 | loss: 0.81247 | error: 0.42969\n",
      "  Ended batch 026 / 034 | loss: 0.82051 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.78109 | error: 0.41406\n",
      "  Ended batch 028 / 034 | loss: 0.82053 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.74854 | error: 0.41406\n",
      "  Ended batch 030 / 034 | loss: 0.72023 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.76321 | error: 0.39062\n",
      "  Ended batch 032 / 034 | loss: 0.87193 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.79075 | error: 0.40625\n",
      "  Ended batch 034 / 034 | loss: 0.96256 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 072 sec | loss: 0.73585 | error: 0.26163\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.08_model.pt.\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.78120 | error: 0.41406\n",
      "  Ended batch 002 / 034 | loss: 0.79475 | error: 0.42969\n",
      "  Ended batch 003 / 034 | loss: 0.79835 | error: 0.42969\n",
      "  Ended batch 004 / 034 | loss: 0.74777 | error: 0.41406\n",
      "  Ended batch 005 / 034 | loss: 0.75264 | error: 0.39062\n",
      "  Ended batch 006 / 034 | loss: 0.66208 | error: 0.29688\n",
      "  Ended batch 007 / 034 | loss: 0.91044 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.73039 | error: 0.38281\n",
      "  Ended batch 009 / 034 | loss: 0.77501 | error: 0.38281\n",
      "  Ended batch 010 / 034 | loss: 0.70499 | error: 0.28906\n",
      "  Ended batch 011 / 034 | loss: 0.68923 | error: 0.29688\n",
      "  Ended batch 012 / 034 | loss: 0.88985 | error: 0.50781\n",
      "  Ended batch 013 / 034 | loss: 0.79742 | error: 0.42969\n",
      "  Ended batch 014 / 034 | loss: 0.77921 | error: 0.38281\n",
      "  Ended batch 015 / 034 | loss: 0.81632 | error: 0.44531\n",
      "  Ended batch 016 / 034 | loss: 0.74058 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.78122 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.71660 | error: 0.36719\n",
      "  Ended batch 019 / 034 | loss: 0.71699 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.85975 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.83640 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.69506 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.75258 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.82577 | error: 0.45312\n",
      "  Ended batch 025 / 034 | loss: 0.83114 | error: 0.44531\n",
      "  Ended batch 026 / 034 | loss: 0.81626 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.78531 | error: 0.42969\n",
      "  Ended batch 028 / 034 | loss: 0.81991 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.69509 | error: 0.32031\n",
      "  Ended batch 030 / 034 | loss: 0.70972 | error: 0.33594\n",
      "  Ended batch 031 / 034 | loss: 0.75988 | error: 0.39062\n",
      "  Ended batch 032 / 034 | loss: 0.88497 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.78842 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.96728 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 071 sec | loss: 0.76137 | error: 0.27139\n",
      "Starting epoch 006 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.81020 | error: 0.42188\n",
      "  Ended batch 002 / 034 | loss: 0.75912 | error: 0.35156\n",
      "  Ended batch 003 / 034 | loss: 0.76646 | error: 0.35938\n",
      "  Ended batch 004 / 034 | loss: 0.65718 | error: 0.24219\n",
      "  Ended batch 005 / 034 | loss: 0.72878 | error: 0.32812\n",
      "  Ended batch 006 / 034 | loss: 0.64053 | error: 0.32812\n",
      "  Ended batch 007 / 034 | loss: 0.99092 | error: 0.53125\n",
      "  Ended batch 008 / 034 | loss: 0.80284 | error: 0.42969\n",
      "  Ended batch 009 / 034 | loss: 0.82970 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.71377 | error: 0.35938\n",
      "  Ended batch 011 / 034 | loss: 0.69529 | error: 0.32812\n",
      "  Ended batch 012 / 034 | loss: 0.84990 | error: 0.50000\n",
      "  Ended batch 013 / 034 | loss: 0.74981 | error: 0.32812\n",
      "  Ended batch 014 / 034 | loss: 0.81549 | error: 0.33594\n",
      "  Ended batch 015 / 034 | loss: 0.77108 | error: 0.41406\n",
      "  Ended batch 016 / 034 | loss: 0.74415 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.81450 | error: 0.43750\n",
      "  Ended batch 018 / 034 | loss: 0.74253 | error: 0.39062\n",
      "  Ended batch 019 / 034 | loss: 0.71725 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.84530 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.82919 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.70303 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.76767 | error: 0.40625\n",
      "  Ended batch 024 / 034 | loss: 0.85414 | error: 0.46094\n",
      "  Ended batch 025 / 034 | loss: 0.83112 | error: 0.44531\n",
      "  Ended batch 026 / 034 | loss: 0.82475 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.73068 | error: 0.28125\n",
      "  Ended batch 028 / 034 | loss: 0.77361 | error: 0.29688\n",
      "  Ended batch 029 / 034 | loss: 0.67206 | error: 0.28906\n",
      "  Ended batch 030 / 034 | loss: 0.70093 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.75336 | error: 0.39062\n",
      "  Ended batch 032 / 034 | loss: 0.94032 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.77228 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.94907 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 006 / 016 | time: 083 sec | loss: 0.97338 | error: 0.38012\n",
      "Starting epoch 007 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.78645 | error: 0.36719\n",
      "  Ended batch 002 / 034 | loss: 0.72679 | error: 0.34375\n",
      "  Ended batch 003 / 034 | loss: 0.77496 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.71923 | error: 0.39062\n",
      "  Ended batch 005 / 034 | loss: 0.75494 | error: 0.39844\n",
      "  Ended batch 006 / 034 | loss: 0.70567 | error: 0.38281\n",
      "  Ended batch 007 / 034 | loss: 0.94502 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.73096 | error: 0.36719\n",
      "  Ended batch 009 / 034 | loss: 0.74300 | error: 0.25000\n",
      "  Ended batch 010 / 034 | loss: 0.75514 | error: 0.23438\n",
      "  Ended batch 011 / 034 | loss: 0.69753 | error: 0.34375\n",
      "  Ended batch 012 / 034 | loss: 1.07040 | error: 0.58594\n",
      "  Ended batch 013 / 034 | loss: 0.81092 | error: 0.42969\n",
      "  Ended batch 014 / 034 | loss: 0.82327 | error: 0.41406\n",
      "  Ended batch 015 / 034 | loss: 0.83038 | error: 0.45312\n",
      "  Ended batch 016 / 034 | loss: 0.74465 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.78089 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.73434 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.71235 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.87446 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.85335 | error: 0.45312\n",
      "  Ended batch 022 / 034 | loss: 0.69939 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.75846 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.75432 | error: 0.39844\n",
      "  Ended batch 025 / 034 | loss: 0.80570 | error: 0.42188\n",
      "  Ended batch 026 / 034 | loss: 0.81835 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.78569 | error: 0.42188\n",
      "  Ended batch 028 / 034 | loss: 0.84055 | error: 0.44531\n",
      "  Ended batch 029 / 034 | loss: 0.76896 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.69006 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.75455 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.88534 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.75125 | error: 0.37500\n",
      "  Ended batch 034 / 034 | loss: 0.94652 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 007 / 016 | time: 081 sec | loss: 0.78838 | error: 0.28116\n",
      "Starting epoch 008 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80390 | error: 0.41406\n",
      "  Ended batch 002 / 034 | loss: 0.78473 | error: 0.42188\n",
      "  Ended batch 003 / 034 | loss: 0.77666 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.74559 | error: 0.41406\n",
      "  Ended batch 005 / 034 | loss: 0.74846 | error: 0.39844\n",
      "  Ended batch 006 / 034 | loss: 0.67322 | error: 0.35938\n",
      "  Ended batch 007 / 034 | loss: 0.93840 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.78364 | error: 0.42969\n",
      "  Ended batch 009 / 034 | loss: 0.78780 | error: 0.43750\n",
      "  Ended batch 010 / 034 | loss: 0.72219 | error: 0.31250\n",
      "  Ended batch 011 / 034 | loss: 0.69147 | error: 0.22656\n",
      "  Ended batch 012 / 034 | loss: 0.83572 | error: 0.47656\n",
      "  Ended batch 013 / 034 | loss: 0.75085 | error: 0.35938\n",
      "  Ended batch 014 / 034 | loss: 0.82948 | error: 0.35156\n",
      "  Ended batch 015 / 034 | loss: 0.77597 | error: 0.40625\n",
      "  Ended batch 016 / 034 | loss: 0.74261 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.79634 | error: 0.44531\n",
      "  Ended batch 018 / 034 | loss: 0.72213 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.71040 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.88563 | error: 0.45312\n",
      "  Ended batch 021 / 034 | loss: 0.82832 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.70599 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.75311 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.81287 | error: 0.44531\n",
      "  Ended batch 025 / 034 | loss: 0.83365 | error: 0.44531\n",
      "  Ended batch 026 / 034 | loss: 0.81620 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.77350 | error: 0.42188\n",
      "  Ended batch 028 / 034 | loss: 0.82873 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.76653 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.68412 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.75583 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.89444 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.77817 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.94458 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 008 / 016 | time: 088 sec | loss: 0.82074 | error: 0.29808\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 10.238763356208802 minutes (614.3258013725281 seconds).\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.08_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.08_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "---------- Calculating results ----------\n",
      "Evaluating using epsilon: 0.08\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0903, -0.2800, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "         -0.0746,  0.0482, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919,  0.1656,\n",
      "          0.3650,  0.3795, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "          0.0652, -0.1662, -0.0495,  0.3771, -0.0304, -0.0473, -0.0763]])\n",
      "Evaluating using epsilon: 0.08\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1049, -0.2018, -0.0955, -0.1046, -0.0416, -0.0824, -0.1024, -0.1012,\n",
      "         -0.0521,  0.0256, -0.1143, -0.1988, -0.0808, -0.0564, -0.0925],\n",
      "        [-0.4573,  1.8667, -0.5114, -0.5353, -0.0336, -0.3836, -0.3800,  0.7698,\n",
      "          0.9464, -0.1730, -0.7222, -0.3971, -0.2215, -0.1252, -0.4730],\n",
      "        [-0.0899,  0.0705, -0.1003, -0.1096, -0.0412, -0.0858, -0.1055, -0.0943,\n",
      "          0.0943, -0.1954, -0.0844,  0.3661, -0.0383, -0.0526, -0.0973]])\n",
      "Evaluating using epsilon: 0.08\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1038, -0.1970, -0.0946, -0.1027, -0.0459, -0.0804, -0.1023, -0.0944,\n",
      "         -0.0498,  0.0239, -0.1184, -0.1943, -0.0863, -0.0548, -0.0892],\n",
      "        [-0.3272,  1.5550, -0.3751, -0.3766, -0.0956, -0.2543, -0.2915,  0.7408,\n",
      "          0.8393, -0.0835, -0.6798, -0.2662, -0.2961, -0.0999, -0.3228],\n",
      "        [-0.0938,  0.1103, -0.1054, -0.1133, -0.0478, -0.0871, -0.1096, -0.0734,\n",
      "          0.1074, -0.2074, -0.1022,  0.3703, -0.0501, -0.0515, -0.0982]])\n",
      "Evaluating using epsilon: 0.08\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1194,  0.3748, -0.3200, -0.3155, -0.3306, -0.2394, -0.1496, -0.2174,\n",
      "         -0.1468, -0.0703, -0.4595, -0.1944, -0.0610,  0.1484, -0.1706],\n",
      "        [-0.0970,  0.5547, -0.2944, -0.2913, -0.3000, -0.2215, -0.1435,  0.0826,\n",
      "          0.3004,  0.2736, -0.4200, -0.1939, -0.0625,  0.1271, -0.1606],\n",
      "        [-0.0961,  0.5338, -0.2867, -0.2840, -0.2909, -0.2161, -0.1417, -0.2046,\n",
      "          0.0030, -0.2683, -0.3703,  0.3736, -0.0186,  0.1257, -0.1576]])\n",
      "------------------------- 0.1 -------------------------\n",
      "---------- Training non-strategically with epsilon=0.1 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98923 | error: 0.43750\n",
      "  Ended batch 002 / 034 | loss: 0.83193 | error: 0.39844\n",
      "  Ended batch 003 / 034 | loss: 0.75601 | error: 0.38281\n",
      "  Ended batch 004 / 034 | loss: 0.62267 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.53573 | error: 0.26562\n",
      "  Ended batch 006 / 034 | loss: 0.43920 | error: 0.17188\n",
      "  Ended batch 007 / 034 | loss: 0.55319 | error: 0.24219\n",
      "  Ended batch 008 / 034 | loss: 0.43821 | error: 0.17188\n",
      "  Ended batch 009 / 034 | loss: 0.48477 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.44141 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.34122 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.39537 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.46204 | error: 0.17969\n",
      "  Ended batch 014 / 034 | loss: 0.61706 | error: 0.21094\n",
      "  Ended batch 015 / 034 | loss: 0.40735 | error: 0.15625\n",
      "  Ended batch 016 / 034 | loss: 0.44818 | error: 0.19531\n",
      "  Ended batch 017 / 034 | loss: 0.42789 | error: 0.17969\n",
      "  Ended batch 018 / 034 | loss: 0.45081 | error: 0.14062\n",
      "  Ended batch 019 / 034 | loss: 0.40077 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.67522 | error: 0.20312\n",
      "  Ended batch 021 / 034 | loss: 0.44442 | error: 0.17188\n",
      "  Ended batch 022 / 034 | loss: 0.49886 | error: 0.20312\n",
      "  Ended batch 023 / 034 | loss: 0.43399 | error: 0.16406\n",
      "  Ended batch 024 / 034 | loss: 0.42774 | error: 0.17188\n",
      "  Ended batch 025 / 034 | loss: 0.49798 | error: 0.17969\n",
      "  Ended batch 026 / 034 | loss: 0.48518 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34755 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.36209 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30258 | error: 0.14062\n",
      "  Ended batch 030 / 034 | loss: 0.27834 | error: 0.09375\n",
      "  Ended batch 031 / 034 | loss: 0.31573 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.46953 | error: 0.21094\n",
      "  Ended batch 033 / 034 | loss: 0.44510 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.35380 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 000 sec | loss: 0.45466 | error: 0.21103\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/non_strategic_0.1_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.40293 | error: 0.16406\n",
      "  Ended batch 002 / 034 | loss: 0.39698 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39748 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.33817 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31394 | error: 0.13281\n",
      "  Ended batch 006 / 034 | loss: 0.26311 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.40746 | error: 0.21094\n",
      "  Ended batch 008 / 034 | loss: 0.28619 | error: 0.12500\n",
      "  Ended batch 009 / 034 | loss: 0.42815 | error: 0.21875\n",
      "  Ended batch 010 / 034 | loss: 0.37308 | error: 0.14062\n",
      "  Ended batch 011 / 034 | loss: 0.24522 | error: 0.10938\n",
      "  Ended batch 012 / 034 | loss: 0.35200 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.38305 | error: 0.16406\n",
      "  Ended batch 014 / 034 | loss: 0.48793 | error: 0.18750\n",
      "  Ended batch 015 / 034 | loss: 0.29911 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37009 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39094 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.39047 | error: 0.16406\n",
      "  Ended batch 019 / 034 | loss: 0.33128 | error: 0.14844\n",
      "  Ended batch 020 / 034 | loss: 0.62716 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.39453 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.42599 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.40997 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38743 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.49943 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.46154 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34475 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.35799 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30337 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29895 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31889 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44511 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40606 | error: 0.16406\n",
      "  Ended batch 034 / 034 | loss: 0.41604 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 000 sec | loss: 0.44767 | error: 0.21363\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37960 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.38054 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39798 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.32809 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31529 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26527 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.39605 | error: 0.20312\n",
      "  Ended batch 008 / 034 | loss: 0.29298 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42259 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.37166 | error: 0.12500\n",
      "  Ended batch 011 / 034 | loss: 0.24852 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.34251 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37322 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47350 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31297 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.36932 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39082 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38728 | error: 0.15625\n",
      "  Ended batch 019 / 034 | loss: 0.33095 | error: 0.13281\n",
      "  Ended batch 020 / 034 | loss: 0.63589 | error: 0.24219\n",
      "  Ended batch 021 / 034 | loss: 0.39093 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41775 | error: 0.18750\n",
      "  Ended batch 023 / 034 | loss: 0.40417 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38116 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48604 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.45087 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34605 | error: 0.13281\n",
      "  Ended batch 028 / 034 | loss: 0.36658 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30254 | error: 0.10938\n",
      "  Ended batch 030 / 034 | loss: 0.30504 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31964 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44560 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40693 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.43542 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 000 sec | loss: 0.44874 | error: 0.21168\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37193 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37114 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39974 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31597 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31782 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26514 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.39098 | error: 0.18750\n",
      "  Ended batch 008 / 034 | loss: 0.28986 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42376 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.36756 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24499 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34379 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37043 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47858 | error: 0.20312\n",
      "  Ended batch 015 / 034 | loss: 0.30862 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37446 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38862 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38593 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32783 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64187 | error: 0.25000\n",
      "  Ended batch 021 / 034 | loss: 0.38988 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41519 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39861 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37473 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48543 | error: 0.21094\n",
      "  Ended batch 026 / 034 | loss: 0.44854 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34043 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36239 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.29949 | error: 0.11719\n",
      "  Ended batch 030 / 034 | loss: 0.29681 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31391 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45120 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40857 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42124 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 000 sec | loss: 0.44502 | error: 0.21103\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37063 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37067 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39355 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31063 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31461 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26898 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.38740 | error: 0.19531\n",
      "  Ended batch 008 / 034 | loss: 0.28760 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42075 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.36434 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24144 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34192 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.36529 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47503 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31060 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37830 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38680 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38255 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32599 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64071 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.38804 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41321 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39709 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37160 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48514 | error: 0.20312\n",
      "  Ended batch 026 / 034 | loss: 0.44696 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34037 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36098 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30023 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29524 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31327 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45363 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40813 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42413 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 000 sec | loss: 0.44367 | error: 0.21168\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 0.8230018615722656 seconds.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.1_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.1_model.pt.\n",
      "---------- Training strategically with epsilon=0.1 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.94508 | error: 0.46094\n",
      "  Ended batch 002 / 034 | loss: 1.22271 | error: 0.48438\n",
      "  Ended batch 003 / 034 | loss: 1.18819 | error: 0.52344\n",
      "  Ended batch 004 / 034 | loss: 0.93926 | error: 0.50781\n",
      "  Ended batch 005 / 034 | loss: 0.91444 | error: 0.40625\n",
      "  Ended batch 006 / 034 | loss: 0.85578 | error: 0.45312\n",
      "  Ended batch 007 / 034 | loss: 1.20993 | error: 0.58594\n",
      "  Ended batch 008 / 034 | loss: 0.93969 | error: 0.48438\n",
      "  Ended batch 009 / 034 | loss: 0.94081 | error: 0.50000\n",
      "  Ended batch 010 / 034 | loss: 0.80004 | error: 0.28906\n",
      "  Ended batch 011 / 034 | loss: 0.77006 | error: 0.21875\n",
      "  Ended batch 012 / 034 | loss: 0.92553 | error: 0.51562\n",
      "  Ended batch 013 / 034 | loss: 0.78953 | error: 0.38281\n",
      "  Ended batch 014 / 034 | loss: 0.83917 | error: 0.39062\n",
      "  Ended batch 015 / 034 | loss: 0.82828 | error: 0.44531\n",
      "  Ended batch 016 / 034 | loss: 0.80699 | error: 0.42188\n",
      "  Ended batch 017 / 034 | loss: 0.85809 | error: 0.44531\n",
      "  Ended batch 018 / 034 | loss: 0.77403 | error: 0.39844\n",
      "  Ended batch 019 / 034 | loss: 0.74119 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.81772 | error: 0.40625\n",
      "  Ended batch 021 / 034 | loss: 0.84061 | error: 0.43750\n",
      "  Ended batch 022 / 034 | loss: 0.67790 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.78226 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.84094 | error: 0.42969\n",
      "  Ended batch 025 / 034 | loss: 0.86052 | error: 0.44531\n",
      "  Ended batch 026 / 034 | loss: 0.83630 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.66673 | error: 0.28906\n",
      "  Ended batch 028 / 034 | loss: 0.75688 | error: 0.34375\n",
      "  Ended batch 029 / 034 | loss: 0.60929 | error: 0.29688\n",
      "  Ended batch 030 / 034 | loss: 0.66403 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.74833 | error: 0.36719\n",
      "  Ended batch 032 / 034 | loss: 0.94557 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.72707 | error: 0.36719\n",
      "  Ended batch 034 / 034 | loss: 0.96038 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 073 sec | loss: 1.29451 | error: 0.47842\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.1_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.70536 | error: 0.32812\n",
      "  Ended batch 002 / 034 | loss: 0.56592 | error: 0.22656\n",
      "  Ended batch 003 / 034 | loss: 0.75646 | error: 0.39062\n",
      "  Ended batch 004 / 034 | loss: 0.70629 | error: 0.37500\n",
      "  Ended batch 005 / 034 | loss: 0.75321 | error: 0.39062\n",
      "  Ended batch 006 / 034 | loss: 0.61807 | error: 0.32031\n",
      "  Ended batch 007 / 034 | loss: 0.95834 | error: 0.50781\n",
      "  Ended batch 008 / 034 | loss: 0.66925 | error: 0.34375\n",
      "  Ended batch 009 / 034 | loss: 0.75375 | error: 0.38281\n",
      "  Ended batch 010 / 034 | loss: 0.66968 | error: 0.30469\n",
      "  Ended batch 011 / 034 | loss: 0.61744 | error: 0.28906\n",
      "  Ended batch 012 / 034 | loss: 0.79181 | error: 0.42969\n",
      "  Ended batch 013 / 034 | loss: 0.79145 | error: 0.41406\n",
      "  Ended batch 014 / 034 | loss: 0.77359 | error: 0.38281\n",
      "  Ended batch 015 / 034 | loss: 0.78655 | error: 0.41406\n",
      "  Ended batch 016 / 034 | loss: 0.68993 | error: 0.35938\n",
      "  Ended batch 017 / 034 | loss: 0.69217 | error: 0.36719\n",
      "  Ended batch 018 / 034 | loss: 0.67196 | error: 0.33594\n",
      "  Ended batch 019 / 034 | loss: 0.63925 | error: 0.32031\n",
      "  Ended batch 020 / 034 | loss: 0.82082 | error: 0.41406\n",
      "  Ended batch 021 / 034 | loss: 0.82311 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.66949 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.75302 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.71846 | error: 0.37500\n",
      "  Ended batch 025 / 034 | loss: 0.78150 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.83370 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.73324 | error: 0.39062\n",
      "  Ended batch 028 / 034 | loss: 0.81441 | error: 0.42969\n",
      "  Ended batch 029 / 034 | loss: 0.71009 | error: 0.38281\n",
      "  Ended batch 030 / 034 | loss: 0.68369 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.75527 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.86876 | error: 0.46094\n",
      "  Ended batch 033 / 034 | loss: 0.75681 | error: 0.38281\n",
      "  Ended batch 034 / 034 | loss: 0.94046 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 080 sec | loss: 0.94052 | error: 0.34952\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.1_model.pt.\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.79032 | error: 0.40625\n",
      "  Ended batch 002 / 034 | loss: 0.72394 | error: 0.36719\n",
      "  Ended batch 003 / 034 | loss: 0.77313 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.67671 | error: 0.35938\n",
      "  Ended batch 005 / 034 | loss: 0.72368 | error: 0.38281\n",
      "  Ended batch 006 / 034 | loss: 0.62174 | error: 0.32812\n",
      "  Ended batch 007 / 034 | loss: 0.96852 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.77790 | error: 0.42188\n",
      "  Ended batch 009 / 034 | loss: 0.79864 | error: 0.43750\n",
      "  Ended batch 010 / 034 | loss: 0.70041 | error: 0.35156\n",
      "  Ended batch 011 / 034 | loss: 0.68112 | error: 0.32812\n",
      "  Ended batch 012 / 034 | loss: 0.82686 | error: 0.49219\n",
      "  Ended batch 013 / 034 | loss: 0.74015 | error: 0.35156\n",
      "  Ended batch 014 / 034 | loss: 0.80212 | error: 0.34375\n",
      "  Ended batch 015 / 034 | loss: 0.77129 | error: 0.42188\n",
      "  Ended batch 016 / 034 | loss: 0.73839 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.76224 | error: 0.42188\n",
      "  Ended batch 018 / 034 | loss: 0.72319 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.70628 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.87080 | error: 0.44531\n",
      "  Ended batch 021 / 034 | loss: 0.85513 | error: 0.46094\n",
      "  Ended batch 022 / 034 | loss: 0.71297 | error: 0.35938\n",
      "  Ended batch 023 / 034 | loss: 0.74969 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.79678 | error: 0.43750\n",
      "  Ended batch 025 / 034 | loss: 0.81004 | error: 0.43750\n",
      "  Ended batch 026 / 034 | loss: 0.80086 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.73795 | error: 0.39062\n",
      "  Ended batch 028 / 034 | loss: 0.80065 | error: 0.42188\n",
      "  Ended batch 029 / 034 | loss: 0.69865 | error: 0.32812\n",
      "  Ended batch 030 / 034 | loss: 0.70498 | error: 0.33594\n",
      "  Ended batch 031 / 034 | loss: 0.75093 | error: 0.39062\n",
      "  Ended batch 032 / 034 | loss: 0.86745 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.77535 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.95931 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 101 sec | loss: 0.73946 | error: 0.26228\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.1_model.pt.\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80321 | error: 0.42188\n",
      "  Ended batch 002 / 034 | loss: 0.80649 | error: 0.43750\n",
      "  Ended batch 003 / 034 | loss: 0.77043 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.73940 | error: 0.41406\n",
      "  Ended batch 005 / 034 | loss: 0.73955 | error: 0.39844\n",
      "  Ended batch 006 / 034 | loss: 0.63564 | error: 0.33594\n",
      "  Ended batch 007 / 034 | loss: 0.93534 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.76883 | error: 0.42188\n",
      "  Ended batch 009 / 034 | loss: 0.80819 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.70683 | error: 0.32031\n",
      "  Ended batch 011 / 034 | loss: 0.68363 | error: 0.24219\n",
      "  Ended batch 012 / 034 | loss: 0.80233 | error: 0.42969\n",
      "  Ended batch 013 / 034 | loss: 0.74041 | error: 0.27344\n",
      "  Ended batch 014 / 034 | loss: 0.83548 | error: 0.28906\n",
      "  Ended batch 015 / 034 | loss: 0.74526 | error: 0.34375\n",
      "  Ended batch 016 / 034 | loss: 0.73979 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.77806 | error: 0.43750\n",
      "  Ended batch 018 / 034 | loss: 0.70108 | error: 0.36719\n",
      "  Ended batch 019 / 034 | loss: 0.70514 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.87384 | error: 0.44531\n",
      "  Ended batch 021 / 034 | loss: 0.82034 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.70909 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.74798 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.76518 | error: 0.41406\n",
      "  Ended batch 025 / 034 | loss: 0.82331 | error: 0.44531\n",
      "  Ended batch 026 / 034 | loss: 0.80967 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.76582 | error: 0.42188\n",
      "  Ended batch 028 / 034 | loss: 0.81831 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.75513 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.69078 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.74687 | error: 0.39062\n",
      "  Ended batch 032 / 034 | loss: 0.86962 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.78590 | error: 0.40625\n",
      "  Ended batch 034 / 034 | loss: 0.94970 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 088 sec | loss: 0.73572 | error: 0.26358\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80083 | error: 0.42188\n",
      "  Ended batch 002 / 034 | loss: 0.78357 | error: 0.42969\n",
      "  Ended batch 003 / 034 | loss: 0.78772 | error: 0.42969\n",
      "  Ended batch 004 / 034 | loss: 0.71642 | error: 0.39844\n",
      "  Ended batch 005 / 034 | loss: 0.73976 | error: 0.39844\n",
      "  Ended batch 006 / 034 | loss: 0.64298 | error: 0.32812\n",
      "  Ended batch 007 / 034 | loss: 0.91314 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.75230 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.77617 | error: 0.40625\n",
      "  Ended batch 010 / 034 | loss: 0.70122 | error: 0.28125\n",
      "  Ended batch 011 / 034 | loss: 0.67061 | error: 0.21875\n",
      "  Ended batch 012 / 034 | loss: 0.84683 | error: 0.50000\n",
      "  Ended batch 013 / 034 | loss: 0.76888 | error: 0.41406\n",
      "  Ended batch 014 / 034 | loss: 0.80975 | error: 0.40625\n",
      "  Ended batch 015 / 034 | loss: 0.79236 | error: 0.43750\n",
      "  Ended batch 016 / 034 | loss: 0.73546 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.80089 | error: 0.44531\n",
      "  Ended batch 018 / 034 | loss: 0.71702 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.70917 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.83825 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.82044 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.69248 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.74634 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.81305 | error: 0.44531\n",
      "  Ended batch 025 / 034 | loss: 0.80251 | error: 0.42969\n",
      "  Ended batch 026 / 034 | loss: 0.80067 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.73833 | error: 0.39062\n",
      "  Ended batch 028 / 034 | loss: 0.81232 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.70888 | error: 0.38281\n",
      "  Ended batch 030 / 034 | loss: 0.70113 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.75916 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.87664 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.78977 | error: 0.40625\n",
      "  Ended batch 034 / 034 | loss: 0.94589 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 086 sec | loss: 0.74724 | error: 0.26358\n",
      "Starting epoch 006 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80318 | error: 0.42188\n",
      "  Ended batch 002 / 034 | loss: 0.78484 | error: 0.42969\n",
      "  Ended batch 003 / 034 | loss: 0.76729 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.67501 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.71101 | error: 0.32031\n",
      "  Ended batch 006 / 034 | loss: 0.63947 | error: 0.32031\n",
      "  Ended batch 007 / 034 | loss: 0.95110 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.77486 | error: 0.42188\n",
      "  Ended batch 009 / 034 | loss: 0.81110 | error: 0.44531\n",
      "  Ended batch 010 / 034 | loss: 0.70837 | error: 0.36719\n",
      "  Ended batch 011 / 034 | loss: 0.67403 | error: 0.34375\n",
      "  Ended batch 012 / 034 | loss: 0.84934 | error: 0.50000\n",
      "  Ended batch 013 / 034 | loss: 0.74288 | error: 0.34375\n",
      "  Ended batch 014 / 034 | loss: 0.81806 | error: 0.28125\n",
      "  Ended batch 015 / 034 | loss: 0.74611 | error: 0.36719\n",
      "  Ended batch 016 / 034 | loss: 0.73749 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.74812 | error: 0.41406\n",
      "  Ended batch 018 / 034 | loss: 0.71860 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.70418 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.88283 | error: 0.44531\n",
      "  Ended batch 021 / 034 | loss: 0.82594 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.71029 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.75963 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.77844 | error: 0.42188\n",
      "  Ended batch 025 / 034 | loss: 0.84151 | error: 0.45312\n",
      "  Ended batch 026 / 034 | loss: 0.81397 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.77666 | error: 0.42188\n",
      "  Ended batch 028 / 034 | loss: 0.82372 | error: 0.44531\n",
      "  Ended batch 029 / 034 | loss: 0.69780 | error: 0.33594\n",
      "  Ended batch 030 / 034 | loss: 0.70981 | error: 0.33594\n",
      "  Ended batch 031 / 034 | loss: 0.75938 | error: 0.38281\n",
      "  Ended batch 032 / 034 | loss: 0.86956 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.76815 | error: 0.39062\n",
      "  Ended batch 034 / 034 | loss: 0.95187 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 006 / 016 | time: 089 sec | loss: 0.70719 | error: 0.25381\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.1_model.pt.\n",
      "Starting epoch 007 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.79452 | error: 0.42188\n",
      "  Ended batch 002 / 034 | loss: 0.78616 | error: 0.42969\n",
      "  Ended batch 003 / 034 | loss: 0.79131 | error: 0.42969\n",
      "  Ended batch 004 / 034 | loss: 0.70949 | error: 0.39062\n",
      "  Ended batch 005 / 034 | loss: 0.72510 | error: 0.38281\n",
      "  Ended batch 006 / 034 | loss: 0.63958 | error: 0.32812\n",
      "  Ended batch 007 / 034 | loss: 0.91999 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.74644 | error: 0.40625\n",
      "  Ended batch 009 / 034 | loss: 0.80957 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.69975 | error: 0.35938\n",
      "  Ended batch 011 / 034 | loss: 0.67297 | error: 0.34375\n",
      "  Ended batch 012 / 034 | loss: 0.87358 | error: 0.51562\n",
      "  Ended batch 013 / 034 | loss: 0.72934 | error: 0.37500\n",
      "  Ended batch 014 / 034 | loss: 0.80200 | error: 0.34375\n",
      "  Ended batch 015 / 034 | loss: 0.75730 | error: 0.38281\n",
      "  Ended batch 016 / 034 | loss: 0.73595 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.74427 | error: 0.41406\n",
      "  Ended batch 018 / 034 | loss: 0.72286 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.70704 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.86887 | error: 0.44531\n",
      "  Ended batch 021 / 034 | loss: 0.82865 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.69615 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.74779 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.79648 | error: 0.43750\n",
      "  Ended batch 025 / 034 | loss: 0.83634 | error: 0.45312\n",
      "  Ended batch 026 / 034 | loss: 0.81046 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.76667 | error: 0.42188\n",
      "  Ended batch 028 / 034 | loss: 0.81679 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.74635 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.70612 | error: 0.33594\n",
      "  Ended batch 031 / 034 | loss: 0.75058 | error: 0.36719\n",
      "  Ended batch 032 / 034 | loss: 0.84966 | error: 0.46094\n",
      "  Ended batch 033 / 034 | loss: 0.75120 | error: 0.37500\n",
      "  Ended batch 034 / 034 | loss: 0.94919 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 007 / 016 | time: 088 sec | loss: 0.71734 | error: 0.25707\n",
      "Starting epoch 008 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80904 | error: 0.42969\n",
      "  Ended batch 002 / 034 | loss: 0.78567 | error: 0.42969\n",
      "  Ended batch 003 / 034 | loss: 0.79040 | error: 0.42969\n",
      "  Ended batch 004 / 034 | loss: 0.71898 | error: 0.39844\n",
      "  Ended batch 005 / 034 | loss: 0.72591 | error: 0.38281\n",
      "  Ended batch 006 / 034 | loss: 0.63291 | error: 0.29688\n",
      "  Ended batch 007 / 034 | loss: 0.89201 | error: 0.50781\n",
      "  Ended batch 008 / 034 | loss: 0.73631 | error: 0.39844\n",
      "  Ended batch 009 / 034 | loss: 0.78389 | error: 0.43750\n",
      "  Ended batch 010 / 034 | loss: 0.69797 | error: 0.32812\n",
      "  Ended batch 011 / 034 | loss: 0.67804 | error: 0.32031\n",
      "  Ended batch 012 / 034 | loss: 0.84406 | error: 0.49219\n",
      "  Ended batch 013 / 034 | loss: 0.73652 | error: 0.38281\n",
      "  Ended batch 014 / 034 | loss: 0.80820 | error: 0.37500\n",
      "  Ended batch 015 / 034 | loss: 0.77269 | error: 0.42188\n",
      "  Ended batch 016 / 034 | loss: 0.73654 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.76734 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.71800 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.70533 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.85207 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.81144 | error: 0.43750\n",
      "  Ended batch 022 / 034 | loss: 0.70441 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.74837 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.81527 | error: 0.44531\n",
      "  Ended batch 025 / 034 | loss: 0.83646 | error: 0.45312\n",
      "  Ended batch 026 / 034 | loss: 0.80848 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.73595 | error: 0.38281\n",
      "  Ended batch 028 / 034 | loss: 0.79291 | error: 0.39062\n",
      "  Ended batch 029 / 034 | loss: 0.65712 | error: 0.28125\n",
      "  Ended batch 030 / 034 | loss: 0.70555 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.75588 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.90280 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.81865 | error: 0.41406\n",
      "  Ended batch 034 / 034 | loss: 0.95799 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 008 / 016 | time: 096 sec | loss: 0.77477 | error: 0.27400\n",
      "Starting epoch 009 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80841 | error: 0.42188\n",
      "  Ended batch 002 / 034 | loss: 0.71894 | error: 0.25000\n",
      "  Ended batch 003 / 034 | loss: 0.75812 | error: 0.32812\n",
      "  Ended batch 004 / 034 | loss: 0.65086 | error: 0.22656\n",
      "  Ended batch 005 / 034 | loss: 0.71866 | error: 0.35938\n",
      "  Ended batch 006 / 034 | loss: 0.66338 | error: 0.35156\n",
      "  Ended batch 007 / 034 | loss: 0.99287 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.77857 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.82518 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.69324 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.65971 | error: 0.28125\n",
      "  Ended batch 012 / 034 | loss: 0.84031 | error: 0.49219\n",
      "  Ended batch 013 / 034 | loss: 0.73319 | error: 0.37500\n",
      "  Ended batch 014 / 034 | loss: 0.80974 | error: 0.38281\n",
      "  Ended batch 015 / 034 | loss: 0.76238 | error: 0.40625\n",
      "  Ended batch 016 / 034 | loss: 0.73905 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.78217 | error: 0.42188\n",
      "  Ended batch 018 / 034 | loss: 0.70657 | error: 0.36719\n",
      "  Ended batch 019 / 034 | loss: 0.68633 | error: 0.35938\n",
      "  Ended batch 020 / 034 | loss: 0.84418 | error: 0.42969\n",
      "  Ended batch 021 / 034 | loss: 0.79048 | error: 0.42188\n",
      "  Ended batch 022 / 034 | loss: 0.69506 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.73920 | error: 0.39062\n",
      "  Ended batch 024 / 034 | loss: 0.79201 | error: 0.42188\n",
      "  Ended batch 025 / 034 | loss: 0.78439 | error: 0.41406\n",
      "  Ended batch 026 / 034 | loss: 0.81943 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.72398 | error: 0.37500\n",
      "  Ended batch 028 / 034 | loss: 0.81892 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.71997 | error: 0.39062\n",
      "  Ended batch 030 / 034 | loss: 0.69415 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.75103 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.90406 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.79831 | error: 0.41406\n",
      "  Ended batch 034 / 034 | loss: 0.93969 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 009 / 016 | time: 100 sec | loss: 0.82717 | error: 0.30069\n",
      "Starting epoch 010 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80598 | error: 0.42188\n",
      "  Ended batch 002 / 034 | loss: 0.73272 | error: 0.30469\n",
      "  Ended batch 003 / 034 | loss: 0.76614 | error: 0.37500\n",
      "  Ended batch 004 / 034 | loss: 0.66992 | error: 0.28125\n",
      "  Ended batch 005 / 034 | loss: 0.73403 | error: 0.36719\n",
      "  Ended batch 006 / 034 | loss: 0.64699 | error: 0.33594\n",
      "  Ended batch 007 / 034 | loss: 0.97117 | error: 0.53125\n",
      "  Ended batch 008 / 034 | loss: 0.78930 | error: 0.42969\n",
      "  Ended batch 009 / 034 | loss: 0.82085 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.69524 | error: 0.35156\n",
      "  Ended batch 011 / 034 | loss: 0.68017 | error: 0.33594\n",
      "  Ended batch 012 / 034 | loss: 0.80169 | error: 0.46875\n",
      "  Ended batch 013 / 034 | loss: 0.74382 | error: 0.35156\n",
      "  Ended batch 014 / 034 | loss: 0.77628 | error: 0.35938\n",
      "  Ended batch 015 / 034 | loss: 0.77553 | error: 0.42188\n",
      "  Ended batch 016 / 034 | loss: 0.73700 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.77822 | error: 0.41406\n",
      "  Ended batch 018 / 034 | loss: 0.70210 | error: 0.36719\n",
      "  Ended batch 019 / 034 | loss: 0.69206 | error: 0.35938\n",
      "  Ended batch 020 / 034 | loss: 0.81163 | error: 0.42188\n",
      "  Ended batch 021 / 034 | loss: 0.79858 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.68584 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.75883 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.80971 | error: 0.42969\n",
      "  Ended batch 025 / 034 | loss: 0.79866 | error: 0.42188\n",
      "  Ended batch 026 / 034 | loss: 0.83510 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.72775 | error: 0.37500\n",
      "  Ended batch 028 / 034 | loss: 0.80951 | error: 0.42969\n",
      "  Ended batch 029 / 034 | loss: 0.75719 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.69302 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.75261 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.93095 | error: 0.49219\n",
      "  Ended batch 033 / 034 | loss: 0.80258 | error: 0.41406\n",
      "  Ended batch 034 / 034 | loss: 0.93731 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 010 / 016 | time: 093 sec | loss: 0.83292 | error: 0.30590\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 14.892616434892018 minutes (893.5569860935211 seconds).\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.1_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.1_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "---------- Calculating results ----------\n",
      "Evaluating using epsilon: 0.1\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0903, -0.2800, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "         -0.0746,  0.0482, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919,  0.1656,\n",
      "          0.3650,  0.3795, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "          0.0652, -0.1662, -0.0495,  0.3771, -0.0304, -0.0473, -0.0763]])\n",
      "Evaluating using epsilon: 0.1\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1017, -0.2191, -0.0919, -0.1008, -0.0419, -0.0798, -0.1001, -0.1063,\n",
      "         -0.0571,  0.0306, -0.1083, -0.1969, -0.0795, -0.0555, -0.0890],\n",
      "        [-0.1183,  0.2232, -0.1324, -0.1429, -0.0382, -0.1084, -0.1262,  0.2416,\n",
      "          0.4380,  0.3062, -0.1747, -0.2179, -0.0945, -0.0656, -0.1290],\n",
      "        [-0.0855,  0.0472, -0.0954, -0.1045, -0.0416, -0.0823, -0.1024, -0.1013,\n",
      "          0.0876, -0.1887, -0.0763,  0.3686, -0.0365, -0.0514, -0.0925]])\n",
      "Evaluating using epsilon: 0.1\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1048, -0.1972, -0.0939, -0.1051, -0.0372, -0.0886, -0.1041, -0.1004,\n",
      "         -0.0537,  0.0255, -0.1172, -0.1893, -0.0784, -0.0516, -0.0917],\n",
      "        [-0.3107,  1.3467, -0.3251, -0.3791,  0.0546, -0.3677, -0.2954,  0.5676,\n",
      "          0.7128,  0.0029, -0.5842, -0.1738, -0.1353, -0.0402, -0.3326],\n",
      "        [-0.1053,  0.1662, -0.1142, -0.1292, -0.0291, -0.1132, -0.1209, -0.0671,\n",
      "          0.1149, -0.2200, -0.1205,  0.3794, -0.0391, -0.0456, -0.1130]])\n",
      "Evaluating using epsilon: 0.1\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1180,  0.3431, -0.3083, -0.3044, -0.3166, -0.2312, -0.1468, -0.2129,\n",
      "         -0.1433, -0.0645, -0.4414, -0.1942, -0.0617,  0.1387, -0.1661],\n",
      "        [-0.0956,  0.5230, -0.2827, -0.2802, -0.2861, -0.2133, -0.1407,  0.0871,\n",
      "          0.3039,  0.2794, -0.4019, -0.1937, -0.0631,  0.1174, -0.1561],\n",
      "        [-0.0947,  0.5021, -0.2750, -0.2730, -0.2770, -0.2079, -0.1389, -0.2001,\n",
      "          0.0065, -0.2625, -0.3523,  0.3738, -0.0193,  0.1160, -0.1531]])\n",
      "------------------------- 0.15 -------------------------\n",
      "---------- Training non-strategically with epsilon=0.15 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98923 | error: 0.43750\n",
      "  Ended batch 002 / 034 | loss: 0.83193 | error: 0.39844\n",
      "  Ended batch 003 / 034 | loss: 0.75601 | error: 0.38281\n",
      "  Ended batch 004 / 034 | loss: 0.62267 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.53573 | error: 0.26562\n",
      "  Ended batch 006 / 034 | loss: 0.43920 | error: 0.17188\n",
      "  Ended batch 007 / 034 | loss: 0.55319 | error: 0.24219\n",
      "  Ended batch 008 / 034 | loss: 0.43821 | error: 0.17188\n",
      "  Ended batch 009 / 034 | loss: 0.48477 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.44141 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.34122 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.39537 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.46204 | error: 0.17969\n",
      "  Ended batch 014 / 034 | loss: 0.61706 | error: 0.21094\n",
      "  Ended batch 015 / 034 | loss: 0.40735 | error: 0.15625\n",
      "  Ended batch 016 / 034 | loss: 0.44818 | error: 0.19531\n",
      "  Ended batch 017 / 034 | loss: 0.42789 | error: 0.17969\n",
      "  Ended batch 018 / 034 | loss: 0.45081 | error: 0.14062\n",
      "  Ended batch 019 / 034 | loss: 0.40077 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.67522 | error: 0.20312\n",
      "  Ended batch 021 / 034 | loss: 0.44442 | error: 0.17188\n",
      "  Ended batch 022 / 034 | loss: 0.49886 | error: 0.20312\n",
      "  Ended batch 023 / 034 | loss: 0.43399 | error: 0.16406\n",
      "  Ended batch 024 / 034 | loss: 0.42774 | error: 0.17188\n",
      "  Ended batch 025 / 034 | loss: 0.49798 | error: 0.17969\n",
      "  Ended batch 026 / 034 | loss: 0.48518 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34755 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.36209 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30258 | error: 0.14062\n",
      "  Ended batch 030 / 034 | loss: 0.27834 | error: 0.09375\n",
      "  Ended batch 031 / 034 | loss: 0.31573 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.46953 | error: 0.21094\n",
      "  Ended batch 033 / 034 | loss: 0.44510 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.35380 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 000 sec | loss: 0.45466 | error: 0.21103\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/non_strategic_0.15_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.40293 | error: 0.16406\n",
      "  Ended batch 002 / 034 | loss: 0.39698 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39748 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.33817 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31394 | error: 0.13281\n",
      "  Ended batch 006 / 034 | loss: 0.26311 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.40746 | error: 0.21094\n",
      "  Ended batch 008 / 034 | loss: 0.28619 | error: 0.12500\n",
      "  Ended batch 009 / 034 | loss: 0.42815 | error: 0.21875\n",
      "  Ended batch 010 / 034 | loss: 0.37308 | error: 0.14062\n",
      "  Ended batch 011 / 034 | loss: 0.24522 | error: 0.10938\n",
      "  Ended batch 012 / 034 | loss: 0.35200 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.38305 | error: 0.16406\n",
      "  Ended batch 014 / 034 | loss: 0.48793 | error: 0.18750\n",
      "  Ended batch 015 / 034 | loss: 0.29911 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37009 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39094 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.39047 | error: 0.16406\n",
      "  Ended batch 019 / 034 | loss: 0.33128 | error: 0.14844\n",
      "  Ended batch 020 / 034 | loss: 0.62716 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.39453 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.42599 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.40997 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38743 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.49943 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.46154 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34475 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.35799 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30337 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29895 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31889 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44511 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40606 | error: 0.16406\n",
      "  Ended batch 034 / 034 | loss: 0.41604 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 000 sec | loss: 0.44767 | error: 0.21363\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37960 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.38054 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39798 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.32809 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31529 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26527 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.39605 | error: 0.20312\n",
      "  Ended batch 008 / 034 | loss: 0.29298 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42259 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.37166 | error: 0.12500\n",
      "  Ended batch 011 / 034 | loss: 0.24852 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.34251 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37322 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47350 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31297 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.36932 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39082 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38728 | error: 0.15625\n",
      "  Ended batch 019 / 034 | loss: 0.33095 | error: 0.13281\n",
      "  Ended batch 020 / 034 | loss: 0.63589 | error: 0.24219\n",
      "  Ended batch 021 / 034 | loss: 0.39093 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41775 | error: 0.18750\n",
      "  Ended batch 023 / 034 | loss: 0.40417 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38116 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48604 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.45087 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34605 | error: 0.13281\n",
      "  Ended batch 028 / 034 | loss: 0.36658 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30254 | error: 0.10938\n",
      "  Ended batch 030 / 034 | loss: 0.30504 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31964 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44560 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40693 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.43542 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 000 sec | loss: 0.44874 | error: 0.21168\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37193 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37114 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39974 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31597 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31782 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26514 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.39098 | error: 0.18750\n",
      "  Ended batch 008 / 034 | loss: 0.28986 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42376 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.36756 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24499 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34379 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37043 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47858 | error: 0.20312\n",
      "  Ended batch 015 / 034 | loss: 0.30862 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37446 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38862 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38593 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32783 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64187 | error: 0.25000\n",
      "  Ended batch 021 / 034 | loss: 0.38988 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41519 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39861 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37473 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48543 | error: 0.21094\n",
      "  Ended batch 026 / 034 | loss: 0.44854 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34043 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36239 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.29949 | error: 0.11719\n",
      "  Ended batch 030 / 034 | loss: 0.29681 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31391 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45120 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40857 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42124 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 000 sec | loss: 0.44502 | error: 0.21103\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37063 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37067 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39355 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31063 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31461 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26898 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.38740 | error: 0.19531\n",
      "  Ended batch 008 / 034 | loss: 0.28760 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42075 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.36434 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24144 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34192 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.36529 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47503 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31060 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37830 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38680 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38255 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32599 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64071 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.38804 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41321 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39709 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37160 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48514 | error: 0.20312\n",
      "  Ended batch 026 / 034 | loss: 0.44696 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34037 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36098 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30023 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29524 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31327 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45363 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40813 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42413 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 000 sec | loss: 0.44367 | error: 0.21168\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 0.843998908996582 seconds.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.15_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.15_model.pt.\n",
      "---------- Training strategically with epsilon=0.15 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.95704 | error: 0.46094\n",
      "  Ended batch 002 / 034 | loss: 1.11447 | error: 0.48438\n",
      "  Ended batch 003 / 034 | loss: 1.07237 | error: 0.52344\n",
      "  Ended batch 004 / 034 | loss: 0.91400 | error: 0.50000\n",
      "  Ended batch 005 / 034 | loss: 0.90228 | error: 0.35156\n",
      "  Ended batch 006 / 034 | loss: 0.83401 | error: 0.40625\n",
      "  Ended batch 007 / 034 | loss: 1.06223 | error: 0.57031\n",
      "  Ended batch 008 / 034 | loss: 0.85779 | error: 0.46875\n",
      "  Ended batch 009 / 034 | loss: 0.90577 | error: 0.47656\n",
      "  Ended batch 010 / 034 | loss: 0.76088 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.71793 | error: 0.28125\n",
      "  Ended batch 012 / 034 | loss: 0.91923 | error: 0.53906\n",
      "  Ended batch 013 / 034 | loss: 0.79569 | error: 0.42969\n",
      "  Ended batch 014 / 034 | loss: 0.79124 | error: 0.40625\n",
      "  Ended batch 015 / 034 | loss: 0.82774 | error: 0.46094\n",
      "  Ended batch 016 / 034 | loss: 0.81481 | error: 0.44531\n",
      "  Ended batch 017 / 034 | loss: 0.77324 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.74420 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.70274 | error: 0.35156\n",
      "  Ended batch 020 / 034 | loss: 0.83283 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.80204 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.67564 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.74013 | error: 0.39062\n",
      "  Ended batch 024 / 034 | loss: 0.76065 | error: 0.39844\n",
      "  Ended batch 025 / 034 | loss: 0.77740 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.75194 | error: 0.38281\n",
      "  Ended batch 027 / 034 | loss: 0.62233 | error: 0.29688\n",
      "  Ended batch 028 / 034 | loss: 0.75801 | error: 0.37500\n",
      "  Ended batch 029 / 034 | loss: 0.59179 | error: 0.30469\n",
      "  Ended batch 030 / 034 | loss: 0.64071 | error: 0.33594\n",
      "  Ended batch 031 / 034 | loss: 0.73308 | error: 0.37500\n",
      "  Ended batch 032 / 034 | loss: 0.81552 | error: 0.41406\n",
      "  Ended batch 033 / 034 | loss: 0.68975 | error: 0.34375\n",
      "  Ended batch 034 / 034 | loss: 0.93169 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 074 sec | loss: 1.20741 | error: 0.46215\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.15_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.72119 | error: 0.34375\n",
      "  Ended batch 002 / 034 | loss: 0.54390 | error: 0.21094\n",
      "  Ended batch 003 / 034 | loss: 0.74240 | error: 0.39062\n",
      "  Ended batch 004 / 034 | loss: 0.65445 | error: 0.35156\n",
      "  Ended batch 005 / 034 | loss: 0.70139 | error: 0.36719\n",
      "  Ended batch 006 / 034 | loss: 0.58490 | error: 0.30469\n",
      "  Ended batch 007 / 034 | loss: 0.93671 | error: 0.50000\n",
      "  Ended batch 008 / 034 | loss: 0.69730 | error: 0.37500\n",
      "  Ended batch 009 / 034 | loss: 0.74916 | error: 0.39844\n",
      "  Ended batch 010 / 034 | loss: 0.66495 | error: 0.30469\n",
      "  Ended batch 011 / 034 | loss: 0.57016 | error: 0.25000\n",
      "  Ended batch 012 / 034 | loss: 0.75457 | error: 0.42188\n",
      "  Ended batch 013 / 034 | loss: 0.70150 | error: 0.35938\n",
      "  Ended batch 014 / 034 | loss: 0.75985 | error: 0.37500\n",
      "  Ended batch 015 / 034 | loss: 0.77471 | error: 0.41406\n",
      "  Ended batch 016 / 034 | loss: 0.71931 | error: 0.37500\n",
      "  Ended batch 017 / 034 | loss: 0.71093 | error: 0.37500\n",
      "  Ended batch 018 / 034 | loss: 0.65943 | error: 0.34375\n",
      "  Ended batch 019 / 034 | loss: 0.61058 | error: 0.30469\n",
      "  Ended batch 020 / 034 | loss: 0.75815 | error: 0.38281\n",
      "  Ended batch 021 / 034 | loss: 0.75605 | error: 0.40625\n",
      "  Ended batch 022 / 034 | loss: 0.66571 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.74704 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.75610 | error: 0.40625\n",
      "  Ended batch 025 / 034 | loss: 0.75876 | error: 0.39844\n",
      "  Ended batch 026 / 034 | loss: 0.81108 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.69168 | error: 0.35938\n",
      "  Ended batch 028 / 034 | loss: 0.78373 | error: 0.41406\n",
      "  Ended batch 029 / 034 | loss: 0.64820 | error: 0.34375\n",
      "  Ended batch 030 / 034 | loss: 0.66674 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.74259 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.86920 | error: 0.46094\n",
      "  Ended batch 033 / 034 | loss: 0.79449 | error: 0.41406\n",
      "  Ended batch 034 / 034 | loss: 0.92740 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 101 sec | loss: 0.81060 | error: 0.28962\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.15_model.pt.\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.78472 | error: 0.41406\n",
      "  Ended batch 002 / 034 | loss: 0.70455 | error: 0.25000\n",
      "  Ended batch 003 / 034 | loss: 0.74444 | error: 0.34375\n",
      "  Ended batch 004 / 034 | loss: 0.65530 | error: 0.25000\n",
      "  Ended batch 005 / 034 | loss: 0.68951 | error: 0.31250\n",
      "  Ended batch 006 / 034 | loss: 0.59547 | error: 0.29688\n",
      "  Ended batch 007 / 034 | loss: 0.91396 | error: 0.50781\n",
      "  Ended batch 008 / 034 | loss: 0.78371 | error: 0.42969\n",
      "  Ended batch 009 / 034 | loss: 0.79954 | error: 0.43750\n",
      "  Ended batch 010 / 034 | loss: 0.70598 | error: 0.37500\n",
      "  Ended batch 011 / 034 | loss: 0.64907 | error: 0.33594\n",
      "  Ended batch 012 / 034 | loss: 0.78719 | error: 0.46094\n",
      "  Ended batch 013 / 034 | loss: 0.73991 | error: 0.35156\n",
      "  Ended batch 014 / 034 | loss: 0.79603 | error: 0.32031\n",
      "  Ended batch 015 / 034 | loss: 0.72285 | error: 0.35938\n",
      "  Ended batch 016 / 034 | loss: 0.69668 | error: 0.35938\n",
      "  Ended batch 017 / 034 | loss: 0.73581 | error: 0.40625\n",
      "  Ended batch 018 / 034 | loss: 0.70828 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.68236 | error: 0.36719\n",
      "  Ended batch 020 / 034 | loss: 0.85923 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.81019 | error: 0.43750\n",
      "  Ended batch 022 / 034 | loss: 0.68326 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.73936 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.75493 | error: 0.41406\n",
      "  Ended batch 025 / 034 | loss: 0.79209 | error: 0.42969\n",
      "  Ended batch 026 / 034 | loss: 0.80057 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.74675 | error: 0.41406\n",
      "  Ended batch 028 / 034 | loss: 0.79840 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.73203 | error: 0.41406\n",
      "  Ended batch 030 / 034 | loss: 0.67775 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.74504 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.85216 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.77076 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.92518 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 094 sec | loss: 0.72590 | error: 0.25707\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.15_model.pt.\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.79602 | error: 0.42969\n",
      "  Ended batch 002 / 034 | loss: 0.76261 | error: 0.42188\n",
      "  Ended batch 003 / 034 | loss: 0.76518 | error: 0.42188\n",
      "  Ended batch 004 / 034 | loss: 0.68695 | error: 0.38281\n",
      "  Ended batch 005 / 034 | loss: 0.71578 | error: 0.38281\n",
      "  Ended batch 006 / 034 | loss: 0.60399 | error: 0.31250\n",
      "  Ended batch 007 / 034 | loss: 0.90181 | error: 0.52344\n",
      "  Ended batch 008 / 034 | loss: 0.74022 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.77089 | error: 0.43750\n",
      "  Ended batch 010 / 034 | loss: 0.67165 | error: 0.31250\n",
      "  Ended batch 011 / 034 | loss: 0.65508 | error: 0.28125\n",
      "  Ended batch 012 / 034 | loss: 0.80035 | error: 0.47656\n",
      "  Ended batch 013 / 034 | loss: 0.73017 | error: 0.38281\n",
      "  Ended batch 014 / 034 | loss: 0.78216 | error: 0.36719\n",
      "  Ended batch 015 / 034 | loss: 0.75437 | error: 0.41406\n",
      "  Ended batch 016 / 034 | loss: 0.71437 | error: 0.38281\n",
      "  Ended batch 017 / 034 | loss: 0.72608 | error: 0.40625\n",
      "  Ended batch 018 / 034 | loss: 0.70563 | error: 0.36719\n",
      "  Ended batch 019 / 034 | loss: 0.68938 | error: 0.36719\n",
      "  Ended batch 020 / 034 | loss: 0.84862 | error: 0.44531\n",
      "  Ended batch 021 / 034 | loss: 0.82755 | error: 0.45312\n",
      "  Ended batch 022 / 034 | loss: 0.68155 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.73478 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.76354 | error: 0.42188\n",
      "  Ended batch 025 / 034 | loss: 0.79050 | error: 0.42969\n",
      "  Ended batch 026 / 034 | loss: 0.79860 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.73327 | error: 0.40625\n",
      "  Ended batch 028 / 034 | loss: 0.78339 | error: 0.42188\n",
      "  Ended batch 029 / 034 | loss: 0.72138 | error: 0.40625\n",
      "  Ended batch 030 / 034 | loss: 0.67365 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.74144 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.87568 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.77089 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.92925 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 114 sec | loss: 0.72926 | error: 0.25837\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.79927 | error: 0.42969\n",
      "  Ended batch 002 / 034 | loss: 0.76016 | error: 0.42188\n",
      "  Ended batch 003 / 034 | loss: 0.76482 | error: 0.42188\n",
      "  Ended batch 004 / 034 | loss: 0.68747 | error: 0.38281\n",
      "  Ended batch 005 / 034 | loss: 0.71041 | error: 0.35938\n",
      "  Ended batch 006 / 034 | loss: 0.59538 | error: 0.29688\n",
      "  Ended batch 007 / 034 | loss: 0.89091 | error: 0.51562\n",
      "  Ended batch 008 / 034 | loss: 0.75171 | error: 0.42188\n",
      "  Ended batch 009 / 034 | loss: 0.79488 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.68607 | error: 0.32812\n",
      "  Ended batch 011 / 034 | loss: 0.65537 | error: 0.29688\n",
      "  Ended batch 012 / 034 | loss: 0.80709 | error: 0.46875\n",
      "  Ended batch 013 / 034 | loss: 0.73422 | error: 0.30469\n",
      "  Ended batch 014 / 034 | loss: 0.80931 | error: 0.28906\n",
      "  Ended batch 015 / 034 | loss: 0.70894 | error: 0.31250\n",
      "  Ended batch 016 / 034 | loss: 0.69970 | error: 0.35156\n",
      "  Ended batch 017 / 034 | loss: 0.74238 | error: 0.42188\n",
      "  Ended batch 018 / 034 | loss: 0.70748 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.68409 | error: 0.36719\n",
      "  Ended batch 020 / 034 | loss: 0.86072 | error: 0.44531\n",
      "  Ended batch 021 / 034 | loss: 0.82607 | error: 0.45312\n",
      "  Ended batch 022 / 034 | loss: 0.69091 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.73678 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.75059 | error: 0.41406\n",
      "  Ended batch 025 / 034 | loss: 0.76761 | error: 0.41406\n",
      "  Ended batch 026 / 034 | loss: 0.78992 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.71364 | error: 0.38281\n",
      "  Ended batch 028 / 034 | loss: 0.77530 | error: 0.41406\n",
      "  Ended batch 029 / 034 | loss: 0.68558 | error: 0.37500\n",
      "  Ended batch 030 / 034 | loss: 0.67436 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.74065 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.89496 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.77421 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.92293 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 112 sec | loss: 0.77917 | error: 0.27269\n",
      "Starting epoch 006 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.78166 | error: 0.38281\n",
      "  Ended batch 002 / 034 | loss: 0.70587 | error: 0.24219\n",
      "  Ended batch 003 / 034 | loss: 0.73125 | error: 0.32812\n",
      "  Ended batch 004 / 034 | loss: 0.64535 | error: 0.25000\n",
      "  Ended batch 005 / 034 | loss: 0.68790 | error: 0.35938\n",
      "  Ended batch 006 / 034 | loss: 0.57728 | error: 0.29688\n",
      "  Ended batch 007 / 034 | loss: 0.98183 | error: 0.53125\n",
      "  Ended batch 008 / 034 | loss: 0.80099 | error: 0.42969\n",
      "  Ended batch 009 / 034 | loss: 0.82724 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.67789 | error: 0.35156\n",
      "  Ended batch 011 / 034 | loss: 0.63586 | error: 0.30469\n",
      "  Ended batch 012 / 034 | loss: 0.74548 | error: 0.43750\n",
      "  Ended batch 013 / 034 | loss: 0.70156 | error: 0.27344\n",
      "  Ended batch 014 / 034 | loss: 0.78238 | error: 0.29688\n",
      "  Ended batch 015 / 034 | loss: 0.75362 | error: 0.41406\n",
      "  Ended batch 016 / 034 | loss: 0.69524 | error: 0.36719\n",
      "  Ended batch 017 / 034 | loss: 0.77112 | error: 0.42188\n",
      "  Ended batch 018 / 034 | loss: 0.70977 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.68262 | error: 0.36719\n",
      "  Ended batch 020 / 034 | loss: 0.81632 | error: 0.42969\n",
      "  Ended batch 021 / 034 | loss: 0.77545 | error: 0.42188\n",
      "  Ended batch 022 / 034 | loss: 0.68715 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.73793 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.76836 | error: 0.42188\n",
      "  Ended batch 025 / 034 | loss: 0.80147 | error: 0.42969\n",
      "  Ended batch 026 / 034 | loss: 0.79179 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.74267 | error: 0.40625\n",
      "  Ended batch 028 / 034 | loss: 0.79402 | error: 0.42969\n",
      "  Ended batch 029 / 034 | loss: 0.65858 | error: 0.35156\n",
      "  Ended batch 030 / 034 | loss: 0.66667 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.74185 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.88417 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.75207 | error: 0.38281\n",
      "  Ended batch 034 / 034 | loss: 0.93103 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 006 / 016 | time: 103 sec | loss: 0.74074 | error: 0.25316\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.15_model.pt.\n",
      "Starting epoch 007 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.79389 | error: 0.42188\n",
      "  Ended batch 002 / 034 | loss: 0.70769 | error: 0.35938\n",
      "  Ended batch 003 / 034 | loss: 0.75515 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.64729 | error: 0.33594\n",
      "  Ended batch 005 / 034 | loss: 0.68651 | error: 0.35938\n",
      "  Ended batch 006 / 034 | loss: 0.60592 | error: 0.32031\n",
      "  Ended batch 007 / 034 | loss: 0.95067 | error: 0.53125\n",
      "  Ended batch 008 / 034 | loss: 0.74985 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.80289 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.67463 | error: 0.32031\n",
      "  Ended batch 011 / 034 | loss: 0.65642 | error: 0.25781\n",
      "  Ended batch 012 / 034 | loss: 0.76187 | error: 0.41406\n",
      "  Ended batch 013 / 034 | loss: 0.72996 | error: 0.25781\n",
      "  Ended batch 014 / 034 | loss: 0.80470 | error: 0.24219\n",
      "  Ended batch 015 / 034 | loss: 0.70420 | error: 0.31250\n",
      "  Ended batch 016 / 034 | loss: 0.70059 | error: 0.36719\n",
      "  Ended batch 017 / 034 | loss: 0.74424 | error: 0.41406\n",
      "  Ended batch 018 / 034 | loss: 0.71822 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.69370 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.81599 | error: 0.42969\n",
      "  Ended batch 021 / 034 | loss: 0.78476 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.69260 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.73649 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.77397 | error: 0.42969\n",
      "  Ended batch 025 / 034 | loss: 0.79227 | error: 0.42969\n",
      "  Ended batch 026 / 034 | loss: 0.80053 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.73442 | error: 0.40625\n",
      "  Ended batch 028 / 034 | loss: 0.78339 | error: 0.42188\n",
      "  Ended batch 029 / 034 | loss: 0.71178 | error: 0.39844\n",
      "  Ended batch 030 / 034 | loss: 0.67099 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.74194 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.88165 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.76073 | error: 0.39062\n",
      "  Ended batch 034 / 034 | loss: 0.93038 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 007 / 016 | time: 094 sec | loss: 0.73000 | error: 0.25186\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.15_model.pt.\n",
      "Starting epoch 008 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.80201 | error: 0.42969\n",
      "  Ended batch 002 / 034 | loss: 0.75405 | error: 0.41406\n",
      "  Ended batch 003 / 034 | loss: 0.75511 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.68526 | error: 0.35938\n",
      "  Ended batch 005 / 034 | loss: 0.69099 | error: 0.34375\n",
      "  Ended batch 006 / 034 | loss: 0.59450 | error: 0.28906\n",
      "  Ended batch 007 / 034 | loss: 0.89951 | error: 0.51562\n",
      "  Ended batch 008 / 034 | loss: 0.75585 | error: 0.42188\n",
      "  Ended batch 009 / 034 | loss: 0.80725 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.69964 | error: 0.37500\n",
      "  Ended batch 011 / 034 | loss: 0.65706 | error: 0.34375\n",
      "  Ended batch 012 / 034 | loss: 0.81905 | error: 0.49219\n",
      "  Ended batch 013 / 034 | loss: 0.73068 | error: 0.28906\n",
      "  Ended batch 014 / 034 | loss: 0.80243 | error: 0.28906\n",
      "  Ended batch 015 / 034 | loss: 0.70951 | error: 0.33594\n",
      "  Ended batch 016 / 034 | loss: 0.70386 | error: 0.36719\n",
      "  Ended batch 017 / 034 | loss: 0.75247 | error: 0.42188\n",
      "  Ended batch 018 / 034 | loss: 0.72045 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.68456 | error: 0.36719\n",
      "  Ended batch 020 / 034 | loss: 0.84117 | error: 0.44531\n",
      "  Ended batch 021 / 034 | loss: 0.79401 | error: 0.43750\n",
      "  Ended batch 022 / 034 | loss: 0.70661 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.74019 | error: 0.39062\n",
      "  Ended batch 024 / 034 | loss: 0.78030 | error: 0.42969\n",
      "  Ended batch 025 / 034 | loss: 0.76876 | error: 0.41406\n",
      "  Ended batch 026 / 034 | loss: 0.81695 | error: 0.44531\n",
      "  Ended batch 027 / 034 | loss: 0.71525 | error: 0.38281\n",
      "  Ended batch 028 / 034 | loss: 0.78229 | error: 0.42188\n",
      "  Ended batch 029 / 034 | loss: 0.66886 | error: 0.35938\n",
      "  Ended batch 030 / 034 | loss: 0.67275 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.74132 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.89697 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.75021 | error: 0.38281\n",
      "  Ended batch 034 / 034 | loss: 0.92656 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 008 / 016 | time: 100 sec | loss: 0.76536 | error: 0.26293\n",
      "Starting epoch 009 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.79471 | error: 0.41406\n",
      "  Ended batch 002 / 034 | loss: 0.70377 | error: 0.35938\n",
      "  Ended batch 003 / 034 | loss: 0.74796 | error: 0.40625\n",
      "  Ended batch 004 / 034 | loss: 0.69921 | error: 0.39062\n",
      "  Ended batch 005 / 034 | loss: 0.71135 | error: 0.38281\n",
      "  Ended batch 006 / 034 | loss: 0.61998 | error: 0.32812\n",
      "  Ended batch 007 / 034 | loss: 0.91287 | error: 0.51562\n",
      "  Ended batch 008 / 034 | loss: 0.75022 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.80772 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.69296 | error: 0.35938\n",
      "  Ended batch 011 / 034 | loss: 0.65514 | error: 0.33594\n",
      "  Ended batch 012 / 034 | loss: 0.82145 | error: 0.48438\n",
      "  Ended batch 013 / 034 | loss: 0.74388 | error: 0.39062\n",
      "  Ended batch 014 / 034 | loss: 0.79779 | error: 0.37500\n",
      "  Ended batch 015 / 034 | loss: 0.72975 | error: 0.38281\n",
      "  Ended batch 016 / 034 | loss: 0.70860 | error: 0.37500\n",
      "  Ended batch 017 / 034 | loss: 0.70864 | error: 0.39062\n",
      "  Ended batch 018 / 034 | loss: 0.69872 | error: 0.36719\n",
      "  Ended batch 019 / 034 | loss: 0.69478 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.86251 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.82647 | error: 0.44531\n",
      "  Ended batch 022 / 034 | loss: 0.68339 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.74022 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.77318 | error: 0.42969\n",
      "  Ended batch 025 / 034 | loss: 0.81104 | error: 0.44531\n",
      "  Ended batch 026 / 034 | loss: 0.80032 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.74590 | error: 0.41406\n",
      "  Ended batch 028 / 034 | loss: 0.79835 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.74037 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.69105 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.75025 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.82907 | error: 0.45312\n",
      "  Ended batch 033 / 034 | loss: 0.75902 | error: 0.38281\n",
      "  Ended batch 034 / 034 | loss: 0.92188 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 009 / 016 | time: 106 sec | loss: 0.72199 | error: 0.25056\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.15_model.pt.\n",
      "Starting epoch 010 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.79844 | error: 0.42969\n",
      "  Ended batch 002 / 034 | loss: 0.75121 | error: 0.41406\n",
      "  Ended batch 003 / 034 | loss: 0.75485 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.68872 | error: 0.38281\n",
      "  Ended batch 005 / 034 | loss: 0.70740 | error: 0.38281\n",
      "  Ended batch 006 / 034 | loss: 0.60322 | error: 0.32031\n",
      "  Ended batch 007 / 034 | loss: 0.94054 | error: 0.53125\n",
      "  Ended batch 008 / 034 | loss: 0.74407 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.77295 | error: 0.43750\n",
      "  Ended batch 010 / 034 | loss: 0.67579 | error: 0.31250\n",
      "  Ended batch 011 / 034 | loss: 0.63722 | error: 0.23438\n",
      "  Ended batch 012 / 034 | loss: 0.81577 | error: 0.48438\n",
      "  Ended batch 013 / 034 | loss: 0.73054 | error: 0.38281\n",
      "  Ended batch 014 / 034 | loss: 0.79836 | error: 0.39844\n",
      "  Ended batch 015 / 034 | loss: 0.77442 | error: 0.42969\n",
      "  Ended batch 016 / 034 | loss: 0.71441 | error: 0.38281\n",
      "  Ended batch 017 / 034 | loss: 0.76932 | error: 0.42969\n",
      "  Ended batch 018 / 034 | loss: 0.70686 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.68537 | error: 0.36719\n",
      "  Ended batch 020 / 034 | loss: 0.84199 | error: 0.44531\n",
      "  Ended batch 021 / 034 | loss: 0.79519 | error: 0.43750\n",
      "  Ended batch 022 / 034 | loss: 0.69111 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.73457 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.76447 | error: 0.42188\n",
      "  Ended batch 025 / 034 | loss: 0.79517 | error: 0.42969\n",
      "  Ended batch 026 / 034 | loss: 0.78923 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.73399 | error: 0.40625\n",
      "  Ended batch 028 / 034 | loss: 0.80443 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.71054 | error: 0.39844\n",
      "  Ended batch 030 / 034 | loss: 0.68036 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.74783 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.85516 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.76485 | error: 0.39062\n",
      "  Ended batch 034 / 034 | loss: 0.92902 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 010 / 016 | time: 100 sec | loss: 0.72593 | error: 0.25512\n",
      "Starting epoch 011 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.79528 | error: 0.42969\n",
      "  Ended batch 002 / 034 | loss: 0.75090 | error: 0.41406\n",
      "  Ended batch 003 / 034 | loss: 0.76714 | error: 0.42188\n",
      "  Ended batch 004 / 034 | loss: 0.68772 | error: 0.38281\n",
      "  Ended batch 005 / 034 | loss: 0.71650 | error: 0.39062\n",
      "  Ended batch 006 / 034 | loss: 0.57453 | error: 0.29688\n",
      "  Ended batch 007 / 034 | loss: 0.94213 | error: 0.53125\n",
      "  Ended batch 008 / 034 | loss: 0.74430 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.79885 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.67771 | error: 0.32812\n",
      "  Ended batch 011 / 034 | loss: 0.65463 | error: 0.28906\n",
      "  Ended batch 012 / 034 | loss: 0.80469 | error: 0.45312\n",
      "  Ended batch 013 / 034 | loss: 0.73152 | error: 0.27344\n",
      "  Ended batch 014 / 034 | loss: 0.79952 | error: 0.23438\n",
      "  Ended batch 015 / 034 | loss: 0.71620 | error: 0.30469\n",
      "  Ended batch 016 / 034 | loss: 0.69533 | error: 0.35938\n",
      "  Ended batch 017 / 034 | loss: 0.73693 | error: 0.41406\n",
      "  Ended batch 018 / 034 | loss: 0.71700 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.69592 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.83596 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.78435 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.70532 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.73675 | error: 0.38281\n",
      "  Ended batch 024 / 034 | loss: 0.74346 | error: 0.40625\n",
      "  Ended batch 025 / 034 | loss: 0.76945 | error: 0.41406\n",
      "  Ended batch 026 / 034 | loss: 0.82632 | error: 0.45312\n",
      "  Ended batch 027 / 034 | loss: 0.72506 | error: 0.39844\n",
      "  Ended batch 028 / 034 | loss: 0.77537 | error: 0.41406\n",
      "  Ended batch 029 / 034 | loss: 0.70334 | error: 0.39062\n",
      "  Ended batch 030 / 034 | loss: 0.66536 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.74125 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.88980 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.75191 | error: 0.38281\n",
      "  Ended batch 034 / 034 | loss: 0.91999 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 011 / 016 | time: 091 sec | loss: 0.76588 | error: 0.26553\n",
      "Starting epoch 012 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.77600 | error: 0.39062\n",
      "  Ended batch 002 / 034 | loss: 0.70759 | error: 0.27344\n",
      "  Ended batch 003 / 034 | loss: 0.75313 | error: 0.39062\n",
      "  Ended batch 004 / 034 | loss: 0.65693 | error: 0.34375\n",
      "  Ended batch 005 / 034 | loss: 0.68813 | error: 0.35938\n",
      "  Ended batch 006 / 034 | loss: 0.62275 | error: 0.33594\n",
      "  Ended batch 007 / 034 | loss: 0.95300 | error: 0.53125\n",
      "  Ended batch 008 / 034 | loss: 0.74757 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.79013 | error: 0.44531\n",
      "  Ended batch 010 / 034 | loss: 0.68095 | error: 0.32812\n",
      "  Ended batch 011 / 034 | loss: 0.62863 | error: 0.25781\n",
      "  Ended batch 012 / 034 | loss: 0.79142 | error: 0.46875\n",
      "  Ended batch 013 / 034 | loss: 0.73820 | error: 0.39062\n",
      "  Ended batch 014 / 034 | loss: 0.77261 | error: 0.37500\n",
      "  Ended batch 015 / 034 | loss: 0.78247 | error: 0.43750\n",
      "  Ended batch 016 / 034 | loss: 0.72606 | error: 0.39062\n",
      "  Ended batch 017 / 034 | loss: 0.74660 | error: 0.41406\n",
      "  Ended batch 018 / 034 | loss: 0.71756 | error: 0.38281\n",
      "  Ended batch 019 / 034 | loss: 0.68509 | error: 0.36719\n",
      "  Ended batch 020 / 034 | loss: 0.84833 | error: 0.44531\n",
      "  Ended batch 021 / 034 | loss: 0.81720 | error: 0.45312\n",
      "  Ended batch 022 / 034 | loss: 0.68884 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.73735 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.75598 | error: 0.41406\n",
      "  Ended batch 025 / 034 | loss: 0.77362 | error: 0.41406\n",
      "  Ended batch 026 / 034 | loss: 0.78855 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.74630 | error: 0.41406\n",
      "  Ended batch 028 / 034 | loss: 0.80771 | error: 0.43750\n",
      "  Ended batch 029 / 034 | loss: 0.74900 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.67460 | error: 0.35938\n",
      "  Ended batch 031 / 034 | loss: 0.73793 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.86451 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.78001 | error: 0.40625\n",
      "  Ended batch 034 / 034 | loss: 0.91978 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 012 / 016 | time: 094 sec | loss: 0.75139 | error: 0.26032\n",
      "Starting epoch 013 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.78657 | error: 0.36719\n",
      "  Ended batch 002 / 034 | loss: 0.71165 | error: 0.24219\n",
      "  Ended batch 003 / 034 | loss: 0.75117 | error: 0.36719\n",
      "  Ended batch 004 / 034 | loss: 0.64941 | error: 0.25000\n",
      "  Ended batch 005 / 034 | loss: 0.69232 | error: 0.32812\n",
      "  Ended batch 006 / 034 | loss: 0.58331 | error: 0.29688\n",
      "  Ended batch 007 / 034 | loss: 0.93991 | error: 0.51562\n",
      "  Ended batch 008 / 034 | loss: 0.77484 | error: 0.42188\n",
      "  Ended batch 009 / 034 | loss: 0.80623 | error: 0.44531\n",
      "  Ended batch 010 / 034 | loss: 0.67513 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.65761 | error: 0.33594\n",
      "  Ended batch 012 / 034 | loss: 0.78101 | error: 0.46094\n",
      "  Ended batch 013 / 034 | loss: 0.74383 | error: 0.31250\n",
      "  Ended batch 014 / 034 | loss: 0.80468 | error: 0.28906\n",
      "  Ended batch 015 / 034 | loss: 0.72763 | error: 0.35156\n",
      "  Ended batch 016 / 034 | loss: 0.69784 | error: 0.35938\n",
      "  Ended batch 017 / 034 | loss: 0.75292 | error: 0.42188\n",
      "  Ended batch 018 / 034 | loss: 0.70598 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.69751 | error: 0.37500\n",
      "  Ended batch 020 / 034 | loss: 0.83292 | error: 0.42969\n",
      "  Ended batch 021 / 034 | loss: 0.79770 | error: 0.43750\n",
      "  Ended batch 022 / 034 | loss: 0.68852 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.73834 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.74656 | error: 0.40625\n",
      "  Ended batch 025 / 034 | loss: 0.76424 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.80176 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.72761 | error: 0.39844\n",
      "  Ended batch 028 / 034 | loss: 0.79265 | error: 0.42969\n",
      "  Ended batch 029 / 034 | loss: 0.74874 | error: 0.42188\n",
      "  Ended batch 030 / 034 | loss: 0.66624 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.74019 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.86406 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.74935 | error: 0.38281\n",
      "  Ended batch 034 / 034 | loss: 0.93026 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 013 / 016 | time: 087 sec | loss: 0.73432 | error: 0.25251\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 21.1682554880778 minutes (1270.095329284668 seconds).\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.15_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.15_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "---------- Calculating results ----------\n",
      "Evaluating using epsilon: 0.15\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0903, -0.2800, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "         -0.0746,  0.0482, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919,  0.1656,\n",
      "          0.3650,  0.3795, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "          0.0652, -0.1662, -0.0495,  0.3771, -0.0304, -0.0473, -0.0763]])\n",
      "Evaluating using epsilon: 0.15\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0977, -0.2407, -0.0873, -0.0961, -0.0424, -0.0766, -0.0972, -0.1128,\n",
      "         -0.0633,  0.0368, -0.1009, -0.1946, -0.0778, -0.0544, -0.0845],\n",
      "        [-0.0945,  0.0952, -0.1055, -0.1150, -0.0407, -0.0894, -0.1089,  0.2033,\n",
      "          0.4012,  0.3432, -0.1307, -0.2040, -0.0845, -0.0589, -0.1024],\n",
      "        [-0.0802,  0.0189, -0.0895, -0.0984, -0.0422, -0.0781, -0.0986, -0.1097,\n",
      "          0.0794, -0.1805, -0.0666,  0.3717, -0.0343, -0.0499, -0.0866]])\n",
      "Evaluating using epsilon: 0.15\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0975, -0.2290, -0.0853, -0.0975, -0.0449, -0.0815, -0.0985, -0.1094,\n",
      "         -0.0600,  0.0339, -0.1062, -0.1901, -0.0750, -0.0523, -0.0851],\n",
      "        [-0.2440,  1.2057, -0.2309, -0.3270, -0.0857, -0.3297, -0.2524,  0.5329,\n",
      "          0.7196,  0.0349, -0.5436, -0.1854, -0.0811, -0.0519, -0.2891],\n",
      "        [-0.0860,  0.0761, -0.0922, -0.1082, -0.0468, -0.0931, -0.1057, -0.0929,\n",
      "          0.0958, -0.1960, -0.0889,  0.3775, -0.0310, -0.0473, -0.0947]])\n",
      "Evaluating using epsilon: 0.15\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1156,  0.2893, -0.2885, -0.2857, -0.2930, -0.2173, -0.1421, -0.2053,\n",
      "         -0.1374, -0.0548, -0.4108, -0.1938, -0.0628,  0.1222, -0.1583],\n",
      "        [-0.0932,  0.4693, -0.2630, -0.2616, -0.2626, -0.1995, -0.1360,  0.0947,\n",
      "          0.3098,  0.2891, -0.3714, -0.1934, -0.0643,  0.1009, -0.1483],\n",
      "        [-0.0923,  0.4485, -0.2553, -0.2543, -0.2534, -0.1941, -0.1341, -0.1925,\n",
      "          0.0124, -0.2529, -0.3218,  0.3741, -0.0204,  0.0996, -0.1453]])\n",
      "------------------------- 0.2 -------------------------\n",
      "---------- Training non-strategically with epsilon=0.2 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98923 | error: 0.43750\n",
      "  Ended batch 002 / 034 | loss: 0.83193 | error: 0.39844\n",
      "  Ended batch 003 / 034 | loss: 0.75601 | error: 0.38281\n",
      "  Ended batch 004 / 034 | loss: 0.62267 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.53573 | error: 0.26562\n",
      "  Ended batch 006 / 034 | loss: 0.43920 | error: 0.17188\n",
      "  Ended batch 007 / 034 | loss: 0.55319 | error: 0.24219\n",
      "  Ended batch 008 / 034 | loss: 0.43821 | error: 0.17188\n",
      "  Ended batch 009 / 034 | loss: 0.48477 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.44141 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.34122 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.39537 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.46204 | error: 0.17969\n",
      "  Ended batch 014 / 034 | loss: 0.61706 | error: 0.21094\n",
      "  Ended batch 015 / 034 | loss: 0.40735 | error: 0.15625\n",
      "  Ended batch 016 / 034 | loss: 0.44818 | error: 0.19531\n",
      "  Ended batch 017 / 034 | loss: 0.42789 | error: 0.17969\n",
      "  Ended batch 018 / 034 | loss: 0.45081 | error: 0.14062\n",
      "  Ended batch 019 / 034 | loss: 0.40077 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.67522 | error: 0.20312\n",
      "  Ended batch 021 / 034 | loss: 0.44442 | error: 0.17188\n",
      "  Ended batch 022 / 034 | loss: 0.49886 | error: 0.20312\n",
      "  Ended batch 023 / 034 | loss: 0.43399 | error: 0.16406\n",
      "  Ended batch 024 / 034 | loss: 0.42774 | error: 0.17188\n",
      "  Ended batch 025 / 034 | loss: 0.49798 | error: 0.17969\n",
      "  Ended batch 026 / 034 | loss: 0.48518 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34755 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.36209 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30258 | error: 0.14062\n",
      "  Ended batch 030 / 034 | loss: 0.27834 | error: 0.09375\n",
      "  Ended batch 031 / 034 | loss: 0.31573 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.46953 | error: 0.21094\n",
      "  Ended batch 033 / 034 | loss: 0.44510 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.35380 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 000 sec | loss: 0.45466 | error: 0.21103\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/non_strategic_0.2_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.40293 | error: 0.16406\n",
      "  Ended batch 002 / 034 | loss: 0.39698 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39748 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.33817 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31394 | error: 0.13281\n",
      "  Ended batch 006 / 034 | loss: 0.26311 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.40746 | error: 0.21094\n",
      "  Ended batch 008 / 034 | loss: 0.28619 | error: 0.12500\n",
      "  Ended batch 009 / 034 | loss: 0.42815 | error: 0.21875\n",
      "  Ended batch 010 / 034 | loss: 0.37308 | error: 0.14062\n",
      "  Ended batch 011 / 034 | loss: 0.24522 | error: 0.10938\n",
      "  Ended batch 012 / 034 | loss: 0.35200 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.38305 | error: 0.16406\n",
      "  Ended batch 014 / 034 | loss: 0.48793 | error: 0.18750\n",
      "  Ended batch 015 / 034 | loss: 0.29911 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37009 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39094 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.39047 | error: 0.16406\n",
      "  Ended batch 019 / 034 | loss: 0.33128 | error: 0.14844\n",
      "  Ended batch 020 / 034 | loss: 0.62716 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.39453 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.42599 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.40997 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38743 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.49943 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.46154 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34475 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.35799 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30337 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29895 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31889 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44511 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40606 | error: 0.16406\n",
      "  Ended batch 034 / 034 | loss: 0.41604 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 000 sec | loss: 0.44767 | error: 0.21363\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37960 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.38054 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39798 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.32809 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31529 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26527 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.39605 | error: 0.20312\n",
      "  Ended batch 008 / 034 | loss: 0.29298 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42259 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.37166 | error: 0.12500\n",
      "  Ended batch 011 / 034 | loss: 0.24852 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.34251 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37322 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47350 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31297 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.36932 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39082 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38728 | error: 0.15625\n",
      "  Ended batch 019 / 034 | loss: 0.33095 | error: 0.13281\n",
      "  Ended batch 020 / 034 | loss: 0.63589 | error: 0.24219\n",
      "  Ended batch 021 / 034 | loss: 0.39093 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41775 | error: 0.18750\n",
      "  Ended batch 023 / 034 | loss: 0.40417 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38116 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48604 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.45087 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34605 | error: 0.13281\n",
      "  Ended batch 028 / 034 | loss: 0.36658 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30254 | error: 0.10938\n",
      "  Ended batch 030 / 034 | loss: 0.30504 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31964 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44560 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40693 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.43542 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 000 sec | loss: 0.44874 | error: 0.21168\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37193 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37114 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39974 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31597 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31782 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26514 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.39098 | error: 0.18750\n",
      "  Ended batch 008 / 034 | loss: 0.28986 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42376 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.36756 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24499 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34379 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37043 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47858 | error: 0.20312\n",
      "  Ended batch 015 / 034 | loss: 0.30862 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37446 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38862 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38593 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32783 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64187 | error: 0.25000\n",
      "  Ended batch 021 / 034 | loss: 0.38988 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41519 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39861 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37473 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48543 | error: 0.21094\n",
      "  Ended batch 026 / 034 | loss: 0.44854 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34043 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36239 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.29949 | error: 0.11719\n",
      "  Ended batch 030 / 034 | loss: 0.29681 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31391 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45120 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40857 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42124 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 000 sec | loss: 0.44502 | error: 0.21103\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37063 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37067 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39355 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31063 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31461 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26898 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.38740 | error: 0.19531\n",
      "  Ended batch 008 / 034 | loss: 0.28760 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42075 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.36434 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24144 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34192 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.36529 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47503 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31060 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37830 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38680 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38255 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32599 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64071 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.38804 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41321 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39709 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37160 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48514 | error: 0.20312\n",
      "  Ended batch 026 / 034 | loss: 0.44696 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34037 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36098 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30023 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29524 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31327 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45363 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40813 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42413 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 000 sec | loss: 0.44367 | error: 0.21168\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 0.7780165672302246 seconds.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.2_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.2_model.pt.\n",
      "---------- Training strategically with epsilon=0.2 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.96411 | error: 0.45312\n",
      "  Ended batch 002 / 034 | loss: 1.04023 | error: 0.48438\n",
      "  Ended batch 003 / 034 | loss: 1.00034 | error: 0.52344\n",
      "  Ended batch 004 / 034 | loss: 0.89281 | error: 0.47656\n",
      "  Ended batch 005 / 034 | loss: 0.87306 | error: 0.32812\n",
      "  Ended batch 006 / 034 | loss: 0.78484 | error: 0.32812\n",
      "  Ended batch 007 / 034 | loss: 0.99196 | error: 0.56250\n",
      "  Ended batch 008 / 034 | loss: 0.81151 | error: 0.46094\n",
      "  Ended batch 009 / 034 | loss: 0.86221 | error: 0.46875\n",
      "  Ended batch 010 / 034 | loss: 0.74528 | error: 0.38281\n",
      "  Ended batch 011 / 034 | loss: 0.73143 | error: 0.36719\n",
      "  Ended batch 012 / 034 | loss: 0.92508 | error: 0.53906\n",
      "  Ended batch 013 / 034 | loss: 0.79317 | error: 0.42969\n",
      "  Ended batch 014 / 034 | loss: 0.77264 | error: 0.38281\n",
      "  Ended batch 015 / 034 | loss: 0.81755 | error: 0.46875\n",
      "  Ended batch 016 / 034 | loss: 0.78811 | error: 0.43750\n",
      "  Ended batch 017 / 034 | loss: 0.75314 | error: 0.42188\n",
      "  Ended batch 018 / 034 | loss: 0.73750 | error: 0.39062\n",
      "  Ended batch 019 / 034 | loss: 0.67138 | error: 0.34375\n",
      "  Ended batch 020 / 034 | loss: 0.84059 | error: 0.44531\n",
      "  Ended batch 021 / 034 | loss: 0.77786 | error: 0.42188\n",
      "  Ended batch 022 / 034 | loss: 0.67861 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.70358 | error: 0.37500\n",
      "  Ended batch 024 / 034 | loss: 0.73347 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.75897 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.74622 | error: 0.38281\n",
      "  Ended batch 027 / 034 | loss: 0.65689 | error: 0.33594\n",
      "  Ended batch 028 / 034 | loss: 0.74451 | error: 0.37500\n",
      "  Ended batch 029 / 034 | loss: 0.58430 | error: 0.30469\n",
      "  Ended batch 030 / 034 | loss: 0.61959 | error: 0.32812\n",
      "  Ended batch 031 / 034 | loss: 0.72188 | error: 0.37500\n",
      "  Ended batch 032 / 034 | loss: 0.81558 | error: 0.42188\n",
      "  Ended batch 033 / 034 | loss: 0.67328 | error: 0.33594\n",
      "  Ended batch 034 / 034 | loss: 0.91837 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 076 sec | loss: 1.02414 | error: 0.37751\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.2_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.70960 | error: 0.35156\n",
      "  Ended batch 002 / 034 | loss: 0.64524 | error: 0.31250\n",
      "  Ended batch 003 / 034 | loss: 0.73547 | error: 0.39062\n",
      "  Ended batch 004 / 034 | loss: 0.65041 | error: 0.35156\n",
      "  Ended batch 005 / 034 | loss: 0.67581 | error: 0.36719\n",
      "  Ended batch 006 / 034 | loss: 0.57095 | error: 0.30469\n",
      "  Ended batch 007 / 034 | loss: 0.91779 | error: 0.50000\n",
      "  Ended batch 008 / 034 | loss: 0.69882 | error: 0.38281\n",
      "  Ended batch 009 / 034 | loss: 0.73602 | error: 0.39844\n",
      "  Ended batch 010 / 034 | loss: 0.64788 | error: 0.30469\n",
      "  Ended batch 011 / 034 | loss: 0.59477 | error: 0.28906\n",
      "  Ended batch 012 / 034 | loss: 0.76442 | error: 0.43750\n",
      "  Ended batch 013 / 034 | loss: 0.70035 | error: 0.35938\n",
      "  Ended batch 014 / 034 | loss: 0.76622 | error: 0.37500\n",
      "  Ended batch 015 / 034 | loss: 0.73298 | error: 0.39844\n",
      "  Ended batch 016 / 034 | loss: 0.68862 | error: 0.36719\n",
      "  Ended batch 017 / 034 | loss: 0.71349 | error: 0.38281\n",
      "  Ended batch 018 / 034 | loss: 0.65225 | error: 0.34375\n",
      "  Ended batch 019 / 034 | loss: 0.66507 | error: 0.35156\n",
      "  Ended batch 020 / 034 | loss: 0.75844 | error: 0.39062\n",
      "  Ended batch 021 / 034 | loss: 0.75854 | error: 0.41406\n",
      "  Ended batch 022 / 034 | loss: 0.66085 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.73429 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.72147 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.76565 | error: 0.41406\n",
      "  Ended batch 026 / 034 | loss: 0.79998 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.71631 | error: 0.39844\n",
      "  Ended batch 028 / 034 | loss: 0.76509 | error: 0.41406\n",
      "  Ended batch 029 / 034 | loss: 0.64070 | error: 0.30469\n",
      "  Ended batch 030 / 034 | loss: 0.66755 | error: 0.32812\n",
      "  Ended batch 031 / 034 | loss: 0.72624 | error: 0.39062\n",
      "  Ended batch 032 / 034 | loss: 0.83525 | error: 0.46094\n",
      "  Ended batch 033 / 034 | loss: 0.78654 | error: 0.41406\n",
      "  Ended batch 034 / 034 | loss: 0.92493 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 087 sec | loss: 0.68226 | error: 0.24014\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.2_model.pt.\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.78705 | error: 0.42969\n",
      "  Ended batch 002 / 034 | loss: 0.73092 | error: 0.40625\n",
      "  Ended batch 003 / 034 | loss: 0.75733 | error: 0.42188\n",
      "  Ended batch 004 / 034 | loss: 0.68640 | error: 0.39062\n",
      "  Ended batch 005 / 034 | loss: 0.68105 | error: 0.35938\n",
      "  Ended batch 006 / 034 | loss: 0.58889 | error: 0.28906\n",
      "  Ended batch 007 / 034 | loss: 0.85640 | error: 0.50000\n",
      "  Ended batch 008 / 034 | loss: 0.71961 | error: 0.40625\n",
      "  Ended batch 009 / 034 | loss: 0.76637 | error: 0.43750\n",
      "  Ended batch 010 / 034 | loss: 0.65405 | error: 0.32812\n",
      "  Ended batch 011 / 034 | loss: 0.63677 | error: 0.32812\n",
      "  Ended batch 012 / 034 | loss: 0.79338 | error: 0.46875\n",
      "  Ended batch 013 / 034 | loss: 0.71310 | error: 0.37500\n",
      "  Ended batch 014 / 034 | loss: 0.77662 | error: 0.38281\n",
      "  Ended batch 015 / 034 | loss: 0.75041 | error: 0.42188\n",
      "  Ended batch 016 / 034 | loss: 0.68630 | error: 0.36719\n",
      "  Ended batch 017 / 034 | loss: 0.73511 | error: 0.41406\n",
      "  Ended batch 018 / 034 | loss: 0.69680 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.65578 | error: 0.35156\n",
      "  Ended batch 020 / 034 | loss: 0.81772 | error: 0.42969\n",
      "  Ended batch 021 / 034 | loss: 0.78667 | error: 0.43750\n",
      "  Ended batch 022 / 034 | loss: 0.66723 | error: 0.32812\n",
      "  Ended batch 023 / 034 | loss: 0.72540 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.77141 | error: 0.43750\n",
      "  Ended batch 025 / 034 | loss: 0.76007 | error: 0.41406\n",
      "  Ended batch 026 / 034 | loss: 0.81094 | error: 0.45312\n",
      "  Ended batch 027 / 034 | loss: 0.72188 | error: 0.40625\n",
      "  Ended batch 028 / 034 | loss: 0.75711 | error: 0.41406\n",
      "  Ended batch 029 / 034 | loss: 0.67478 | error: 0.37500\n",
      "  Ended batch 030 / 034 | loss: 0.65424 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.72967 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.86784 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.75595 | error: 0.39062\n",
      "  Ended batch 034 / 034 | loss: 0.91678 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 074 sec | loss: 0.71315 | error: 0.24730\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.79053 | error: 0.42969\n",
      "  Ended batch 002 / 034 | loss: 0.68979 | error: 0.32812\n",
      "  Ended batch 003 / 034 | loss: 0.74062 | error: 0.35938\n",
      "  Ended batch 004 / 034 | loss: 0.64540 | error: 0.25781\n",
      "  Ended batch 005 / 034 | loss: 0.66610 | error: 0.29688\n",
      "  Ended batch 006 / 034 | loss: 0.58358 | error: 0.28906\n",
      "  Ended batch 007 / 034 | loss: 0.88720 | error: 0.50781\n",
      "  Ended batch 008 / 034 | loss: 0.73981 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.80131 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.66299 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.62345 | error: 0.32031\n",
      "  Ended batch 012 / 034 | loss: 0.77481 | error: 0.46094\n",
      "  Ended batch 013 / 034 | loss: 0.73100 | error: 0.36719\n",
      "  Ended batch 014 / 034 | loss: 0.78232 | error: 0.31250\n",
      "  Ended batch 015 / 034 | loss: 0.70521 | error: 0.35156\n",
      "  Ended batch 016 / 034 | loss: 0.67058 | error: 0.34375\n",
      "  Ended batch 017 / 034 | loss: 0.72449 | error: 0.40625\n",
      "  Ended batch 018 / 034 | loss: 0.68590 | error: 0.36719\n",
      "  Ended batch 019 / 034 | loss: 0.67525 | error: 0.36719\n",
      "  Ended batch 020 / 034 | loss: 0.83103 | error: 0.42969\n",
      "  Ended batch 021 / 034 | loss: 0.77949 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.69135 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.72969 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.76129 | error: 0.42969\n",
      "  Ended batch 025 / 034 | loss: 0.76059 | error: 0.41406\n",
      "  Ended batch 026 / 034 | loss: 0.81371 | error: 0.45312\n",
      "  Ended batch 027 / 034 | loss: 0.72447 | error: 0.40625\n",
      "  Ended batch 028 / 034 | loss: 0.74941 | error: 0.40625\n",
      "  Ended batch 029 / 034 | loss: 0.67838 | error: 0.36719\n",
      "  Ended batch 030 / 034 | loss: 0.66195 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.73623 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.86223 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.75754 | error: 0.39062\n",
      "  Ended batch 034 / 034 | loss: 0.91473 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 091 sec | loss: 0.70655 | error: 0.24340\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.78779 | error: 0.42969\n",
      "  Ended batch 002 / 034 | loss: 0.72940 | error: 0.40625\n",
      "  Ended batch 003 / 034 | loss: 0.74448 | error: 0.40625\n",
      "  Ended batch 004 / 034 | loss: 0.64644 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.66694 | error: 0.31250\n",
      "  Ended batch 006 / 034 | loss: 0.58905 | error: 0.29688\n",
      "  Ended batch 007 / 034 | loss: 0.88808 | error: 0.50781\n",
      "  Ended batch 008 / 034 | loss: 0.73959 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.80089 | error: 0.45312\n",
      "  Ended batch 010 / 034 | loss: 0.67124 | error: 0.35156\n",
      "  Ended batch 011 / 034 | loss: 0.62577 | error: 0.31250\n",
      "  Ended batch 012 / 034 | loss: 0.75007 | error: 0.43750\n",
      "  Ended batch 013 / 034 | loss: 0.71957 | error: 0.29688\n",
      "  Ended batch 014 / 034 | loss: 0.77560 | error: 0.30469\n",
      "  Ended batch 015 / 034 | loss: 0.68270 | error: 0.34375\n",
      "  Ended batch 016 / 034 | loss: 0.68825 | error: 0.35938\n",
      "  Ended batch 017 / 034 | loss: 0.70543 | error: 0.39062\n",
      "  Ended batch 018 / 034 | loss: 0.70158 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.66559 | error: 0.35938\n",
      "  Ended batch 020 / 034 | loss: 0.78695 | error: 0.40625\n",
      "  Ended batch 021 / 034 | loss: 0.76512 | error: 0.42188\n",
      "  Ended batch 022 / 034 | loss: 0.70158 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.69415 | error: 0.36719\n",
      "  Ended batch 024 / 034 | loss: 0.71008 | error: 0.38281\n",
      "  Ended batch 025 / 034 | loss: 0.77406 | error: 0.42188\n",
      "  Ended batch 026 / 034 | loss: 0.79247 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.71697 | error: 0.39844\n",
      "  Ended batch 028 / 034 | loss: 0.75953 | error: 0.41406\n",
      "  Ended batch 029 / 034 | loss: 0.69544 | error: 0.39062\n",
      "  Ended batch 030 / 034 | loss: 0.64696 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.73008 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.86084 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.74412 | error: 0.38281\n",
      "  Ended batch 034 / 034 | loss: 0.91672 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 075 sec | loss: 0.72831 | error: 0.24860\n",
      "Starting epoch 006 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.79186 | error: 0.42969\n",
      "  Ended batch 002 / 034 | loss: 0.68752 | error: 0.36719\n",
      "  Ended batch 003 / 034 | loss: 0.74737 | error: 0.41406\n",
      "  Ended batch 004 / 034 | loss: 0.65427 | error: 0.35938\n",
      "  Ended batch 005 / 034 | loss: 0.67326 | error: 0.34375\n",
      "  Ended batch 006 / 034 | loss: 0.58081 | error: 0.30469\n",
      "  Ended batch 007 / 034 | loss: 0.89729 | error: 0.51562\n",
      "  Ended batch 008 / 034 | loss: 0.72353 | error: 0.40625\n",
      "  Ended batch 009 / 034 | loss: 0.76298 | error: 0.42969\n",
      "  Ended batch 010 / 034 | loss: 0.64980 | error: 0.32812\n",
      "  Ended batch 011 / 034 | loss: 0.62850 | error: 0.32812\n",
      "  Ended batch 012 / 034 | loss: 0.80068 | error: 0.46875\n",
      "  Ended batch 013 / 034 | loss: 0.70370 | error: 0.37500\n",
      "  Ended batch 014 / 034 | loss: 0.76070 | error: 0.37500\n",
      "  Ended batch 015 / 034 | loss: 0.75433 | error: 0.42188\n",
      "  Ended batch 016 / 034 | loss: 0.69382 | error: 0.37500\n",
      "  Ended batch 017 / 034 | loss: 0.73447 | error: 0.41406\n",
      "  Ended batch 018 / 034 | loss: 0.68701 | error: 0.36719\n",
      "  Ended batch 019 / 034 | loss: 0.66765 | error: 0.35938\n",
      "  Ended batch 020 / 034 | loss: 0.79916 | error: 0.42188\n",
      "  Ended batch 021 / 034 | loss: 0.77689 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.66664 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.72674 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.74628 | error: 0.41406\n",
      "  Ended batch 025 / 034 | loss: 0.77997 | error: 0.42969\n",
      "  Ended batch 026 / 034 | loss: 0.81418 | error: 0.45312\n",
      "  Ended batch 027 / 034 | loss: 0.70427 | error: 0.39062\n",
      "  Ended batch 028 / 034 | loss: 0.76546 | error: 0.42188\n",
      "  Ended batch 029 / 034 | loss: 0.64238 | error: 0.32031\n",
      "  Ended batch 030 / 034 | loss: 0.66134 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.73475 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.88726 | error: 0.48438\n",
      "  Ended batch 033 / 034 | loss: 0.77150 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.91548 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 006 / 016 | time: 074 sec | loss: 0.69835 | error: 0.24405\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 7.951301256815593 minutes (477.07807540893555 seconds).\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.2_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.2_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "---------- Calculating results ----------\n",
      "Evaluating using epsilon: 0.2\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0903, -0.2800, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "         -0.0746,  0.0482, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919,  0.1656,\n",
      "          0.3650,  0.3795, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "          0.0652, -0.1662, -0.0495,  0.3771, -0.0304, -0.0473, -0.0763]])\n",
      "Evaluating using epsilon: 0.2\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0957, -0.2510, -0.0852, -0.0939, -0.0426, -0.0751, -0.0958, -0.1159,\n",
      "         -0.0663,  0.0398, -0.0973, -0.1934, -0.0770, -0.0538, -0.0823],\n",
      "        [-0.0872,  0.0560, -0.0973, -0.1065, -0.0414, -0.0836, -0.1036,  0.1916,\n",
      "          0.3899,  0.3545, -0.1172, -0.1997, -0.0815, -0.0569, -0.0943],\n",
      "        [-0.0778,  0.0057, -0.0867, -0.0955, -0.0424, -0.0762, -0.0968, -0.1137,\n",
      "          0.0757, -0.1767, -0.0621,  0.3731, -0.0333, -0.0492, -0.0839]])\n",
      "Evaluating using epsilon: 0.2\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0999, -0.2291, -0.0924, -0.0982, -0.0400, -0.0783, -0.0979, -0.1081,\n",
      "         -0.0620,  0.0344, -0.1053, -0.1939, -0.0766, -0.0533, -0.0890],\n",
      "        [-0.2560,  0.9507, -0.3371, -0.2937,  0.0177, -0.2159, -0.2081,  0.4823,\n",
      "          0.6074,  0.1146, -0.4341, -0.2603, -0.1112, -0.0715, -0.3203],\n",
      "        [-0.0932,  0.0870, -0.1100, -0.1123, -0.0358, -0.0882, -0.1058, -0.0866,\n",
      "          0.0943, -0.1980, -0.0911,  0.3687, -0.0348, -0.0496, -0.1056]])\n",
      "Evaluating using epsilon: 0.2\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1140,  0.2538, -0.2754, -0.2733, -0.2774, -0.2082, -0.1390, -0.2003,\n",
      "         -0.1335, -0.0484, -0.3907, -0.1936, -0.0636,  0.1113, -0.1532],\n",
      "        [-0.0916,  0.4340, -0.2500, -0.2493, -0.2471, -0.1904, -0.1329,  0.0997,\n",
      "          0.3137,  0.2954, -0.3514, -0.1932, -0.0650,  0.0901, -0.1433],\n",
      "        [-0.0907,  0.4133, -0.2424, -0.2421, -0.2380, -0.1851, -0.1310, -0.1875,\n",
      "          0.0162, -0.2465, -0.3018,  0.3743, -0.0211,  0.0888, -0.1403]])\n",
      "------------------------- 0.3 -------------------------\n",
      "---------- Training non-strategically with epsilon=0.3 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98923 | error: 0.43750\n",
      "  Ended batch 002 / 034 | loss: 0.83193 | error: 0.39844\n",
      "  Ended batch 003 / 034 | loss: 0.75601 | error: 0.38281\n",
      "  Ended batch 004 / 034 | loss: 0.62267 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.53573 | error: 0.26562\n",
      "  Ended batch 006 / 034 | loss: 0.43920 | error: 0.17188\n",
      "  Ended batch 007 / 034 | loss: 0.55319 | error: 0.24219\n",
      "  Ended batch 008 / 034 | loss: 0.43821 | error: 0.17188\n",
      "  Ended batch 009 / 034 | loss: 0.48477 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.44141 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.34122 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.39537 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.46204 | error: 0.17969\n",
      "  Ended batch 014 / 034 | loss: 0.61706 | error: 0.21094\n",
      "  Ended batch 015 / 034 | loss: 0.40735 | error: 0.15625\n",
      "  Ended batch 016 / 034 | loss: 0.44818 | error: 0.19531\n",
      "  Ended batch 017 / 034 | loss: 0.42789 | error: 0.17969\n",
      "  Ended batch 018 / 034 | loss: 0.45081 | error: 0.14062\n",
      "  Ended batch 019 / 034 | loss: 0.40077 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.67522 | error: 0.20312\n",
      "  Ended batch 021 / 034 | loss: 0.44442 | error: 0.17188\n",
      "  Ended batch 022 / 034 | loss: 0.49886 | error: 0.20312\n",
      "  Ended batch 023 / 034 | loss: 0.43399 | error: 0.16406\n",
      "  Ended batch 024 / 034 | loss: 0.42774 | error: 0.17188\n",
      "  Ended batch 025 / 034 | loss: 0.49798 | error: 0.17969\n",
      "  Ended batch 026 / 034 | loss: 0.48518 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34755 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.36209 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30258 | error: 0.14062\n",
      "  Ended batch 030 / 034 | loss: 0.27834 | error: 0.09375\n",
      "  Ended batch 031 / 034 | loss: 0.31573 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.46953 | error: 0.21094\n",
      "  Ended batch 033 / 034 | loss: 0.44510 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.35380 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 000 sec | loss: 0.45466 | error: 0.21103\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/non_strategic_0.3_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.40293 | error: 0.16406\n",
      "  Ended batch 002 / 034 | loss: 0.39698 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39748 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.33817 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31394 | error: 0.13281\n",
      "  Ended batch 006 / 034 | loss: 0.26311 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.40746 | error: 0.21094\n",
      "  Ended batch 008 / 034 | loss: 0.28619 | error: 0.12500\n",
      "  Ended batch 009 / 034 | loss: 0.42815 | error: 0.21875\n",
      "  Ended batch 010 / 034 | loss: 0.37308 | error: 0.14062\n",
      "  Ended batch 011 / 034 | loss: 0.24522 | error: 0.10938\n",
      "  Ended batch 012 / 034 | loss: 0.35200 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.38305 | error: 0.16406\n",
      "  Ended batch 014 / 034 | loss: 0.48793 | error: 0.18750\n",
      "  Ended batch 015 / 034 | loss: 0.29911 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37009 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39094 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.39047 | error: 0.16406\n",
      "  Ended batch 019 / 034 | loss: 0.33128 | error: 0.14844\n",
      "  Ended batch 020 / 034 | loss: 0.62716 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.39453 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.42599 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.40997 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38743 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.49943 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.46154 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34475 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.35799 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30337 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29895 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31889 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44511 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40606 | error: 0.16406\n",
      "  Ended batch 034 / 034 | loss: 0.41604 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 000 sec | loss: 0.44767 | error: 0.21363\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37960 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.38054 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39798 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.32809 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31529 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26527 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.39605 | error: 0.20312\n",
      "  Ended batch 008 / 034 | loss: 0.29298 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42259 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.37166 | error: 0.12500\n",
      "  Ended batch 011 / 034 | loss: 0.24852 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.34251 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37322 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47350 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31297 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.36932 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39082 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38728 | error: 0.15625\n",
      "  Ended batch 019 / 034 | loss: 0.33095 | error: 0.13281\n",
      "  Ended batch 020 / 034 | loss: 0.63589 | error: 0.24219\n",
      "  Ended batch 021 / 034 | loss: 0.39093 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41775 | error: 0.18750\n",
      "  Ended batch 023 / 034 | loss: 0.40417 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38116 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48604 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.45087 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34605 | error: 0.13281\n",
      "  Ended batch 028 / 034 | loss: 0.36658 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30254 | error: 0.10938\n",
      "  Ended batch 030 / 034 | loss: 0.30504 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31964 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44560 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40693 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.43542 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 000 sec | loss: 0.44874 | error: 0.21168\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37193 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37114 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39974 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31597 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31782 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26514 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.39098 | error: 0.18750\n",
      "  Ended batch 008 / 034 | loss: 0.28986 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42376 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.36756 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24499 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34379 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37043 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47858 | error: 0.20312\n",
      "  Ended batch 015 / 034 | loss: 0.30862 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37446 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38862 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38593 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32783 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64187 | error: 0.25000\n",
      "  Ended batch 021 / 034 | loss: 0.38988 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41519 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39861 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37473 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48543 | error: 0.21094\n",
      "  Ended batch 026 / 034 | loss: 0.44854 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34043 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36239 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.29949 | error: 0.11719\n",
      "  Ended batch 030 / 034 | loss: 0.29681 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31391 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45120 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40857 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42124 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 000 sec | loss: 0.44502 | error: 0.21103\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37063 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37067 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39355 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31063 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31461 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26898 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.38740 | error: 0.19531\n",
      "  Ended batch 008 / 034 | loss: 0.28760 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42075 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.36434 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24144 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34192 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.36529 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47503 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31060 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37830 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38680 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38255 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32599 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64071 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.38804 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41321 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39709 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37160 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48514 | error: 0.20312\n",
      "  Ended batch 026 / 034 | loss: 0.44696 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34037 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36098 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30023 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29524 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31327 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45363 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40813 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42413 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 000 sec | loss: 0.44367 | error: 0.21168\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 0.8500244617462158 seconds.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.3_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.3_model.pt.\n",
      "---------- Training strategically with epsilon=0.3 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.97190 | error: 0.45312\n",
      "  Ended batch 002 / 034 | loss: 0.95149 | error: 0.48438\n",
      "  Ended batch 003 / 034 | loss: 0.94833 | error: 0.52344\n",
      "  Ended batch 004 / 034 | loss: 0.83494 | error: 0.45312\n",
      "  Ended batch 005 / 034 | loss: 0.78146 | error: 0.39844\n",
      "  Ended batch 006 / 034 | loss: 0.70385 | error: 0.39062\n",
      "  Ended batch 007 / 034 | loss: 0.94655 | error: 0.54688\n",
      "  Ended batch 008 / 034 | loss: 0.77173 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.81693 | error: 0.44531\n",
      "  Ended batch 010 / 034 | loss: 0.73132 | error: 0.32031\n",
      "  Ended batch 011 / 034 | loss: 0.71044 | error: 0.30469\n",
      "  Ended batch 012 / 034 | loss: 0.83147 | error: 0.46094\n",
      "  Ended batch 013 / 034 | loss: 0.74544 | error: 0.33594\n",
      "  Ended batch 014 / 034 | loss: 0.76822 | error: 0.31250\n",
      "  Ended batch 015 / 034 | loss: 0.74598 | error: 0.38281\n",
      "  Ended batch 016 / 034 | loss: 0.69707 | error: 0.34375\n",
      "  Ended batch 017 / 034 | loss: 0.66989 | error: 0.36719\n",
      "  Ended batch 018 / 034 | loss: 0.70006 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.64140 | error: 0.33594\n",
      "  Ended batch 020 / 034 | loss: 0.81390 | error: 0.41406\n",
      "  Ended batch 021 / 034 | loss: 0.78830 | error: 0.40625\n",
      "  Ended batch 022 / 034 | loss: 0.67681 | error: 0.32812\n",
      "  Ended batch 023 / 034 | loss: 0.61826 | error: 0.32031\n",
      "  Ended batch 024 / 034 | loss: 0.71361 | error: 0.35938\n",
      "  Ended batch 025 / 034 | loss: 0.70104 | error: 0.35938\n",
      "  Ended batch 026 / 034 | loss: 0.67734 | error: 0.33594\n",
      "  Ended batch 027 / 034 | loss: 0.60098 | error: 0.31250\n",
      "  Ended batch 028 / 034 | loss: 0.72698 | error: 0.37500\n",
      "  Ended batch 029 / 034 | loss: 0.57773 | error: 0.30469\n",
      "  Ended batch 030 / 034 | loss: 0.59355 | error: 0.31250\n",
      "  Ended batch 031 / 034 | loss: 0.61335 | error: 0.32031\n",
      "  Ended batch 032 / 034 | loss: 0.68264 | error: 0.35938\n",
      "  Ended batch 033 / 034 | loss: 0.65927 | error: 0.32812\n",
      "  Ended batch 034 / 034 | loss: 0.90825 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 072 sec | loss: 0.99635 | error: 0.36709\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.3_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.68071 | error: 0.35156\n",
      "  Ended batch 002 / 034 | loss: 0.59377 | error: 0.30469\n",
      "  Ended batch 003 / 034 | loss: 0.73860 | error: 0.39844\n",
      "  Ended batch 004 / 034 | loss: 0.69935 | error: 0.37500\n",
      "  Ended batch 005 / 034 | loss: 0.65499 | error: 0.35938\n",
      "  Ended batch 006 / 034 | loss: 0.54075 | error: 0.28906\n",
      "  Ended batch 007 / 034 | loss: 0.77006 | error: 0.42969\n",
      "  Ended batch 008 / 034 | loss: 0.60912 | error: 0.32812\n",
      "  Ended batch 009 / 034 | loss: 0.70760 | error: 0.38281\n",
      "  Ended batch 010 / 034 | loss: 0.65082 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.55446 | error: 0.29688\n",
      "  Ended batch 012 / 034 | loss: 0.78142 | error: 0.42969\n",
      "  Ended batch 013 / 034 | loss: 0.66777 | error: 0.35156\n",
      "  Ended batch 014 / 034 | loss: 0.73943 | error: 0.36719\n",
      "  Ended batch 015 / 034 | loss: 0.68782 | error: 0.37500\n",
      "  Ended batch 016 / 034 | loss: 0.61838 | error: 0.32031\n",
      "  Ended batch 017 / 034 | loss: 0.70921 | error: 0.39062\n",
      "  Ended batch 018 / 034 | loss: 0.67142 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.59075 | error: 0.30469\n",
      "  Ended batch 020 / 034 | loss: 0.75429 | error: 0.39062\n",
      "  Ended batch 021 / 034 | loss: 0.73176 | error: 0.39062\n",
      "  Ended batch 022 / 034 | loss: 0.67484 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.69456 | error: 0.38281\n",
      "  Ended batch 024 / 034 | loss: 0.70750 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.73919 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.78193 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.70677 | error: 0.40625\n",
      "  Ended batch 028 / 034 | loss: 0.72110 | error: 0.39844\n",
      "  Ended batch 029 / 034 | loss: 0.62466 | error: 0.33594\n",
      "  Ended batch 030 / 034 | loss: 0.63805 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.72077 | error: 0.40625\n",
      "  Ended batch 032 / 034 | loss: 0.83152 | error: 0.46094\n",
      "  Ended batch 033 / 034 | loss: 0.77777 | error: 0.41406\n",
      "  Ended batch 034 / 034 | loss: 0.88607 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 076 sec | loss: 0.66468 | error: 0.22712\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.3_model.pt.\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.75498 | error: 0.39062\n",
      "  Ended batch 002 / 034 | loss: 0.65858 | error: 0.21094\n",
      "  Ended batch 003 / 034 | loss: 0.70237 | error: 0.29688\n",
      "  Ended batch 004 / 034 | loss: 0.62520 | error: 0.23438\n",
      "  Ended batch 005 / 034 | loss: 0.64138 | error: 0.27344\n",
      "  Ended batch 006 / 034 | loss: 0.53804 | error: 0.25781\n",
      "  Ended batch 007 / 034 | loss: 0.89433 | error: 0.51562\n",
      "  Ended batch 008 / 034 | loss: 0.75004 | error: 0.42188\n",
      "  Ended batch 009 / 034 | loss: 0.76949 | error: 0.42969\n",
      "  Ended batch 010 / 034 | loss: 0.63644 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.58976 | error: 0.31250\n",
      "  Ended batch 012 / 034 | loss: 0.72118 | error: 0.42188\n",
      "  Ended batch 013 / 034 | loss: 0.70599 | error: 0.36719\n",
      "  Ended batch 014 / 034 | loss: 0.76575 | error: 0.36719\n",
      "  Ended batch 015 / 034 | loss: 0.68969 | error: 0.38281\n",
      "  Ended batch 016 / 034 | loss: 0.64401 | error: 0.33594\n",
      "  Ended batch 017 / 034 | loss: 0.67670 | error: 0.37500\n",
      "  Ended batch 018 / 034 | loss: 0.66402 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.63272 | error: 0.34375\n",
      "  Ended batch 020 / 034 | loss: 0.83002 | error: 0.43750\n",
      "  Ended batch 021 / 034 | loss: 0.75397 | error: 0.42188\n",
      "  Ended batch 022 / 034 | loss: 0.66157 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.66892 | error: 0.35938\n",
      "  Ended batch 024 / 034 | loss: 0.71011 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.73683 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.76430 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.65314 | error: 0.35938\n",
      "  Ended batch 028 / 034 | loss: 0.74095 | error: 0.41406\n",
      "  Ended batch 029 / 034 | loss: 0.65051 | error: 0.36719\n",
      "  Ended batch 030 / 034 | loss: 0.62656 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.72530 | error: 0.40625\n",
      "  Ended batch 032 / 034 | loss: 0.82774 | error: 0.46094\n",
      "  Ended batch 033 / 034 | loss: 0.72973 | error: 0.37500\n",
      "  Ended batch 034 / 034 | loss: 0.87470 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 075 sec | loss: 0.73021 | error: 0.24535\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.69989 | error: 0.35938\n",
      "  Ended batch 002 / 034 | loss: 0.66273 | error: 0.24219\n",
      "  Ended batch 003 / 034 | loss: 0.70698 | error: 0.32812\n",
      "  Ended batch 004 / 034 | loss: 0.62885 | error: 0.29688\n",
      "  Ended batch 005 / 034 | loss: 0.61932 | error: 0.32031\n",
      "  Ended batch 006 / 034 | loss: 0.53962 | error: 0.28906\n",
      "  Ended batch 007 / 034 | loss: 0.89678 | error: 0.50000\n",
      "  Ended batch 008 / 034 | loss: 0.74981 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.77622 | error: 0.42969\n",
      "  Ended batch 010 / 034 | loss: 0.63680 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.58465 | error: 0.30469\n",
      "  Ended batch 012 / 034 | loss: 0.69617 | error: 0.39844\n",
      "  Ended batch 013 / 034 | loss: 0.70275 | error: 0.34375\n",
      "  Ended batch 014 / 034 | loss: 0.76987 | error: 0.34375\n",
      "  Ended batch 015 / 034 | loss: 0.67557 | error: 0.36719\n",
      "  Ended batch 016 / 034 | loss: 0.64505 | error: 0.33594\n",
      "  Ended batch 017 / 034 | loss: 0.67905 | error: 0.37500\n",
      "  Ended batch 018 / 034 | loss: 0.67550 | error: 0.36719\n",
      "  Ended batch 019 / 034 | loss: 0.63218 | error: 0.34375\n",
      "  Ended batch 020 / 034 | loss: 0.78237 | error: 0.40625\n",
      "  Ended batch 021 / 034 | loss: 0.76804 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.68882 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.66734 | error: 0.35938\n",
      "  Ended batch 024 / 034 | loss: 0.70055 | error: 0.38281\n",
      "  Ended batch 025 / 034 | loss: 0.73019 | error: 0.39844\n",
      "  Ended batch 026 / 034 | loss: 0.75848 | error: 0.41406\n",
      "  Ended batch 027 / 034 | loss: 0.68677 | error: 0.39062\n",
      "  Ended batch 028 / 034 | loss: 0.73206 | error: 0.40625\n",
      "  Ended batch 029 / 034 | loss: 0.63144 | error: 0.35156\n",
      "  Ended batch 030 / 034 | loss: 0.62541 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.72507 | error: 0.40625\n",
      "  Ended batch 032 / 034 | loss: 0.81520 | error: 0.45312\n",
      "  Ended batch 033 / 034 | loss: 0.71362 | error: 0.35938\n",
      "  Ended batch 034 / 034 | loss: 0.87649 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 071 sec | loss: 0.73573 | error: 0.24991\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.69785 | error: 0.35938\n",
      "  Ended batch 002 / 034 | loss: 0.65884 | error: 0.26562\n",
      "  Ended batch 003 / 034 | loss: 0.70909 | error: 0.37500\n",
      "  Ended batch 004 / 034 | loss: 0.63471 | error: 0.35156\n",
      "  Ended batch 005 / 034 | loss: 0.62401 | error: 0.33594\n",
      "  Ended batch 006 / 034 | loss: 0.54335 | error: 0.29688\n",
      "  Ended batch 007 / 034 | loss: 0.90835 | error: 0.50781\n",
      "  Ended batch 008 / 034 | loss: 0.72743 | error: 0.40625\n",
      "  Ended batch 009 / 034 | loss: 0.74174 | error: 0.41406\n",
      "  Ended batch 010 / 034 | loss: 0.64899 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.56495 | error: 0.25781\n",
      "  Ended batch 012 / 034 | loss: 0.69209 | error: 0.39844\n",
      "  Ended batch 013 / 034 | loss: 0.68672 | error: 0.33594\n",
      "  Ended batch 014 / 034 | loss: 0.76800 | error: 0.37500\n",
      "  Ended batch 015 / 034 | loss: 0.69967 | error: 0.39062\n",
      "  Ended batch 016 / 034 | loss: 0.63804 | error: 0.33594\n",
      "  Ended batch 017 / 034 | loss: 0.68807 | error: 0.37500\n",
      "  Ended batch 018 / 034 | loss: 0.66661 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.62187 | error: 0.33594\n",
      "  Ended batch 020 / 034 | loss: 0.77056 | error: 0.40625\n",
      "  Ended batch 021 / 034 | loss: 0.75107 | error: 0.42188\n",
      "  Ended batch 022 / 034 | loss: 0.66454 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.71026 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.69507 | error: 0.38281\n",
      "  Ended batch 025 / 034 | loss: 0.73976 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.78044 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.70698 | error: 0.40625\n",
      "  Ended batch 028 / 034 | loss: 0.73733 | error: 0.41406\n",
      "  Ended batch 029 / 034 | loss: 0.61209 | error: 0.32812\n",
      "  Ended batch 030 / 034 | loss: 0.64461 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.70961 | error: 0.39062\n",
      "  Ended batch 032 / 034 | loss: 0.81889 | error: 0.45312\n",
      "  Ended batch 033 / 034 | loss: 0.74860 | error: 0.39062\n",
      "  Ended batch 034 / 034 | loss: 0.89750 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 078 sec | loss: 0.67158 | error: 0.22582\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.3_model.pt.\n",
      "Starting epoch 006 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.73182 | error: 0.39844\n",
      "  Ended batch 002 / 034 | loss: 0.67915 | error: 0.37500\n",
      "  Ended batch 003 / 034 | loss: 0.73920 | error: 0.42188\n",
      "  Ended batch 004 / 034 | loss: 0.63378 | error: 0.34375\n",
      "  Ended batch 005 / 034 | loss: 0.62949 | error: 0.33594\n",
      "  Ended batch 006 / 034 | loss: 0.56528 | error: 0.30469\n",
      "  Ended batch 007 / 034 | loss: 0.86745 | error: 0.50000\n",
      "  Ended batch 008 / 034 | loss: 0.72093 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.73546 | error: 0.41406\n",
      "  Ended batch 010 / 034 | loss: 0.66087 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.59106 | error: 0.28125\n",
      "  Ended batch 012 / 034 | loss: 0.72369 | error: 0.42188\n",
      "  Ended batch 013 / 034 | loss: 0.70204 | error: 0.36719\n",
      "  Ended batch 014 / 034 | loss: 0.77132 | error: 0.38281\n",
      "  Ended batch 015 / 034 | loss: 0.70914 | error: 0.39844\n",
      "  Ended batch 016 / 034 | loss: 0.68372 | error: 0.37500\n",
      "  Ended batch 017 / 034 | loss: 0.68863 | error: 0.38281\n",
      "  Ended batch 018 / 034 | loss: 0.68474 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.63262 | error: 0.34375\n",
      "  Ended batch 020 / 034 | loss: 0.76649 | error: 0.40625\n",
      "  Ended batch 021 / 034 | loss: 0.75896 | error: 0.42969\n",
      "  Ended batch 022 / 034 | loss: 0.66180 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.72782 | error: 0.41406\n",
      "  Ended batch 024 / 034 | loss: 0.71217 | error: 0.39844\n",
      "  Ended batch 025 / 034 | loss: 0.74495 | error: 0.41406\n",
      "  Ended batch 026 / 034 | loss: 0.77214 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.70247 | error: 0.40625\n",
      "  Ended batch 028 / 034 | loss: 0.73080 | error: 0.40625\n",
      "  Ended batch 029 / 034 | loss: 0.64795 | error: 0.35156\n",
      "  Ended batch 030 / 034 | loss: 0.64979 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.71583 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.81971 | error: 0.45312\n",
      "  Ended batch 033 / 034 | loss: 0.76211 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.88628 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 006 / 016 | time: 069 sec | loss: 0.70026 | error: 0.23884\n",
      "Starting epoch 007 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.72720 | error: 0.38281\n",
      "  Ended batch 002 / 034 | loss: 0.66182 | error: 0.29688\n",
      "  Ended batch 003 / 034 | loss: 0.72914 | error: 0.39062\n",
      "  Ended batch 004 / 034 | loss: 0.63511 | error: 0.34375\n",
      "  Ended batch 005 / 034 | loss: 0.63628 | error: 0.34375\n",
      "  Ended batch 006 / 034 | loss: 0.56079 | error: 0.30469\n",
      "  Ended batch 007 / 034 | loss: 0.87408 | error: 0.50000\n",
      "  Ended batch 008 / 034 | loss: 0.72392 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.72393 | error: 0.40625\n",
      "  Ended batch 010 / 034 | loss: 0.66222 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.56814 | error: 0.25781\n",
      "  Ended batch 012 / 034 | loss: 0.72190 | error: 0.42188\n",
      "  Ended batch 013 / 034 | loss: 0.70715 | error: 0.35938\n",
      "  Ended batch 014 / 034 | loss: 0.76498 | error: 0.37500\n",
      "  Ended batch 015 / 034 | loss: 0.70976 | error: 0.39844\n",
      "  Ended batch 016 / 034 | loss: 0.67722 | error: 0.36719\n",
      "  Ended batch 017 / 034 | loss: 0.73344 | error: 0.41406\n",
      "  Ended batch 018 / 034 | loss: 0.68499 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.63294 | error: 0.34375\n",
      "  Ended batch 020 / 034 | loss: 0.76761 | error: 0.40625\n",
      "  Ended batch 021 / 034 | loss: 0.75085 | error: 0.42188\n",
      "  Ended batch 022 / 034 | loss: 0.64162 | error: 0.32812\n",
      "  Ended batch 023 / 034 | loss: 0.72339 | error: 0.40625\n",
      "  Ended batch 024 / 034 | loss: 0.70419 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.73804 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.76344 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.70443 | error: 0.40625\n",
      "  Ended batch 028 / 034 | loss: 0.72943 | error: 0.40625\n",
      "  Ended batch 029 / 034 | loss: 0.61559 | error: 0.33594\n",
      "  Ended batch 030 / 034 | loss: 0.63996 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.72513 | error: 0.40625\n",
      "  Ended batch 032 / 034 | loss: 0.85778 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.75156 | error: 0.39062\n",
      "  Ended batch 034 / 034 | loss: 0.88364 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 007 / 016 | time: 072 sec | loss: 0.71641 | error: 0.24405\n",
      "Starting epoch 008 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.73311 | error: 0.35938\n",
      "  Ended batch 002 / 034 | loss: 0.66588 | error: 0.25781\n",
      "  Ended batch 003 / 034 | loss: 0.70136 | error: 0.32812\n",
      "  Ended batch 004 / 034 | loss: 0.60910 | error: 0.27344\n",
      "  Ended batch 005 / 034 | loss: 0.62513 | error: 0.32031\n",
      "  Ended batch 006 / 034 | loss: 0.52650 | error: 0.27344\n",
      "  Ended batch 007 / 034 | loss: 0.90306 | error: 0.50000\n",
      "  Ended batch 008 / 034 | loss: 0.75737 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.78324 | error: 0.42969\n",
      "  Ended batch 010 / 034 | loss: 0.63931 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.55684 | error: 0.28125\n",
      "  Ended batch 012 / 034 | loss: 0.72829 | error: 0.42188\n",
      "  Ended batch 013 / 034 | loss: 0.68892 | error: 0.34375\n",
      "  Ended batch 014 / 034 | loss: 0.74982 | error: 0.35156\n",
      "  Ended batch 015 / 034 | loss: 0.67544 | error: 0.36719\n",
      "  Ended batch 016 / 034 | loss: 0.65176 | error: 0.34375\n",
      "  Ended batch 017 / 034 | loss: 0.68678 | error: 0.37500\n",
      "  Ended batch 018 / 034 | loss: 0.66657 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.62271 | error: 0.33594\n",
      "  Ended batch 020 / 034 | loss: 0.77682 | error: 0.40625\n",
      "  Ended batch 021 / 034 | loss: 0.73387 | error: 0.40625\n",
      "  Ended batch 022 / 034 | loss: 0.64315 | error: 0.31250\n",
      "  Ended batch 023 / 034 | loss: 0.69431 | error: 0.38281\n",
      "  Ended batch 024 / 034 | loss: 0.70911 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.74054 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.76975 | error: 0.42969\n",
      "  Ended batch 027 / 034 | loss: 0.69030 | error: 0.39062\n",
      "  Ended batch 028 / 034 | loss: 0.72391 | error: 0.39844\n",
      "  Ended batch 029 / 034 | loss: 0.61148 | error: 0.32812\n",
      "  Ended batch 030 / 034 | loss: 0.63169 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.71536 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.84928 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.74783 | error: 0.39062\n",
      "  Ended batch 034 / 034 | loss: 0.89301 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 008 / 016 | time: 082 sec | loss: 0.71470 | error: 0.24275\n",
      "Starting epoch 009 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.73495 | error: 0.38281\n",
      "  Ended batch 002 / 034 | loss: 0.66439 | error: 0.27344\n",
      "  Ended batch 003 / 034 | loss: 0.71207 | error: 0.35156\n",
      "  Ended batch 004 / 034 | loss: 0.62516 | error: 0.25781\n",
      "  Ended batch 005 / 034 | loss: 0.62528 | error: 0.29688\n",
      "  Ended batch 006 / 034 | loss: 0.54805 | error: 0.28125\n",
      "  Ended batch 007 / 034 | loss: 0.87946 | error: 0.50000\n",
      "  Ended batch 008 / 034 | loss: 0.73710 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.76886 | error: 0.42969\n",
      "  Ended batch 010 / 034 | loss: 0.64946 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.58380 | error: 0.30469\n",
      "  Ended batch 012 / 034 | loss: 0.71421 | error: 0.41406\n",
      "  Ended batch 013 / 034 | loss: 0.70755 | error: 0.36719\n",
      "  Ended batch 014 / 034 | loss: 0.76178 | error: 0.37500\n",
      "  Ended batch 015 / 034 | loss: 0.70885 | error: 0.39844\n",
      "  Ended batch 016 / 034 | loss: 0.67528 | error: 0.36719\n",
      "  Ended batch 017 / 034 | loss: 0.66898 | error: 0.36719\n",
      "  Ended batch 018 / 034 | loss: 0.68665 | error: 0.37500\n",
      "  Ended batch 019 / 034 | loss: 0.63211 | error: 0.34375\n",
      "  Ended batch 020 / 034 | loss: 0.76987 | error: 0.40625\n",
      "  Ended batch 021 / 034 | loss: 0.77056 | error: 0.43750\n",
      "  Ended batch 022 / 034 | loss: 0.66831 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.69876 | error: 0.39062\n",
      "  Ended batch 024 / 034 | loss: 0.71207 | error: 0.39844\n",
      "  Ended batch 025 / 034 | loss: 0.74531 | error: 0.41406\n",
      "  Ended batch 026 / 034 | loss: 0.77163 | error: 0.43750\n",
      "  Ended batch 027 / 034 | loss: 0.70588 | error: 0.40625\n",
      "  Ended batch 028 / 034 | loss: 0.73015 | error: 0.40625\n",
      "  Ended batch 029 / 034 | loss: 0.63038 | error: 0.35156\n",
      "  Ended batch 030 / 034 | loss: 0.65379 | error: 0.36719\n",
      "  Ended batch 031 / 034 | loss: 0.72188 | error: 0.40625\n",
      "  Ended batch 032 / 034 | loss: 0.84571 | error: 0.47656\n",
      "  Ended batch 033 / 034 | loss: 0.71513 | error: 0.35938\n",
      "  Ended batch 034 / 034 | loss: 0.87740 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 009 / 016 | time: 073 sec | loss: 0.71890 | error: 0.24470\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 11.128755315144856 minutes (667.7253189086914 seconds).\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.3_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.3_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "---------- Calculating results ----------\n",
      "Evaluating using epsilon: 0.3\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0903, -0.2800, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "         -0.0746,  0.0482, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919,  0.1656,\n",
      "          0.3650,  0.3795, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "          0.0652, -0.1662, -0.0495,  0.3771, -0.0304, -0.0473, -0.0763]])\n",
      "Evaluating using epsilon: 0.3\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0939, -0.2610, -0.0831, -0.0917, -0.0427, -0.0736, -0.0944, -0.1189,\n",
      "         -0.0691,  0.0427, -0.0939, -0.1924, -0.0762, -0.0533, -0.0803],\n",
      "        [-0.0810,  0.0231, -0.0904, -0.0993, -0.0421, -0.0787, -0.0991,  0.1817,\n",
      "          0.3804,  0.3640, -0.1058, -0.1961, -0.0789, -0.0551, -0.0875],\n",
      "        [-0.0754, -0.0069, -0.0841, -0.0927, -0.0427, -0.0743, -0.0951, -0.1174,\n",
      "          0.0720, -0.1731, -0.0577,  0.3745, -0.0323, -0.0486, -0.0813]])\n",
      "Evaluating using epsilon: 0.3\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0946, -0.2504, -0.0852, -0.0931, -0.0423, -0.0762, -0.0946, -0.1158,\n",
      "         -0.0674,  0.0401, -0.0970, -0.1912, -0.0739, -0.0509, -0.0824],\n",
      "        [-0.2021,  0.8723, -0.2670, -0.2560, -0.0192, -0.2354, -0.1741,  0.4340,\n",
      "          0.5838,  0.1340, -0.3814, -0.2183, -0.0496, -0.0095, -0.2604],\n",
      "        [-0.0818,  0.0439, -0.0946, -0.1015, -0.0411, -0.0844, -0.0987, -0.1024,\n",
      "          0.0833, -0.1865, -0.0738,  0.3748, -0.0284, -0.0438, -0.0915]])\n",
      "Evaluating using epsilon: 0.3\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1120,  0.2070, -0.2582, -0.2570, -0.2569, -0.1961, -0.1348, -0.1936,\n",
      "         -0.1283, -0.0399, -0.3641, -0.1933, -0.0645,  0.0970, -0.1465],\n",
      "        [-0.0896,  0.3877, -0.2330, -0.2331, -0.2267, -0.1785, -0.1288,  0.1063,\n",
      "          0.3188,  0.3038, -0.3251, -0.1929, -0.0660,  0.0759, -0.1366],\n",
      "        [-0.0887,  0.3672, -0.2254, -0.2260, -0.2177, -0.1732, -0.1270, -0.1810,\n",
      "          0.0213, -0.2381, -0.2756,  0.3746, -0.0221,  0.0746, -0.1336]])\n",
      "------------------------- 0.4 -------------------------\n",
      "---------- Training non-strategically with epsilon=0.4 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98923 | error: 0.43750\n",
      "  Ended batch 002 / 034 | loss: 0.83193 | error: 0.39844\n",
      "  Ended batch 003 / 034 | loss: 0.75601 | error: 0.38281\n",
      "  Ended batch 004 / 034 | loss: 0.62267 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.53573 | error: 0.26562\n",
      "  Ended batch 006 / 034 | loss: 0.43920 | error: 0.17188\n",
      "  Ended batch 007 / 034 | loss: 0.55319 | error: 0.24219\n",
      "  Ended batch 008 / 034 | loss: 0.43821 | error: 0.17188\n",
      "  Ended batch 009 / 034 | loss: 0.48477 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.44141 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.34122 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.39537 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.46204 | error: 0.17969\n",
      "  Ended batch 014 / 034 | loss: 0.61706 | error: 0.21094\n",
      "  Ended batch 015 / 034 | loss: 0.40735 | error: 0.15625\n",
      "  Ended batch 016 / 034 | loss: 0.44818 | error: 0.19531\n",
      "  Ended batch 017 / 034 | loss: 0.42789 | error: 0.17969\n",
      "  Ended batch 018 / 034 | loss: 0.45081 | error: 0.14062\n",
      "  Ended batch 019 / 034 | loss: 0.40077 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.67522 | error: 0.20312\n",
      "  Ended batch 021 / 034 | loss: 0.44442 | error: 0.17188\n",
      "  Ended batch 022 / 034 | loss: 0.49886 | error: 0.20312\n",
      "  Ended batch 023 / 034 | loss: 0.43399 | error: 0.16406\n",
      "  Ended batch 024 / 034 | loss: 0.42774 | error: 0.17188\n",
      "  Ended batch 025 / 034 | loss: 0.49798 | error: 0.17969\n",
      "  Ended batch 026 / 034 | loss: 0.48518 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34755 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.36209 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30258 | error: 0.14062\n",
      "  Ended batch 030 / 034 | loss: 0.27834 | error: 0.09375\n",
      "  Ended batch 031 / 034 | loss: 0.31573 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.46953 | error: 0.21094\n",
      "  Ended batch 033 / 034 | loss: 0.44510 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.35380 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 000 sec | loss: 0.45466 | error: 0.21103\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/non_strategic_0.4_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.40293 | error: 0.16406\n",
      "  Ended batch 002 / 034 | loss: 0.39698 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39748 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.33817 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31394 | error: 0.13281\n",
      "  Ended batch 006 / 034 | loss: 0.26311 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.40746 | error: 0.21094\n",
      "  Ended batch 008 / 034 | loss: 0.28619 | error: 0.12500\n",
      "  Ended batch 009 / 034 | loss: 0.42815 | error: 0.21875\n",
      "  Ended batch 010 / 034 | loss: 0.37308 | error: 0.14062\n",
      "  Ended batch 011 / 034 | loss: 0.24522 | error: 0.10938\n",
      "  Ended batch 012 / 034 | loss: 0.35200 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.38305 | error: 0.16406\n",
      "  Ended batch 014 / 034 | loss: 0.48793 | error: 0.18750\n",
      "  Ended batch 015 / 034 | loss: 0.29911 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37009 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39094 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.39047 | error: 0.16406\n",
      "  Ended batch 019 / 034 | loss: 0.33128 | error: 0.14844\n",
      "  Ended batch 020 / 034 | loss: 0.62716 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.39453 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.42599 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.40997 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38743 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.49943 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.46154 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34475 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.35799 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30337 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29895 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31889 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44511 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40606 | error: 0.16406\n",
      "  Ended batch 034 / 034 | loss: 0.41604 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 000 sec | loss: 0.44767 | error: 0.21363\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37960 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.38054 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39798 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.32809 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31529 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26527 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.39605 | error: 0.20312\n",
      "  Ended batch 008 / 034 | loss: 0.29298 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42259 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.37166 | error: 0.12500\n",
      "  Ended batch 011 / 034 | loss: 0.24852 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.34251 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37322 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47350 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31297 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.36932 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39082 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38728 | error: 0.15625\n",
      "  Ended batch 019 / 034 | loss: 0.33095 | error: 0.13281\n",
      "  Ended batch 020 / 034 | loss: 0.63589 | error: 0.24219\n",
      "  Ended batch 021 / 034 | loss: 0.39093 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41775 | error: 0.18750\n",
      "  Ended batch 023 / 034 | loss: 0.40417 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38116 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48604 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.45087 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34605 | error: 0.13281\n",
      "  Ended batch 028 / 034 | loss: 0.36658 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30254 | error: 0.10938\n",
      "  Ended batch 030 / 034 | loss: 0.30504 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31964 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44560 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40693 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.43542 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 000 sec | loss: 0.44874 | error: 0.21168\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37193 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37114 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39974 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31597 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31782 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26514 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.39098 | error: 0.18750\n",
      "  Ended batch 008 / 034 | loss: 0.28986 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42376 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.36756 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24499 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34379 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37043 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47858 | error: 0.20312\n",
      "  Ended batch 015 / 034 | loss: 0.30862 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37446 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38862 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38593 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32783 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64187 | error: 0.25000\n",
      "  Ended batch 021 / 034 | loss: 0.38988 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41519 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39861 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37473 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48543 | error: 0.21094\n",
      "  Ended batch 026 / 034 | loss: 0.44854 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34043 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36239 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.29949 | error: 0.11719\n",
      "  Ended batch 030 / 034 | loss: 0.29681 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31391 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45120 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40857 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42124 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 000 sec | loss: 0.44502 | error: 0.21103\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37063 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37067 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39355 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31063 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31461 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26898 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.38740 | error: 0.19531\n",
      "  Ended batch 008 / 034 | loss: 0.28760 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42075 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.36434 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24144 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34192 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.36529 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47503 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31060 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37830 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38680 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38255 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32599 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64071 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.38804 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41321 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39709 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37160 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48514 | error: 0.20312\n",
      "  Ended batch 026 / 034 | loss: 0.44696 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34037 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36098 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30023 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29524 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31327 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45363 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40813 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42413 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 000 sec | loss: 0.44367 | error: 0.21168\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 0.6800084114074707 seconds.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.4_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.4_model.pt.\n",
      "---------- Training strategically with epsilon=0.4 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.97605 | error: 0.45312\n",
      "  Ended batch 002 / 034 | loss: 0.90325 | error: 0.48438\n",
      "  Ended batch 003 / 034 | loss: 0.92566 | error: 0.52344\n",
      "  Ended batch 004 / 034 | loss: 0.79341 | error: 0.43750\n",
      "  Ended batch 005 / 034 | loss: 0.74430 | error: 0.39844\n",
      "  Ended batch 006 / 034 | loss: 0.69103 | error: 0.39062\n",
      "  Ended batch 007 / 034 | loss: 0.97351 | error: 0.55469\n",
      "  Ended batch 008 / 034 | loss: 0.75431 | error: 0.42188\n",
      "  Ended batch 009 / 034 | loss: 0.80488 | error: 0.44531\n",
      "  Ended batch 010 / 034 | loss: 0.70693 | error: 0.32812\n",
      "  Ended batch 011 / 034 | loss: 0.64840 | error: 0.25000\n",
      "  Ended batch 012 / 034 | loss: 0.80521 | error: 0.44531\n",
      "  Ended batch 013 / 034 | loss: 0.70457 | error: 0.28125\n",
      "  Ended batch 014 / 034 | loss: 0.74752 | error: 0.26562\n",
      "  Ended batch 015 / 034 | loss: 0.68374 | error: 0.25000\n",
      "  Ended batch 016 / 034 | loss: 0.62906 | error: 0.28125\n",
      "  Ended batch 017 / 034 | loss: 0.65696 | error: 0.35938\n",
      "  Ended batch 018 / 034 | loss: 0.68916 | error: 0.36719\n",
      "  Ended batch 019 / 034 | loss: 0.60557 | error: 0.30469\n",
      "  Ended batch 020 / 034 | loss: 0.77351 | error: 0.39062\n",
      "  Ended batch 021 / 034 | loss: 0.71459 | error: 0.35938\n",
      "  Ended batch 022 / 034 | loss: 0.64259 | error: 0.29688\n",
      "  Ended batch 023 / 034 | loss: 0.59381 | error: 0.30469\n",
      "  Ended batch 024 / 034 | loss: 0.68891 | error: 0.34375\n",
      "  Ended batch 025 / 034 | loss: 0.66037 | error: 0.33594\n",
      "  Ended batch 026 / 034 | loss: 0.66155 | error: 0.32812\n",
      "  Ended batch 027 / 034 | loss: 0.54912 | error: 0.28125\n",
      "  Ended batch 028 / 034 | loss: 0.65799 | error: 0.34375\n",
      "  Ended batch 029 / 034 | loss: 0.58514 | error: 0.31250\n",
      "  Ended batch 030 / 034 | loss: 0.54112 | error: 0.28125\n",
      "  Ended batch 031 / 034 | loss: 0.61083 | error: 0.32031\n",
      "  Ended batch 032 / 034 | loss: 0.70136 | error: 0.37500\n",
      "  Ended batch 033 / 034 | loss: 0.63782 | error: 0.32031\n",
      "  Ended batch 034 / 034 | loss: 0.75898 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 074 sec | loss: 1.01547 | error: 0.38012\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.4_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.68704 | error: 0.35156\n",
      "  Ended batch 002 / 034 | loss: 0.51996 | error: 0.24219\n",
      "  Ended batch 003 / 034 | loss: 0.67107 | error: 0.35938\n",
      "  Ended batch 004 / 034 | loss: 0.66138 | error: 0.35156\n",
      "  Ended batch 005 / 034 | loss: 0.62106 | error: 0.33594\n",
      "  Ended batch 006 / 034 | loss: 0.54361 | error: 0.29688\n",
      "  Ended batch 007 / 034 | loss: 0.74828 | error: 0.41406\n",
      "  Ended batch 008 / 034 | loss: 0.53183 | error: 0.28125\n",
      "  Ended batch 009 / 034 | loss: 0.62733 | error: 0.32812\n",
      "  Ended batch 010 / 034 | loss: 0.61786 | error: 0.30469\n",
      "  Ended batch 011 / 034 | loss: 0.48519 | error: 0.25000\n",
      "  Ended batch 012 / 034 | loss: 0.73274 | error: 0.39844\n",
      "  Ended batch 013 / 034 | loss: 0.68187 | error: 0.34375\n",
      "  Ended batch 014 / 034 | loss: 0.69570 | error: 0.33594\n",
      "  Ended batch 015 / 034 | loss: 0.66797 | error: 0.35938\n",
      "  Ended batch 016 / 034 | loss: 0.57555 | error: 0.29688\n",
      "  Ended batch 017 / 034 | loss: 0.62952 | error: 0.33594\n",
      "  Ended batch 018 / 034 | loss: 0.63910 | error: 0.33594\n",
      "  Ended batch 019 / 034 | loss: 0.56694 | error: 0.28906\n",
      "  Ended batch 020 / 034 | loss: 0.70495 | error: 0.35938\n",
      "  Ended batch 021 / 034 | loss: 0.72536 | error: 0.39062\n",
      "  Ended batch 022 / 034 | loss: 0.65293 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.65185 | error: 0.35156\n",
      "  Ended batch 024 / 034 | loss: 0.69277 | error: 0.38281\n",
      "  Ended batch 025 / 034 | loss: 0.72451 | error: 0.39844\n",
      "  Ended batch 026 / 034 | loss: 0.75190 | error: 0.41406\n",
      "  Ended batch 027 / 034 | loss: 0.63413 | error: 0.34375\n",
      "  Ended batch 028 / 034 | loss: 0.67419 | error: 0.35938\n",
      "  Ended batch 029 / 034 | loss: 0.60587 | error: 0.32812\n",
      "  Ended batch 030 / 034 | loss: 0.61576 | error: 0.33594\n",
      "  Ended batch 031 / 034 | loss: 0.69300 | error: 0.39062\n",
      "  Ended batch 032 / 034 | loss: 0.80448 | error: 0.44531\n",
      "  Ended batch 033 / 034 | loss: 0.76089 | error: 0.40625\n",
      "  Ended batch 034 / 034 | loss: 0.86800 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 077 sec | loss: 0.69075 | error: 0.23363\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.4_model.pt.\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.69466 | error: 0.36719\n",
      "  Ended batch 002 / 034 | loss: 0.62284 | error: 0.23438\n",
      "  Ended batch 003 / 034 | loss: 0.70253 | error: 0.33594\n",
      "  Ended batch 004 / 034 | loss: 0.61660 | error: 0.28906\n",
      "  Ended batch 005 / 034 | loss: 0.63011 | error: 0.29688\n",
      "  Ended batch 006 / 034 | loss: 0.48782 | error: 0.23438\n",
      "  Ended batch 007 / 034 | loss: 0.83908 | error: 0.48438\n",
      "  Ended batch 008 / 034 | loss: 0.69115 | error: 0.39062\n",
      "  Ended batch 009 / 034 | loss: 0.73002 | error: 0.40625\n",
      "  Ended batch 010 / 034 | loss: 0.62631 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.55147 | error: 0.29688\n",
      "  Ended batch 012 / 034 | loss: 0.73436 | error: 0.43750\n",
      "  Ended batch 013 / 034 | loss: 0.66350 | error: 0.34375\n",
      "  Ended batch 014 / 034 | loss: 0.72134 | error: 0.35156\n",
      "  Ended batch 015 / 034 | loss: 0.68766 | error: 0.39062\n",
      "  Ended batch 016 / 034 | loss: 0.63587 | error: 0.34375\n",
      "  Ended batch 017 / 034 | loss: 0.66148 | error: 0.36719\n",
      "  Ended batch 018 / 034 | loss: 0.65480 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.60365 | error: 0.32812\n",
      "  Ended batch 020 / 034 | loss: 0.78051 | error: 0.41406\n",
      "  Ended batch 021 / 034 | loss: 0.72256 | error: 0.40625\n",
      "  Ended batch 022 / 034 | loss: 0.66942 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.65640 | error: 0.35938\n",
      "  Ended batch 024 / 034 | loss: 0.68717 | error: 0.38281\n",
      "  Ended batch 025 / 034 | loss: 0.72497 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.75146 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.67853 | error: 0.39062\n",
      "  Ended batch 028 / 034 | loss: 0.71535 | error: 0.40625\n",
      "  Ended batch 029 / 034 | loss: 0.60247 | error: 0.33594\n",
      "  Ended batch 030 / 034 | loss: 0.60350 | error: 0.33594\n",
      "  Ended batch 031 / 034 | loss: 0.69159 | error: 0.39062\n",
      "  Ended batch 032 / 034 | loss: 0.79498 | error: 0.44531\n",
      "  Ended batch 033 / 034 | loss: 0.71535 | error: 0.36719\n",
      "  Ended batch 034 / 034 | loss: 0.85900 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 072 sec | loss: 0.69430 | error: 0.23558\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.70028 | error: 0.37500\n",
      "  Ended batch 002 / 034 | loss: 0.65135 | error: 0.25781\n",
      "  Ended batch 003 / 034 | loss: 0.69576 | error: 0.32812\n",
      "  Ended batch 004 / 034 | loss: 0.61570 | error: 0.30469\n",
      "  Ended batch 005 / 034 | loss: 0.61854 | error: 0.30469\n",
      "  Ended batch 006 / 034 | loss: 0.48493 | error: 0.23438\n",
      "  Ended batch 007 / 034 | loss: 0.80690 | error: 0.46094\n",
      "  Ended batch 008 / 034 | loss: 0.72024 | error: 0.40625\n",
      "  Ended batch 009 / 034 | loss: 0.74884 | error: 0.41406\n",
      "  Ended batch 010 / 034 | loss: 0.63791 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.53522 | error: 0.28125\n",
      "  Ended batch 012 / 034 | loss: 0.70908 | error: 0.41406\n",
      "  Ended batch 013 / 034 | loss: 0.67487 | error: 0.34375\n",
      "  Ended batch 014 / 034 | loss: 0.72223 | error: 0.33594\n",
      "  Ended batch 015 / 034 | loss: 0.66540 | error: 0.36719\n",
      "  Ended batch 016 / 034 | loss: 0.63202 | error: 0.33594\n",
      "  Ended batch 017 / 034 | loss: 0.67834 | error: 0.37500\n",
      "  Ended batch 018 / 034 | loss: 0.65632 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.59342 | error: 0.32031\n",
      "  Ended batch 020 / 034 | loss: 0.74644 | error: 0.39062\n",
      "  Ended batch 021 / 034 | loss: 0.71746 | error: 0.39844\n",
      "  Ended batch 022 / 034 | loss: 0.65816 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.65374 | error: 0.35938\n",
      "  Ended batch 024 / 034 | loss: 0.68475 | error: 0.38281\n",
      "  Ended batch 025 / 034 | loss: 0.73077 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.73881 | error: 0.41406\n",
      "  Ended batch 027 / 034 | loss: 0.66795 | error: 0.38281\n",
      "  Ended batch 028 / 034 | loss: 0.71782 | error: 0.40625\n",
      "  Ended batch 029 / 034 | loss: 0.60013 | error: 0.32812\n",
      "  Ended batch 030 / 034 | loss: 0.61853 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.70019 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.80765 | error: 0.46094\n",
      "  Ended batch 033 / 034 | loss: 0.74924 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.86808 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 069 sec | loss: 0.68286 | error: 0.23233\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.4_model.pt.\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.70618 | error: 0.37500\n",
      "  Ended batch 002 / 034 | loss: 0.64205 | error: 0.27344\n",
      "  Ended batch 003 / 034 | loss: 0.69569 | error: 0.33594\n",
      "  Ended batch 004 / 034 | loss: 0.59203 | error: 0.29688\n",
      "  Ended batch 005 / 034 | loss: 0.62221 | error: 0.32031\n",
      "  Ended batch 006 / 034 | loss: 0.51253 | error: 0.26562\n",
      "  Ended batch 007 / 034 | loss: 0.80592 | error: 0.46094\n",
      "  Ended batch 008 / 034 | loss: 0.70288 | error: 0.39844\n",
      "  Ended batch 009 / 034 | loss: 0.73068 | error: 0.40625\n",
      "  Ended batch 010 / 034 | loss: 0.63848 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.51723 | error: 0.26562\n",
      "  Ended batch 012 / 034 | loss: 0.74141 | error: 0.43750\n",
      "  Ended batch 013 / 034 | loss: 0.68872 | error: 0.36719\n",
      "  Ended batch 014 / 034 | loss: 0.73389 | error: 0.36719\n",
      "  Ended batch 015 / 034 | loss: 0.67922 | error: 0.38281\n",
      "  Ended batch 016 / 034 | loss: 0.63688 | error: 0.34375\n",
      "  Ended batch 017 / 034 | loss: 0.67302 | error: 0.37500\n",
      "  Ended batch 018 / 034 | loss: 0.65463 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.60345 | error: 0.32812\n",
      "  Ended batch 020 / 034 | loss: 0.77396 | error: 0.41406\n",
      "  Ended batch 021 / 034 | loss: 0.72018 | error: 0.40625\n",
      "  Ended batch 022 / 034 | loss: 0.66206 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.68900 | error: 0.39062\n",
      "  Ended batch 024 / 034 | loss: 0.68497 | error: 0.38281\n",
      "  Ended batch 025 / 034 | loss: 0.72608 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.75357 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.66610 | error: 0.38281\n",
      "  Ended batch 028 / 034 | loss: 0.71645 | error: 0.40625\n",
      "  Ended batch 029 / 034 | loss: 0.62020 | error: 0.35156\n",
      "  Ended batch 030 / 034 | loss: 0.60898 | error: 0.33594\n",
      "  Ended batch 031 / 034 | loss: 0.69056 | error: 0.39062\n",
      "  Ended batch 032 / 034 | loss: 0.80329 | error: 0.45312\n",
      "  Ended batch 033 / 034 | loss: 0.71466 | error: 0.36719\n",
      "  Ended batch 034 / 034 | loss: 0.86701 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 075 sec | loss: 0.66384 | error: 0.22256\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.4_model.pt.\n",
      "Starting epoch 006 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.70602 | error: 0.39062\n",
      "  Ended batch 002 / 034 | loss: 0.65102 | error: 0.33594\n",
      "  Ended batch 003 / 034 | loss: 0.71518 | error: 0.39844\n",
      "  Ended batch 004 / 034 | loss: 0.63488 | error: 0.34375\n",
      "  Ended batch 005 / 034 | loss: 0.59978 | error: 0.32812\n",
      "  Ended batch 006 / 034 | loss: 0.53496 | error: 0.28906\n",
      "  Ended batch 007 / 034 | loss: 0.81264 | error: 0.46875\n",
      "  Ended batch 008 / 034 | loss: 0.71213 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.72898 | error: 0.41406\n",
      "  Ended batch 010 / 034 | loss: 0.64977 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.53561 | error: 0.26562\n",
      "  Ended batch 012 / 034 | loss: 0.73722 | error: 0.44531\n",
      "  Ended batch 013 / 034 | loss: 0.69149 | error: 0.35938\n",
      "  Ended batch 014 / 034 | loss: 0.73977 | error: 0.35938\n",
      "  Ended batch 015 / 034 | loss: 0.68585 | error: 0.39062\n",
      "  Ended batch 016 / 034 | loss: 0.63985 | error: 0.34375\n",
      "  Ended batch 017 / 034 | loss: 0.66739 | error: 0.37500\n",
      "  Ended batch 018 / 034 | loss: 0.65549 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.60383 | error: 0.32812\n",
      "  Ended batch 020 / 034 | loss: 0.76821 | error: 0.40625\n",
      "  Ended batch 021 / 034 | loss: 0.73104 | error: 0.41406\n",
      "  Ended batch 022 / 034 | loss: 0.66276 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.65693 | error: 0.35938\n",
      "  Ended batch 024 / 034 | loss: 0.68574 | error: 0.38281\n",
      "  Ended batch 025 / 034 | loss: 0.72465 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.74904 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.66469 | error: 0.38281\n",
      "  Ended batch 028 / 034 | loss: 0.71677 | error: 0.40625\n",
      "  Ended batch 029 / 034 | loss: 0.61061 | error: 0.34375\n",
      "  Ended batch 030 / 034 | loss: 0.60269 | error: 0.33594\n",
      "  Ended batch 031 / 034 | loss: 0.70808 | error: 0.40625\n",
      "  Ended batch 032 / 034 | loss: 0.79703 | error: 0.45312\n",
      "  Ended batch 033 / 034 | loss: 0.71830 | error: 0.36719\n",
      "  Ended batch 034 / 034 | loss: 0.84737 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 006 / 016 | time: 085 sec | loss: 0.71132 | error: 0.24209\n",
      "Starting epoch 007 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.70357 | error: 0.35156\n",
      "  Ended batch 002 / 034 | loss: 0.61618 | error: 0.21875\n",
      "  Ended batch 003 / 034 | loss: 0.69945 | error: 0.32031\n",
      "  Ended batch 004 / 034 | loss: 0.63929 | error: 0.35156\n",
      "  Ended batch 005 / 034 | loss: 0.61970 | error: 0.34375\n",
      "  Ended batch 006 / 034 | loss: 0.52926 | error: 0.28906\n",
      "  Ended batch 007 / 034 | loss: 0.85956 | error: 0.48438\n",
      "  Ended batch 008 / 034 | loss: 0.71637 | error: 0.40625\n",
      "  Ended batch 009 / 034 | loss: 0.71493 | error: 0.39844\n",
      "  Ended batch 010 / 034 | loss: 0.65095 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.53254 | error: 0.26562\n",
      "  Ended batch 012 / 034 | loss: 0.72902 | error: 0.42969\n",
      "  Ended batch 013 / 034 | loss: 0.67406 | error: 0.33594\n",
      "  Ended batch 014 / 034 | loss: 0.72680 | error: 0.35156\n",
      "  Ended batch 015 / 034 | loss: 0.69024 | error: 0.39062\n",
      "  Ended batch 016 / 034 | loss: 0.65640 | error: 0.35938\n",
      "  Ended batch 017 / 034 | loss: 0.66981 | error: 0.36719\n",
      "  Ended batch 018 / 034 | loss: 0.65522 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.58345 | error: 0.31250\n",
      "  Ended batch 020 / 034 | loss: 0.77037 | error: 0.41406\n",
      "  Ended batch 021 / 034 | loss: 0.70960 | error: 0.39844\n",
      "  Ended batch 022 / 034 | loss: 0.66273 | error: 0.35156\n",
      "  Ended batch 023 / 034 | loss: 0.69214 | error: 0.39062\n",
      "  Ended batch 024 / 034 | loss: 0.69420 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.72701 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.75288 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.67415 | error: 0.39062\n",
      "  Ended batch 028 / 034 | loss: 0.72307 | error: 0.41406\n",
      "  Ended batch 029 / 034 | loss: 0.59846 | error: 0.32812\n",
      "  Ended batch 030 / 034 | loss: 0.61233 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.70189 | error: 0.39844\n",
      "  Ended batch 032 / 034 | loss: 0.81491 | error: 0.46094\n",
      "  Ended batch 033 / 034 | loss: 0.71371 | error: 0.36719\n",
      "  Ended batch 034 / 034 | loss: 0.86626 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 007 / 016 | time: 079 sec | loss: 0.68177 | error: 0.23103\n",
      "Starting epoch 008 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.70853 | error: 0.38281\n",
      "  Ended batch 002 / 034 | loss: 0.65584 | error: 0.27344\n",
      "  Ended batch 003 / 034 | loss: 0.69380 | error: 0.32812\n",
      "  Ended batch 004 / 034 | loss: 0.60502 | error: 0.30469\n",
      "  Ended batch 005 / 034 | loss: 0.61994 | error: 0.32031\n",
      "  Ended batch 006 / 034 | loss: 0.51450 | error: 0.26562\n",
      "  Ended batch 007 / 034 | loss: 0.83882 | error: 0.47656\n",
      "  Ended batch 008 / 034 | loss: 0.73336 | error: 0.41406\n",
      "  Ended batch 009 / 034 | loss: 0.78022 | error: 0.43750\n",
      "  Ended batch 010 / 034 | loss: 0.64127 | error: 0.35156\n",
      "  Ended batch 011 / 034 | loss: 0.55557 | error: 0.29688\n",
      "  Ended batch 012 / 034 | loss: 0.74271 | error: 0.44531\n",
      "  Ended batch 013 / 034 | loss: 0.66796 | error: 0.34375\n",
      "  Ended batch 014 / 034 | loss: 0.72782 | error: 0.35156\n",
      "  Ended batch 015 / 034 | loss: 0.68959 | error: 0.39062\n",
      "  Ended batch 016 / 034 | loss: 0.63038 | error: 0.33594\n",
      "  Ended batch 017 / 034 | loss: 0.68642 | error: 0.38281\n",
      "  Ended batch 018 / 034 | loss: 0.65593 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.63261 | error: 0.35156\n",
      "  Ended batch 020 / 034 | loss: 0.78508 | error: 0.42188\n",
      "  Ended batch 021 / 034 | loss: 0.72753 | error: 0.41406\n",
      "  Ended batch 022 / 034 | loss: 0.67495 | error: 0.34375\n",
      "  Ended batch 023 / 034 | loss: 0.66474 | error: 0.34375\n",
      "  Ended batch 024 / 034 | loss: 0.68422 | error: 0.38281\n",
      "  Ended batch 025 / 034 | loss: 0.72600 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.77477 | error: 0.44531\n",
      "  Ended batch 027 / 034 | loss: 0.69609 | error: 0.41406\n",
      "  Ended batch 028 / 034 | loss: 0.72067 | error: 0.40625\n",
      "  Ended batch 029 / 034 | loss: 0.62040 | error: 0.35156\n",
      "  Ended batch 030 / 034 | loss: 0.61319 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.71222 | error: 0.40625\n",
      "  Ended batch 032 / 034 | loss: 0.82692 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.71925 | error: 0.36719\n",
      "  Ended batch 034 / 034 | loss: 0.85681 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 008 / 016 | time: 069 sec | loss: 0.69377 | error: 0.23624\n",
      "Starting epoch 009 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.68456 | error: 0.36719\n",
      "  Ended batch 002 / 034 | loss: 0.63009 | error: 0.25000\n",
      "  Ended batch 003 / 034 | loss: 0.70173 | error: 0.35156\n",
      "  Ended batch 004 / 034 | loss: 0.63429 | error: 0.34375\n",
      "  Ended batch 005 / 034 | loss: 0.59929 | error: 0.32812\n",
      "  Ended batch 006 / 034 | loss: 0.53124 | error: 0.28906\n",
      "  Ended batch 007 / 034 | loss: 0.82385 | error: 0.46875\n",
      "  Ended batch 008 / 034 | loss: 0.71234 | error: 0.40625\n",
      "  Ended batch 009 / 034 | loss: 0.71473 | error: 0.39844\n",
      "  Ended batch 010 / 034 | loss: 0.64255 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.53183 | error: 0.26562\n",
      "  Ended batch 012 / 034 | loss: 0.70163 | error: 0.40625\n",
      "  Ended batch 013 / 034 | loss: 0.67257 | error: 0.34375\n",
      "  Ended batch 014 / 034 | loss: 0.75142 | error: 0.37500\n",
      "  Ended batch 015 / 034 | loss: 0.68195 | error: 0.38281\n",
      "  Ended batch 016 / 034 | loss: 0.66091 | error: 0.35938\n",
      "  Ended batch 017 / 034 | loss: 0.68217 | error: 0.37500\n",
      "  Ended batch 018 / 034 | loss: 0.65419 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.60256 | error: 0.32031\n",
      "  Ended batch 020 / 034 | loss: 0.76392 | error: 0.41406\n",
      "  Ended batch 021 / 034 | loss: 0.71557 | error: 0.40625\n",
      "  Ended batch 022 / 034 | loss: 0.66921 | error: 0.35938\n",
      "  Ended batch 023 / 034 | loss: 0.70290 | error: 0.39844\n",
      "  Ended batch 024 / 034 | loss: 0.69337 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.72789 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.75289 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.66515 | error: 0.38281\n",
      "  Ended batch 028 / 034 | loss: 0.71679 | error: 0.40625\n",
      "  Ended batch 029 / 034 | loss: 0.61206 | error: 0.34375\n",
      "  Ended batch 030 / 034 | loss: 0.61222 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.71409 | error: 0.40625\n",
      "  Ended batch 032 / 034 | loss: 0.82316 | error: 0.46875\n",
      "  Ended batch 033 / 034 | loss: 0.72627 | error: 0.37500\n",
      "  Ended batch 034 / 034 | loss: 0.85361 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 009 / 016 | time: 077 sec | loss: 0.71035 | error: 0.24470\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 11.271115954717 minutes (676.26695728302 seconds).\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.4_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.4_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "---------- Calculating results ----------\n",
      "Evaluating using epsilon: 0.4\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0903, -0.2800, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "         -0.0746,  0.0482, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919,  0.1656,\n",
      "          0.3650,  0.3795, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "          0.0652, -0.1662, -0.0495,  0.3771, -0.0304, -0.0473, -0.0763]])\n",
      "Evaluating using epsilon: 0.4\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0930, -0.2658, -0.0820, -0.0907, -0.0428, -0.0729, -0.0938, -0.1203,\n",
      "         -0.0705,  0.0441, -0.0922, -0.1918, -0.0758, -0.0531, -0.0793],\n",
      "        [-0.0782,  0.0082, -0.0872, -0.0960, -0.0424, -0.0765, -0.0971,  0.1773,\n",
      "          0.3761,  0.3683, -0.1007, -0.1945, -0.0778, -0.0544, -0.0844],\n",
      "        [-0.0743, -0.0130, -0.0828, -0.0914, -0.0428, -0.0734, -0.0942, -0.1193,\n",
      "          0.0703, -0.1713, -0.0556,  0.3752, -0.0318, -0.0482, -0.0800]])\n",
      "Evaluating using epsilon: 0.4\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0951, -0.2562, -0.0835, -0.0919, -0.0433, -0.0750, -0.0943, -0.1174,\n",
      "         -0.0688,  0.0418, -0.0959, -0.1918, -0.0756, -0.0523, -0.0795],\n",
      "        [-0.2310,  0.7647, -0.2258, -0.2338, -0.0503, -0.2117, -0.1744,  0.4055,\n",
      "          0.5599,  0.1671, -0.3725, -0.2392, -0.1029, -0.0506, -0.1830],\n",
      "        [-0.0831,  0.0293, -0.0901, -0.0986, -0.0437, -0.0814, -0.0981, -0.1065,\n",
      "          0.0799, -0.1822, -0.0710,  0.3734, -0.0326, -0.0472, -0.0844]])\n",
      "Evaluating using epsilon: 0.4\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1106,  0.1756, -0.2467, -0.2461, -0.2431, -0.1881, -0.1321, -0.1892,\n",
      "         -0.1249, -0.0342, -0.3463, -0.1931, -0.0652,  0.0873, -0.1419],\n",
      "        [-0.0882,  0.3569, -0.2216, -0.2224, -0.2132, -0.1705, -0.1261,  0.1107,\n",
      "          0.3222,  0.3094, -0.3076, -0.1927, -0.0666,  0.0665, -0.1321],\n",
      "        [-0.0873,  0.3366, -0.2142, -0.2154, -0.2043, -0.1653, -0.1243, -0.1766,\n",
      "          0.0247, -0.2326, -0.2582,  0.3748, -0.0227,  0.0653, -0.1292]])\n",
      "------------------------- 0.6 -------------------------\n",
      "---------- Training non-strategically with epsilon=0.6 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98923 | error: 0.43750\n",
      "  Ended batch 002 / 034 | loss: 0.83193 | error: 0.39844\n",
      "  Ended batch 003 / 034 | loss: 0.75601 | error: 0.38281\n",
      "  Ended batch 004 / 034 | loss: 0.62267 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.53573 | error: 0.26562\n",
      "  Ended batch 006 / 034 | loss: 0.43920 | error: 0.17188\n",
      "  Ended batch 007 / 034 | loss: 0.55319 | error: 0.24219\n",
      "  Ended batch 008 / 034 | loss: 0.43821 | error: 0.17188\n",
      "  Ended batch 009 / 034 | loss: 0.48477 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.44141 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.34122 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.39537 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.46204 | error: 0.17969\n",
      "  Ended batch 014 / 034 | loss: 0.61706 | error: 0.21094\n",
      "  Ended batch 015 / 034 | loss: 0.40735 | error: 0.15625\n",
      "  Ended batch 016 / 034 | loss: 0.44818 | error: 0.19531\n",
      "  Ended batch 017 / 034 | loss: 0.42789 | error: 0.17969\n",
      "  Ended batch 018 / 034 | loss: 0.45081 | error: 0.14062\n",
      "  Ended batch 019 / 034 | loss: 0.40077 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.67522 | error: 0.20312\n",
      "  Ended batch 021 / 034 | loss: 0.44442 | error: 0.17188\n",
      "  Ended batch 022 / 034 | loss: 0.49886 | error: 0.20312\n",
      "  Ended batch 023 / 034 | loss: 0.43399 | error: 0.16406\n",
      "  Ended batch 024 / 034 | loss: 0.42774 | error: 0.17188\n",
      "  Ended batch 025 / 034 | loss: 0.49798 | error: 0.17969\n",
      "  Ended batch 026 / 034 | loss: 0.48518 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34755 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.36209 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30258 | error: 0.14062\n",
      "  Ended batch 030 / 034 | loss: 0.27834 | error: 0.09375\n",
      "  Ended batch 031 / 034 | loss: 0.31573 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.46953 | error: 0.21094\n",
      "  Ended batch 033 / 034 | loss: 0.44510 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.35380 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 000 sec | loss: 0.45466 | error: 0.21103\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/non_strategic_0.6_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.40293 | error: 0.16406\n",
      "  Ended batch 002 / 034 | loss: 0.39698 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39748 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.33817 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31394 | error: 0.13281\n",
      "  Ended batch 006 / 034 | loss: 0.26311 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.40746 | error: 0.21094\n",
      "  Ended batch 008 / 034 | loss: 0.28619 | error: 0.12500\n",
      "  Ended batch 009 / 034 | loss: 0.42815 | error: 0.21875\n",
      "  Ended batch 010 / 034 | loss: 0.37308 | error: 0.14062\n",
      "  Ended batch 011 / 034 | loss: 0.24522 | error: 0.10938\n",
      "  Ended batch 012 / 034 | loss: 0.35200 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.38305 | error: 0.16406\n",
      "  Ended batch 014 / 034 | loss: 0.48793 | error: 0.18750\n",
      "  Ended batch 015 / 034 | loss: 0.29911 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37009 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39094 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.39047 | error: 0.16406\n",
      "  Ended batch 019 / 034 | loss: 0.33128 | error: 0.14844\n",
      "  Ended batch 020 / 034 | loss: 0.62716 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.39453 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.42599 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.40997 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38743 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.49943 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.46154 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34475 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.35799 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30337 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29895 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31889 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44511 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40606 | error: 0.16406\n",
      "  Ended batch 034 / 034 | loss: 0.41604 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 000 sec | loss: 0.44767 | error: 0.21363\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37960 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.38054 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39798 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.32809 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31529 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26527 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.39605 | error: 0.20312\n",
      "  Ended batch 008 / 034 | loss: 0.29298 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42259 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.37166 | error: 0.12500\n",
      "  Ended batch 011 / 034 | loss: 0.24852 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.34251 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37322 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47350 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31297 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.36932 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39082 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38728 | error: 0.15625\n",
      "  Ended batch 019 / 034 | loss: 0.33095 | error: 0.13281\n",
      "  Ended batch 020 / 034 | loss: 0.63589 | error: 0.24219\n",
      "  Ended batch 021 / 034 | loss: 0.39093 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41775 | error: 0.18750\n",
      "  Ended batch 023 / 034 | loss: 0.40417 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38116 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48604 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.45087 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34605 | error: 0.13281\n",
      "  Ended batch 028 / 034 | loss: 0.36658 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30254 | error: 0.10938\n",
      "  Ended batch 030 / 034 | loss: 0.30504 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31964 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44560 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40693 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.43542 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 000 sec | loss: 0.44874 | error: 0.21168\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37193 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37114 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39974 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31597 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31782 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26514 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.39098 | error: 0.18750\n",
      "  Ended batch 008 / 034 | loss: 0.28986 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42376 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.36756 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24499 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34379 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37043 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47858 | error: 0.20312\n",
      "  Ended batch 015 / 034 | loss: 0.30862 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37446 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38862 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38593 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32783 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64187 | error: 0.25000\n",
      "  Ended batch 021 / 034 | loss: 0.38988 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41519 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39861 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37473 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48543 | error: 0.21094\n",
      "  Ended batch 026 / 034 | loss: 0.44854 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34043 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36239 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.29949 | error: 0.11719\n",
      "  Ended batch 030 / 034 | loss: 0.29681 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31391 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45120 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40857 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42124 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 000 sec | loss: 0.44502 | error: 0.21103\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37063 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37067 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39355 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31063 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31461 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26898 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.38740 | error: 0.19531\n",
      "  Ended batch 008 / 034 | loss: 0.28760 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42075 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.36434 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24144 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34192 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.36529 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47503 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31060 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37830 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38680 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38255 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32599 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64071 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.38804 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41321 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39709 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37160 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48514 | error: 0.20312\n",
      "  Ended batch 026 / 034 | loss: 0.44696 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34037 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36098 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30023 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29524 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31327 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45363 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40813 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42413 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 000 sec | loss: 0.44367 | error: 0.21168\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 0.775007963180542 seconds.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.6_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.6_model.pt.\n",
      "---------- Training strategically with epsilon=0.6 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98034 | error: 0.44531\n",
      "  Ended batch 002 / 034 | loss: 0.85220 | error: 0.45312\n",
      "  Ended batch 003 / 034 | loss: 0.98315 | error: 0.49219\n",
      "  Ended batch 004 / 034 | loss: 0.85769 | error: 0.46094\n",
      "  Ended batch 005 / 034 | loss: 0.78938 | error: 0.45312\n",
      "  Ended batch 006 / 034 | loss: 0.69499 | error: 0.37500\n",
      "  Ended batch 007 / 034 | loss: 0.89528 | error: 0.51562\n",
      "  Ended batch 008 / 034 | loss: 0.74447 | error: 0.34375\n",
      "  Ended batch 009 / 034 | loss: 0.79034 | error: 0.35156\n",
      "  Ended batch 010 / 034 | loss: 0.72745 | error: 0.30469\n",
      "  Ended batch 011 / 034 | loss: 0.64094 | error: 0.21094\n",
      "  Ended batch 012 / 034 | loss: 0.77403 | error: 0.35938\n",
      "  Ended batch 013 / 034 | loss: 0.67769 | error: 0.28125\n",
      "  Ended batch 014 / 034 | loss: 0.72373 | error: 0.34375\n",
      "  Ended batch 015 / 034 | loss: 0.69611 | error: 0.36719\n",
      "  Ended batch 016 / 034 | loss: 0.62703 | error: 0.33594\n",
      "  Ended batch 017 / 034 | loss: 0.66887 | error: 0.35938\n",
      "  Ended batch 018 / 034 | loss: 0.65501 | error: 0.35156\n",
      "  Ended batch 019 / 034 | loss: 0.56835 | error: 0.28906\n",
      "  Ended batch 020 / 034 | loss: 0.69856 | error: 0.34375\n",
      "  Ended batch 021 / 034 | loss: 0.64926 | error: 0.32031\n",
      "  Ended batch 022 / 034 | loss: 0.64869 | error: 0.29688\n",
      "  Ended batch 023 / 034 | loss: 0.58103 | error: 0.28906\n",
      "  Ended batch 024 / 034 | loss: 0.66173 | error: 0.32812\n",
      "  Ended batch 025 / 034 | loss: 0.63216 | error: 0.32031\n",
      "  Ended batch 026 / 034 | loss: 0.64972 | error: 0.32812\n",
      "  Ended batch 027 / 034 | loss: 0.57782 | error: 0.30469\n",
      "  Ended batch 028 / 034 | loss: 0.62941 | error: 0.32031\n",
      "  Ended batch 029 / 034 | loss: 0.51293 | error: 0.27344\n",
      "  Ended batch 030 / 034 | loss: 0.46804 | error: 0.25000\n",
      "  Ended batch 031 / 034 | loss: 0.54845 | error: 0.28906\n",
      "  Ended batch 032 / 034 | loss: 0.64332 | error: 0.33594\n",
      "  Ended batch 033 / 034 | loss: 0.67072 | error: 0.32812\n",
      "  Ended batch 034 / 034 | loss: 0.68552 | error: 0.36364\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 083 sec | loss: 0.85916 | error: 0.30525\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.6_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.63588 | error: 0.33594\n",
      "  Ended batch 002 / 034 | loss: 0.51850 | error: 0.25781\n",
      "  Ended batch 003 / 034 | loss: 0.63793 | error: 0.34375\n",
      "  Ended batch 004 / 034 | loss: 0.68179 | error: 0.36719\n",
      "  Ended batch 005 / 034 | loss: 0.59238 | error: 0.32031\n",
      "  Ended batch 006 / 034 | loss: 0.47053 | error: 0.25781\n",
      "  Ended batch 007 / 034 | loss: 0.67871 | error: 0.37500\n",
      "  Ended batch 008 / 034 | loss: 0.51943 | error: 0.28125\n",
      "  Ended batch 009 / 034 | loss: 0.67788 | error: 0.36719\n",
      "  Ended batch 010 / 034 | loss: 0.58670 | error: 0.29688\n",
      "  Ended batch 011 / 034 | loss: 0.47373 | error: 0.25000\n",
      "  Ended batch 012 / 034 | loss: 0.66827 | error: 0.36719\n",
      "  Ended batch 013 / 034 | loss: 0.58448 | error: 0.29688\n",
      "  Ended batch 014 / 034 | loss: 0.67449 | error: 0.33594\n",
      "  Ended batch 015 / 034 | loss: 0.61540 | error: 0.33594\n",
      "  Ended batch 016 / 034 | loss: 0.57695 | error: 0.30469\n",
      "  Ended batch 017 / 034 | loss: 0.57684 | error: 0.30469\n",
      "  Ended batch 018 / 034 | loss: 0.62388 | error: 0.32812\n",
      "  Ended batch 019 / 034 | loss: 0.53945 | error: 0.27344\n",
      "  Ended batch 020 / 034 | loss: 0.68109 | error: 0.34375\n",
      "  Ended batch 021 / 034 | loss: 0.66207 | error: 0.35156\n",
      "  Ended batch 022 / 034 | loss: 0.62135 | error: 0.32031\n",
      "  Ended batch 023 / 034 | loss: 0.59448 | error: 0.32031\n",
      "  Ended batch 024 / 034 | loss: 0.66821 | error: 0.36719\n",
      "  Ended batch 025 / 034 | loss: 0.66065 | error: 0.35156\n",
      "  Ended batch 026 / 034 | loss: 0.66998 | error: 0.35938\n",
      "  Ended batch 027 / 034 | loss: 0.59300 | error: 0.32812\n",
      "  Ended batch 028 / 034 | loss: 0.63346 | error: 0.34375\n",
      "  Ended batch 029 / 034 | loss: 0.58498 | error: 0.32812\n",
      "  Ended batch 030 / 034 | loss: 0.59411 | error: 0.33594\n",
      "  Ended batch 031 / 034 | loss: 0.65507 | error: 0.36719\n",
      "  Ended batch 032 / 034 | loss: 0.73003 | error: 0.40625\n",
      "  Ended batch 033 / 034 | loss: 0.68609 | error: 0.35938\n",
      "  Ended batch 034 / 034 | loss: 0.72656 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 075 sec | loss: 0.65045 | error: 0.21215\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.6_model.pt.\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.65947 | error: 0.35938\n",
      "  Ended batch 002 / 034 | loss: 0.59399 | error: 0.29688\n",
      "  Ended batch 003 / 034 | loss: 0.67371 | error: 0.37500\n",
      "  Ended batch 004 / 034 | loss: 0.64561 | error: 0.36719\n",
      "  Ended batch 005 / 034 | loss: 0.60423 | error: 0.34375\n",
      "  Ended batch 006 / 034 | loss: 0.49760 | error: 0.28125\n",
      "  Ended batch 007 / 034 | loss: 0.81093 | error: 0.47656\n",
      "  Ended batch 008 / 034 | loss: 0.64083 | error: 0.37500\n",
      "  Ended batch 009 / 034 | loss: 0.66358 | error: 0.36719\n",
      "  Ended batch 010 / 034 | loss: 0.63263 | error: 0.31250\n",
      "  Ended batch 011 / 034 | loss: 0.48824 | error: 0.20312\n",
      "  Ended batch 012 / 034 | loss: 0.70947 | error: 0.40625\n",
      "  Ended batch 013 / 034 | loss: 0.64415 | error: 0.32031\n",
      "  Ended batch 014 / 034 | loss: 0.70821 | error: 0.34375\n",
      "  Ended batch 015 / 034 | loss: 0.64000 | error: 0.35938\n",
      "  Ended batch 016 / 034 | loss: 0.61833 | error: 0.33594\n",
      "  Ended batch 017 / 034 | loss: 0.67345 | error: 0.37500\n",
      "  Ended batch 018 / 034 | loss: 0.64337 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.60599 | error: 0.32812\n",
      "  Ended batch 020 / 034 | loss: 0.71113 | error: 0.38281\n",
      "  Ended batch 021 / 034 | loss: 0.69414 | error: 0.39062\n",
      "  Ended batch 022 / 034 | loss: 0.64824 | error: 0.33594\n",
      "  Ended batch 023 / 034 | loss: 0.63804 | error: 0.35156\n",
      "  Ended batch 024 / 034 | loss: 0.69737 | error: 0.40625\n",
      "  Ended batch 025 / 034 | loss: 0.70486 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.72880 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.61824 | error: 0.35156\n",
      "  Ended batch 028 / 034 | loss: 0.68118 | error: 0.39062\n",
      "  Ended batch 029 / 034 | loss: 0.58816 | error: 0.33594\n",
      "  Ended batch 030 / 034 | loss: 0.58424 | error: 0.31250\n",
      "  Ended batch 031 / 034 | loss: 0.65758 | error: 0.38281\n",
      "  Ended batch 032 / 034 | loss: 0.73774 | error: 0.42188\n",
      "  Ended batch 033 / 034 | loss: 0.72256 | error: 0.38281\n",
      "  Ended batch 034 / 034 | loss: 0.82574 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 083 sec | loss: 0.62645 | error: 0.20499\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.6_model.pt.\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.67803 | error: 0.37500\n",
      "  Ended batch 002 / 034 | loss: 0.61267 | error: 0.27344\n",
      "  Ended batch 003 / 034 | loss: 0.68102 | error: 0.35156\n",
      "  Ended batch 004 / 034 | loss: 0.61020 | error: 0.31250\n",
      "  Ended batch 005 / 034 | loss: 0.59028 | error: 0.30469\n",
      "  Ended batch 006 / 034 | loss: 0.48619 | error: 0.25781\n",
      "  Ended batch 007 / 034 | loss: 0.73576 | error: 0.42969\n",
      "  Ended batch 008 / 034 | loss: 0.66486 | error: 0.39062\n",
      "  Ended batch 009 / 034 | loss: 0.67626 | error: 0.37500\n",
      "  Ended batch 010 / 034 | loss: 0.62877 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.49628 | error: 0.26562\n",
      "  Ended batch 012 / 034 | loss: 0.73656 | error: 0.44531\n",
      "  Ended batch 013 / 034 | loss: 0.64094 | error: 0.33594\n",
      "  Ended batch 014 / 034 | loss: 0.68222 | error: 0.33594\n",
      "  Ended batch 015 / 034 | loss: 0.63870 | error: 0.35938\n",
      "  Ended batch 016 / 034 | loss: 0.60595 | error: 0.32812\n",
      "  Ended batch 017 / 034 | loss: 0.66284 | error: 0.36719\n",
      "  Ended batch 018 / 034 | loss: 0.63207 | error: 0.35156\n",
      "  Ended batch 019 / 034 | loss: 0.57846 | error: 0.31250\n",
      "  Ended batch 020 / 034 | loss: 0.71362 | error: 0.37500\n",
      "  Ended batch 021 / 034 | loss: 0.70166 | error: 0.39844\n",
      "  Ended batch 022 / 034 | loss: 0.62587 | error: 0.32031\n",
      "  Ended batch 023 / 034 | loss: 0.63800 | error: 0.35938\n",
      "  Ended batch 024 / 034 | loss: 0.68442 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.71252 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.69863 | error: 0.39844\n",
      "  Ended batch 027 / 034 | loss: 0.59941 | error: 0.33594\n",
      "  Ended batch 028 / 034 | loss: 0.64396 | error: 0.35938\n",
      "  Ended batch 029 / 034 | loss: 0.59129 | error: 0.33594\n",
      "  Ended batch 030 / 034 | loss: 0.60328 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.66477 | error: 0.38281\n",
      "  Ended batch 032 / 034 | loss: 0.70667 | error: 0.39844\n",
      "  Ended batch 033 / 034 | loss: 0.72762 | error: 0.38281\n",
      "  Ended batch 034 / 034 | loss: 0.74725 | error: 0.45455\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 077 sec | loss: 0.66153 | error: 0.21801\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.66476 | error: 0.34375\n",
      "  Ended batch 002 / 034 | loss: 0.58215 | error: 0.26562\n",
      "  Ended batch 003 / 034 | loss: 0.67537 | error: 0.37500\n",
      "  Ended batch 004 / 034 | loss: 0.65878 | error: 0.37500\n",
      "  Ended batch 005 / 034 | loss: 0.63928 | error: 0.37500\n",
      "  Ended batch 006 / 034 | loss: 0.52224 | error: 0.29688\n",
      "  Ended batch 007 / 034 | loss: 0.79819 | error: 0.46875\n",
      "  Ended batch 008 / 034 | loss: 0.64336 | error: 0.38281\n",
      "  Ended batch 009 / 034 | loss: 0.67299 | error: 0.37500\n",
      "  Ended batch 010 / 034 | loss: 0.63778 | error: 0.30469\n",
      "  Ended batch 011 / 034 | loss: 0.48858 | error: 0.20312\n",
      "  Ended batch 012 / 034 | loss: 0.72383 | error: 0.42188\n",
      "  Ended batch 013 / 034 | loss: 0.63010 | error: 0.31250\n",
      "  Ended batch 014 / 034 | loss: 0.70240 | error: 0.35938\n",
      "  Ended batch 015 / 034 | loss: 0.67710 | error: 0.39062\n",
      "  Ended batch 016 / 034 | loss: 0.62000 | error: 0.33594\n",
      "  Ended batch 017 / 034 | loss: 0.68672 | error: 0.38281\n",
      "  Ended batch 018 / 034 | loss: 0.64374 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.60009 | error: 0.32031\n",
      "  Ended batch 020 / 034 | loss: 0.70114 | error: 0.37500\n",
      "  Ended batch 021 / 034 | loss: 0.65690 | error: 0.35938\n",
      "  Ended batch 022 / 034 | loss: 0.64277 | error: 0.32812\n",
      "  Ended batch 023 / 034 | loss: 0.64176 | error: 0.35938\n",
      "  Ended batch 024 / 034 | loss: 0.71033 | error: 0.40625\n",
      "  Ended batch 025 / 034 | loss: 0.71023 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.72692 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.60582 | error: 0.32031\n",
      "  Ended batch 028 / 034 | loss: 0.64166 | error: 0.33594\n",
      "  Ended batch 029 / 034 | loss: 0.57773 | error: 0.28906\n",
      "  Ended batch 030 / 034 | loss: 0.58013 | error: 0.28906\n",
      "  Ended batch 031 / 034 | loss: 0.62690 | error: 0.35156\n",
      "  Ended batch 032 / 034 | loss: 0.74939 | error: 0.42188\n",
      "  Ended batch 033 / 034 | loss: 0.74985 | error: 0.39844\n",
      "  Ended batch 034 / 034 | loss: 0.85448 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 087 sec | loss: 0.61293 | error: 0.19782\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.6_model.pt.\n",
      "Starting epoch 006 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.68689 | error: 0.39062\n",
      "  Ended batch 002 / 034 | loss: 0.64844 | error: 0.28906\n",
      "  Ended batch 003 / 034 | loss: 0.67425 | error: 0.35938\n",
      "  Ended batch 004 / 034 | loss: 0.61586 | error: 0.26562\n",
      "  Ended batch 005 / 034 | loss: 0.57828 | error: 0.21094\n",
      "  Ended batch 006 / 034 | loss: 0.45575 | error: 0.19531\n",
      "  Ended batch 007 / 034 | loss: 0.72895 | error: 0.42969\n",
      "  Ended batch 008 / 034 | loss: 0.64847 | error: 0.37500\n",
      "  Ended batch 009 / 034 | loss: 0.69376 | error: 0.38281\n",
      "  Ended batch 010 / 034 | loss: 0.62123 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.48740 | error: 0.26562\n",
      "  Ended batch 012 / 034 | loss: 0.71837 | error: 0.42188\n",
      "  Ended batch 013 / 034 | loss: 0.62321 | error: 0.31250\n",
      "  Ended batch 014 / 034 | loss: 0.70145 | error: 0.34375\n",
      "  Ended batch 015 / 034 | loss: 0.60813 | error: 0.32812\n",
      "  Ended batch 016 / 034 | loss: 0.59778 | error: 0.32031\n",
      "  Ended batch 017 / 034 | loss: 0.66208 | error: 0.36719\n",
      "  Ended batch 018 / 034 | loss: 0.63416 | error: 0.35156\n",
      "  Ended batch 019 / 034 | loss: 0.55560 | error: 0.29688\n",
      "  Ended batch 020 / 034 | loss: 0.72355 | error: 0.37500\n",
      "  Ended batch 021 / 034 | loss: 0.70713 | error: 0.39844\n",
      "  Ended batch 022 / 034 | loss: 0.62571 | error: 0.32031\n",
      "  Ended batch 023 / 034 | loss: 0.61222 | error: 0.33594\n",
      "  Ended batch 024 / 034 | loss: 0.69070 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.71398 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.69251 | error: 0.39062\n",
      "  Ended batch 027 / 034 | loss: 0.59926 | error: 0.33594\n",
      "  Ended batch 028 / 034 | loss: 0.66249 | error: 0.37500\n",
      "  Ended batch 029 / 034 | loss: 0.58094 | error: 0.32812\n",
      "  Ended batch 030 / 034 | loss: 0.59290 | error: 0.34375\n",
      "  Ended batch 031 / 034 | loss: 0.65819 | error: 0.38281\n",
      "  Ended batch 032 / 034 | loss: 0.73190 | error: 0.42188\n",
      "  Ended batch 033 / 034 | loss: 0.71339 | error: 0.38281\n",
      "  Ended batch 034 / 034 | loss: 0.81539 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 006 / 016 | time: 079 sec | loss: 0.65852 | error: 0.21801\n",
      "Starting epoch 007 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.66209 | error: 0.35156\n",
      "  Ended batch 002 / 034 | loss: 0.58558 | error: 0.20312\n",
      "  Ended batch 003 / 034 | loss: 0.67787 | error: 0.35938\n",
      "  Ended batch 004 / 034 | loss: 0.61023 | error: 0.32812\n",
      "  Ended batch 005 / 034 | loss: 0.58427 | error: 0.32031\n",
      "  Ended batch 006 / 034 | loss: 0.47314 | error: 0.25781\n",
      "  Ended batch 007 / 034 | loss: 0.81857 | error: 0.46875\n",
      "  Ended batch 008 / 034 | loss: 0.67712 | error: 0.39062\n",
      "  Ended batch 009 / 034 | loss: 0.66773 | error: 0.36719\n",
      "  Ended batch 010 / 034 | loss: 0.63278 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.49573 | error: 0.25781\n",
      "  Ended batch 012 / 034 | loss: 0.72339 | error: 0.43750\n",
      "  Ended batch 013 / 034 | loss: 0.62414 | error: 0.31250\n",
      "  Ended batch 014 / 034 | loss: 0.68182 | error: 0.34375\n",
      "  Ended batch 015 / 034 | loss: 0.63727 | error: 0.35938\n",
      "  Ended batch 016 / 034 | loss: 0.62597 | error: 0.34375\n",
      "  Ended batch 017 / 034 | loss: 0.66431 | error: 0.36719\n",
      "  Ended batch 018 / 034 | loss: 0.64467 | error: 0.35938\n",
      "  Ended batch 019 / 034 | loss: 0.58890 | error: 0.32031\n",
      "  Ended batch 020 / 034 | loss: 0.71314 | error: 0.38281\n",
      "  Ended batch 021 / 034 | loss: 0.69937 | error: 0.39844\n",
      "  Ended batch 022 / 034 | loss: 0.64121 | error: 0.32812\n",
      "  Ended batch 023 / 034 | loss: 0.63568 | error: 0.35938\n",
      "  Ended batch 024 / 034 | loss: 0.68886 | error: 0.39844\n",
      "  Ended batch 025 / 034 | loss: 0.70447 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.71614 | error: 0.41406\n",
      "  Ended batch 027 / 034 | loss: 0.65308 | error: 0.37500\n",
      "  Ended batch 028 / 034 | loss: 0.68399 | error: 0.36719\n",
      "  Ended batch 029 / 034 | loss: 0.58353 | error: 0.32031\n",
      "  Ended batch 030 / 034 | loss: 0.57881 | error: 0.30469\n",
      "  Ended batch 031 / 034 | loss: 0.63133 | error: 0.35156\n",
      "  Ended batch 032 / 034 | loss: 0.75780 | error: 0.42188\n",
      "  Ended batch 033 / 034 | loss: 0.71707 | error: 0.37500\n",
      "  Ended batch 034 / 034 | loss: 0.83778 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 007 / 016 | time: 076 sec | loss: 0.64159 | error: 0.20889\n",
      "Starting epoch 008 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.67842 | error: 0.37500\n",
      "  Ended batch 002 / 034 | loss: 0.59311 | error: 0.27344\n",
      "  Ended batch 003 / 034 | loss: 0.67938 | error: 0.36719\n",
      "  Ended batch 004 / 034 | loss: 0.63632 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.58060 | error: 0.29688\n",
      "  Ended batch 006 / 034 | loss: 0.48291 | error: 0.25781\n",
      "  Ended batch 007 / 034 | loss: 0.74565 | error: 0.42969\n",
      "  Ended batch 008 / 034 | loss: 0.65698 | error: 0.37500\n",
      "  Ended batch 009 / 034 | loss: 0.66844 | error: 0.36719\n",
      "  Ended batch 010 / 034 | loss: 0.63086 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.47988 | error: 0.25000\n",
      "  Ended batch 012 / 034 | loss: 0.69187 | error: 0.40625\n",
      "  Ended batch 013 / 034 | loss: 0.62785 | error: 0.32031\n",
      "  Ended batch 014 / 034 | loss: 0.68428 | error: 0.33594\n",
      "  Ended batch 015 / 034 | loss: 0.63182 | error: 0.35156\n",
      "  Ended batch 016 / 034 | loss: 0.61675 | error: 0.33594\n",
      "  Ended batch 017 / 034 | loss: 0.66381 | error: 0.36719\n",
      "  Ended batch 018 / 034 | loss: 0.63429 | error: 0.35156\n",
      "  Ended batch 019 / 034 | loss: 0.58402 | error: 0.31250\n",
      "  Ended batch 020 / 034 | loss: 0.70293 | error: 0.36719\n",
      "  Ended batch 021 / 034 | loss: 0.70167 | error: 0.39844\n",
      "  Ended batch 022 / 034 | loss: 0.63004 | error: 0.32031\n",
      "  Ended batch 023 / 034 | loss: 0.63876 | error: 0.35938\n",
      "  Ended batch 024 / 034 | loss: 0.69333 | error: 0.39844\n",
      "  Ended batch 025 / 034 | loss: 0.70013 | error: 0.39844\n",
      "  Ended batch 026 / 034 | loss: 0.70728 | error: 0.39844\n",
      "  Ended batch 027 / 034 | loss: 0.61878 | error: 0.35156\n",
      "  Ended batch 028 / 034 | loss: 0.67120 | error: 0.38281\n",
      "  Ended batch 029 / 034 | loss: 0.58056 | error: 0.32812\n",
      "  Ended batch 030 / 034 | loss: 0.57563 | error: 0.32812\n",
      "  Ended batch 031 / 034 | loss: 0.65422 | error: 0.37500\n",
      "  Ended batch 032 / 034 | loss: 0.71739 | error: 0.39844\n",
      "  Ended batch 033 / 034 | loss: 0.71985 | error: 0.38281\n",
      "  Ended batch 034 / 034 | loss: 0.81715 | error: 0.50000\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 008 / 016 | time: 102 sec | loss: 0.67304 | error: 0.22387\n",
      "Starting epoch 009 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.67188 | error: 0.32812\n",
      "  Ended batch 002 / 034 | loss: 0.59153 | error: 0.19531\n",
      "  Ended batch 003 / 034 | loss: 0.66230 | error: 0.28906\n",
      "  Ended batch 004 / 034 | loss: 0.61216 | error: 0.31250\n",
      "  Ended batch 005 / 034 | loss: 0.57716 | error: 0.30469\n",
      "  Ended batch 006 / 034 | loss: 0.48453 | error: 0.26562\n",
      "  Ended batch 007 / 034 | loss: 0.80760 | error: 0.46094\n",
      "  Ended batch 008 / 034 | loss: 0.67426 | error: 0.38281\n",
      "  Ended batch 009 / 034 | loss: 0.69427 | error: 0.38281\n",
      "  Ended batch 010 / 034 | loss: 0.61756 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.45930 | error: 0.22656\n",
      "  Ended batch 012 / 034 | loss: 0.69164 | error: 0.40625\n",
      "  Ended batch 013 / 034 | loss: 0.62867 | error: 0.30469\n",
      "  Ended batch 014 / 034 | loss: 0.68732 | error: 0.32812\n",
      "  Ended batch 015 / 034 | loss: 0.64408 | error: 0.35938\n",
      "  Ended batch 016 / 034 | loss: 0.59774 | error: 0.32031\n",
      "  Ended batch 017 / 034 | loss: 0.66927 | error: 0.36719\n",
      "  Ended batch 018 / 034 | loss: 0.66374 | error: 0.36719\n",
      "  Ended batch 019 / 034 | loss: 0.58165 | error: 0.31250\n",
      "  Ended batch 020 / 034 | loss: 0.71554 | error: 0.38281\n",
      "  Ended batch 021 / 034 | loss: 0.68126 | error: 0.37500\n",
      "  Ended batch 022 / 034 | loss: 0.64507 | error: 0.32812\n",
      "  Ended batch 023 / 034 | loss: 0.60613 | error: 0.32812\n",
      "  Ended batch 024 / 034 | loss: 0.69557 | error: 0.39844\n",
      "  Ended batch 025 / 034 | loss: 0.70820 | error: 0.40625\n",
      "  Ended batch 026 / 034 | loss: 0.73366 | error: 0.42188\n",
      "  Ended batch 027 / 034 | loss: 0.63442 | error: 0.36719\n",
      "  Ended batch 028 / 034 | loss: 0.64466 | error: 0.35156\n",
      "  Ended batch 029 / 034 | loss: 0.58538 | error: 0.32812\n",
      "  Ended batch 030 / 034 | loss: 0.58788 | error: 0.33594\n",
      "  Ended batch 031 / 034 | loss: 0.64872 | error: 0.37500\n",
      "  Ended batch 032 / 034 | loss: 0.74286 | error: 0.42188\n",
      "  Ended batch 033 / 034 | loss: 0.70862 | error: 0.37500\n",
      "  Ended batch 034 / 034 | loss: 0.72961 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 009 / 016 | time: 079 sec | loss: 0.64944 | error: 0.21150\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 12.328966653347015 minutes (739.7379992008209 seconds).\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.6_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.6_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "---------- Calculating results ----------\n",
      "Evaluating using epsilon: 0.6\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0903, -0.2800, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "         -0.0746,  0.0482, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919,  0.1656,\n",
      "          0.3650,  0.3795, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "          0.0652, -0.1662, -0.0495,  0.3771, -0.0304, -0.0473, -0.0763]])\n",
      "Evaluating using epsilon: 0.6\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0921, -0.2706, -0.0810, -0.0896, -0.0429, -0.0722, -0.0931, -0.1217,\n",
      "         -0.0719,  0.0454, -0.0906, -0.1913, -0.0755, -0.0528, -0.0783],\n",
      "        [-0.0757, -0.0054, -0.0844, -0.0931, -0.0426, -0.0745, -0.0953,  0.1732,\n",
      "          0.3722,  0.3722, -0.0960, -0.1930, -0.0767, -0.0537, -0.0816],\n",
      "        [-0.0732, -0.0190, -0.0815, -0.0901, -0.0429, -0.0725, -0.0934, -0.1211,\n",
      "          0.0686, -0.1696, -0.0535,  0.3758, -0.0313, -0.0479, -0.0787]])\n",
      "Evaluating using epsilon: 0.6\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0949, -0.2629, -0.0823, -0.0907, -0.0427, -0.0733, -0.0937, -0.1203,\n",
      "         -0.0710,  0.0445, -0.0944, -0.1904, -0.0746, -0.0503, -0.0791],\n",
      "        [-0.2443,  0.6165, -0.2010, -0.2043, -0.0287, -0.1648, -0.1623,  0.3258,\n",
      "          0.5023,  0.2411, -0.3529, -0.1953, -0.0679,  0.0253, -0.1821],\n",
      "        [-0.0876,  0.0314, -0.0908, -0.0987, -0.0417, -0.0798, -0.0986, -0.1092,\n",
      "          0.0784, -0.1795, -0.0750,  0.3766, -0.0298, -0.0399, -0.0865]])\n",
      "Evaluating using epsilon: 0.6\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1087,  0.1333, -0.2311, -0.2314, -0.2245, -0.1772, -0.1283, -0.1832,\n",
      "         -0.1202, -0.0266, -0.3222, -0.1929, -0.0661,  0.0744, -0.1359],\n",
      "        [-0.0864,  0.3158, -0.2065, -0.2081, -0.1952, -0.1600, -0.1224,  0.1165,\n",
      "          0.3268,  0.3168, -0.2842, -0.1924, -0.0675,  0.0539, -0.1262],\n",
      "        [-0.0855,  0.2959, -0.1992, -0.2012, -0.1865, -0.1549, -0.1207, -0.1709,\n",
      "          0.0292, -0.2253, -0.2351,  0.3750, -0.0236,  0.0528, -0.1234]])\n",
      "------------------------- 0.8 -------------------------\n",
      "---------- Training non-strategically with epsilon=0.8 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98923 | error: 0.43750\n",
      "  Ended batch 002 / 034 | loss: 0.83193 | error: 0.39844\n",
      "  Ended batch 003 / 034 | loss: 0.75601 | error: 0.38281\n",
      "  Ended batch 004 / 034 | loss: 0.62267 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.53573 | error: 0.26562\n",
      "  Ended batch 006 / 034 | loss: 0.43920 | error: 0.17188\n",
      "  Ended batch 007 / 034 | loss: 0.55319 | error: 0.24219\n",
      "  Ended batch 008 / 034 | loss: 0.43821 | error: 0.17188\n",
      "  Ended batch 009 / 034 | loss: 0.48477 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.44141 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.34122 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.39537 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.46204 | error: 0.17969\n",
      "  Ended batch 014 / 034 | loss: 0.61706 | error: 0.21094\n",
      "  Ended batch 015 / 034 | loss: 0.40735 | error: 0.15625\n",
      "  Ended batch 016 / 034 | loss: 0.44818 | error: 0.19531\n",
      "  Ended batch 017 / 034 | loss: 0.42789 | error: 0.17969\n",
      "  Ended batch 018 / 034 | loss: 0.45081 | error: 0.14062\n",
      "  Ended batch 019 / 034 | loss: 0.40077 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.67522 | error: 0.20312\n",
      "  Ended batch 021 / 034 | loss: 0.44442 | error: 0.17188\n",
      "  Ended batch 022 / 034 | loss: 0.49886 | error: 0.20312\n",
      "  Ended batch 023 / 034 | loss: 0.43399 | error: 0.16406\n",
      "  Ended batch 024 / 034 | loss: 0.42774 | error: 0.17188\n",
      "  Ended batch 025 / 034 | loss: 0.49798 | error: 0.17969\n",
      "  Ended batch 026 / 034 | loss: 0.48518 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34755 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.36209 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30258 | error: 0.14062\n",
      "  Ended batch 030 / 034 | loss: 0.27834 | error: 0.09375\n",
      "  Ended batch 031 / 034 | loss: 0.31573 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.46953 | error: 0.21094\n",
      "  Ended batch 033 / 034 | loss: 0.44510 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.35380 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 000 sec | loss: 0.45466 | error: 0.21103\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/non_strategic_0.8_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.40293 | error: 0.16406\n",
      "  Ended batch 002 / 034 | loss: 0.39698 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39748 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.33817 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31394 | error: 0.13281\n",
      "  Ended batch 006 / 034 | loss: 0.26311 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.40746 | error: 0.21094\n",
      "  Ended batch 008 / 034 | loss: 0.28619 | error: 0.12500\n",
      "  Ended batch 009 / 034 | loss: 0.42815 | error: 0.21875\n",
      "  Ended batch 010 / 034 | loss: 0.37308 | error: 0.14062\n",
      "  Ended batch 011 / 034 | loss: 0.24522 | error: 0.10938\n",
      "  Ended batch 012 / 034 | loss: 0.35200 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.38305 | error: 0.16406\n",
      "  Ended batch 014 / 034 | loss: 0.48793 | error: 0.18750\n",
      "  Ended batch 015 / 034 | loss: 0.29911 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37009 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39094 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.39047 | error: 0.16406\n",
      "  Ended batch 019 / 034 | loss: 0.33128 | error: 0.14844\n",
      "  Ended batch 020 / 034 | loss: 0.62716 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.39453 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.42599 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.40997 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38743 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.49943 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.46154 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34475 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.35799 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30337 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29895 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31889 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44511 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40606 | error: 0.16406\n",
      "  Ended batch 034 / 034 | loss: 0.41604 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 000 sec | loss: 0.44767 | error: 0.21363\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37960 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.38054 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39798 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.32809 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31529 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26527 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.39605 | error: 0.20312\n",
      "  Ended batch 008 / 034 | loss: 0.29298 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42259 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.37166 | error: 0.12500\n",
      "  Ended batch 011 / 034 | loss: 0.24852 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.34251 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37322 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47350 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31297 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.36932 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39082 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38728 | error: 0.15625\n",
      "  Ended batch 019 / 034 | loss: 0.33095 | error: 0.13281\n",
      "  Ended batch 020 / 034 | loss: 0.63589 | error: 0.24219\n",
      "  Ended batch 021 / 034 | loss: 0.39093 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41775 | error: 0.18750\n",
      "  Ended batch 023 / 034 | loss: 0.40417 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38116 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48604 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.45087 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34605 | error: 0.13281\n",
      "  Ended batch 028 / 034 | loss: 0.36658 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30254 | error: 0.10938\n",
      "  Ended batch 030 / 034 | loss: 0.30504 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31964 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44560 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40693 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.43542 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 000 sec | loss: 0.44874 | error: 0.21168\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37193 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37114 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39974 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31597 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31782 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26514 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.39098 | error: 0.18750\n",
      "  Ended batch 008 / 034 | loss: 0.28986 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42376 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.36756 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24499 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34379 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37043 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47858 | error: 0.20312\n",
      "  Ended batch 015 / 034 | loss: 0.30862 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37446 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38862 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38593 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32783 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64187 | error: 0.25000\n",
      "  Ended batch 021 / 034 | loss: 0.38988 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41519 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39861 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37473 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48543 | error: 0.21094\n",
      "  Ended batch 026 / 034 | loss: 0.44854 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34043 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36239 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.29949 | error: 0.11719\n",
      "  Ended batch 030 / 034 | loss: 0.29681 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31391 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45120 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40857 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42124 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 000 sec | loss: 0.44502 | error: 0.21103\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37063 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37067 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39355 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31063 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31461 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26898 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.38740 | error: 0.19531\n",
      "  Ended batch 008 / 034 | loss: 0.28760 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42075 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.36434 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24144 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34192 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.36529 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47503 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31060 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37830 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38680 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38255 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32599 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64071 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.38804 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41321 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39709 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37160 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48514 | error: 0.20312\n",
      "  Ended batch 026 / 034 | loss: 0.44696 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34037 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36098 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30023 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29524 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31327 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45363 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40813 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42413 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 000 sec | loss: 0.44367 | error: 0.21168\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 0.7300589084625244 seconds.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.8_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.8_model.pt.\n",
      "---------- Training strategically with epsilon=0.8 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98253 | error: 0.44531\n",
      "  Ended batch 002 / 034 | loss: 0.85097 | error: 0.45312\n",
      "  Ended batch 003 / 034 | loss: 0.95938 | error: 0.48438\n",
      "  Ended batch 004 / 034 | loss: 0.87151 | error: 0.46094\n",
      "  Ended batch 005 / 034 | loss: 0.80289 | error: 0.45312\n",
      "  Ended batch 006 / 034 | loss: 0.66882 | error: 0.35938\n",
      "  Ended batch 007 / 034 | loss: 0.85727 | error: 0.50000\n",
      "  Ended batch 008 / 034 | loss: 0.72560 | error: 0.26562\n",
      "  Ended batch 009 / 034 | loss: 0.77031 | error: 0.28906\n",
      "  Ended batch 010 / 034 | loss: 0.71946 | error: 0.22656\n",
      "  Ended batch 011 / 034 | loss: 0.61742 | error: 0.17188\n",
      "  Ended batch 012 / 034 | loss: 0.73739 | error: 0.30469\n",
      "  Ended batch 013 / 034 | loss: 0.65153 | error: 0.27344\n",
      "  Ended batch 014 / 034 | loss: 0.68026 | error: 0.31250\n",
      "  Ended batch 015 / 034 | loss: 0.64588 | error: 0.35938\n",
      "  Ended batch 016 / 034 | loss: 0.61586 | error: 0.33594\n",
      "  Ended batch 017 / 034 | loss: 0.64258 | error: 0.33594\n",
      "  Ended batch 018 / 034 | loss: 0.61260 | error: 0.32812\n",
      "  Ended batch 019 / 034 | loss: 0.53469 | error: 0.26562\n",
      "  Ended batch 020 / 034 | loss: 0.70915 | error: 0.35938\n",
      "  Ended batch 021 / 034 | loss: 0.55157 | error: 0.25781\n",
      "  Ended batch 022 / 034 | loss: 0.65451 | error: 0.30469\n",
      "  Ended batch 023 / 034 | loss: 0.55080 | error: 0.26562\n",
      "  Ended batch 024 / 034 | loss: 0.63898 | error: 0.32031\n",
      "  Ended batch 025 / 034 | loss: 0.61336 | error: 0.31250\n",
      "  Ended batch 026 / 034 | loss: 0.63264 | error: 0.32031\n",
      "  Ended batch 027 / 034 | loss: 0.54121 | error: 0.28906\n",
      "  Ended batch 028 / 034 | loss: 0.57972 | error: 0.29688\n",
      "  Ended batch 029 / 034 | loss: 0.54124 | error: 0.29688\n",
      "  Ended batch 030 / 034 | loss: 0.47114 | error: 0.25781\n",
      "  Ended batch 031 / 034 | loss: 0.50246 | error: 0.26562\n",
      "  Ended batch 032 / 034 | loss: 0.65853 | error: 0.35156\n",
      "  Ended batch 033 / 034 | loss: 0.59000 | error: 0.28906\n",
      "  Ended batch 034 / 034 | loss: 0.59948 | error: 0.31818\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 074 sec | loss: 0.75923 | error: 0.25967\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.8_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.63034 | error: 0.33594\n",
      "  Ended batch 002 / 034 | loss: 0.56976 | error: 0.29688\n",
      "  Ended batch 003 / 034 | loss: 0.62441 | error: 0.33594\n",
      "  Ended batch 004 / 034 | loss: 0.68990 | error: 0.37500\n",
      "  Ended batch 005 / 034 | loss: 0.54843 | error: 0.29688\n",
      "  Ended batch 006 / 034 | loss: 0.48998 | error: 0.27344\n",
      "  Ended batch 007 / 034 | loss: 0.68153 | error: 0.38281\n",
      "  Ended batch 008 / 034 | loss: 0.47698 | error: 0.25781\n",
      "  Ended batch 009 / 034 | loss: 0.61543 | error: 0.32812\n",
      "  Ended batch 010 / 034 | loss: 0.48708 | error: 0.23438\n",
      "  Ended batch 011 / 034 | loss: 0.40415 | error: 0.21094\n",
      "  Ended batch 012 / 034 | loss: 0.66568 | error: 0.36719\n",
      "  Ended batch 013 / 034 | loss: 0.53892 | error: 0.27344\n",
      "  Ended batch 014 / 034 | loss: 0.52544 | error: 0.25000\n",
      "  Ended batch 015 / 034 | loss: 0.57375 | error: 0.31250\n",
      "  Ended batch 016 / 034 | loss: 0.52479 | error: 0.27344\n",
      "  Ended batch 017 / 034 | loss: 0.55104 | error: 0.28906\n",
      "  Ended batch 018 / 034 | loss: 0.54203 | error: 0.27344\n",
      "  Ended batch 019 / 034 | loss: 0.50473 | error: 0.25000\n",
      "  Ended batch 020 / 034 | loss: 0.67173 | error: 0.34375\n",
      "  Ended batch 021 / 034 | loss: 0.62357 | error: 0.32812\n",
      "  Ended batch 022 / 034 | loss: 0.58554 | error: 0.30469\n",
      "  Ended batch 023 / 034 | loss: 0.55593 | error: 0.29688\n",
      "  Ended batch 024 / 034 | loss: 0.64146 | error: 0.35156\n",
      "  Ended batch 025 / 034 | loss: 0.61272 | error: 0.32812\n",
      "  Ended batch 026 / 034 | loss: 0.66786 | error: 0.36719\n",
      "  Ended batch 027 / 034 | loss: 0.57138 | error: 0.32031\n",
      "  Ended batch 028 / 034 | loss: 0.61635 | error: 0.34375\n",
      "  Ended batch 029 / 034 | loss: 0.55273 | error: 0.31250\n",
      "  Ended batch 030 / 034 | loss: 0.53776 | error: 0.30469\n",
      "  Ended batch 031 / 034 | loss: 0.58875 | error: 0.32812\n",
      "  Ended batch 032 / 034 | loss: 0.66441 | error: 0.36719\n",
      "  Ended batch 033 / 034 | loss: 0.65109 | error: 0.33594\n",
      "  Ended batch 034 / 034 | loss: 0.67331 | error: 0.36364\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 073 sec | loss: 0.62869 | error: 0.20368\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.8_model.pt.\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.63094 | error: 0.35156\n",
      "  Ended batch 002 / 034 | loss: 0.59409 | error: 0.32031\n",
      "  Ended batch 003 / 034 | loss: 0.66433 | error: 0.37500\n",
      "  Ended batch 004 / 034 | loss: 0.65125 | error: 0.36719\n",
      "  Ended batch 005 / 034 | loss: 0.58560 | error: 0.33594\n",
      "  Ended batch 006 / 034 | loss: 0.43977 | error: 0.24219\n",
      "  Ended batch 007 / 034 | loss: 0.73839 | error: 0.42188\n",
      "  Ended batch 008 / 034 | loss: 0.50717 | error: 0.28125\n",
      "  Ended batch 009 / 034 | loss: 0.64033 | error: 0.35156\n",
      "  Ended batch 010 / 034 | loss: 0.62673 | error: 0.32812\n",
      "  Ended batch 011 / 034 | loss: 0.45580 | error: 0.23438\n",
      "  Ended batch 012 / 034 | loss: 0.69845 | error: 0.40625\n",
      "  Ended batch 013 / 034 | loss: 0.57285 | error: 0.28125\n",
      "  Ended batch 014 / 034 | loss: 0.64641 | error: 0.32031\n",
      "  Ended batch 015 / 034 | loss: 0.64995 | error: 0.36719\n",
      "  Ended batch 016 / 034 | loss: 0.59686 | error: 0.32031\n",
      "  Ended batch 017 / 034 | loss: 0.63481 | error: 0.35156\n",
      "  Ended batch 018 / 034 | loss: 0.61680 | error: 0.34375\n",
      "  Ended batch 019 / 034 | loss: 0.54416 | error: 0.26562\n",
      "  Ended batch 020 / 034 | loss: 0.67499 | error: 0.35938\n",
      "  Ended batch 021 / 034 | loss: 0.64226 | error: 0.35938\n",
      "  Ended batch 022 / 034 | loss: 0.61955 | error: 0.32812\n",
      "  Ended batch 023 / 034 | loss: 0.63305 | error: 0.35938\n",
      "  Ended batch 024 / 034 | loss: 0.68146 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.67499 | error: 0.39062\n",
      "  Ended batch 026 / 034 | loss: 0.66637 | error: 0.37500\n",
      "  Ended batch 027 / 034 | loss: 0.59209 | error: 0.32812\n",
      "  Ended batch 028 / 034 | loss: 0.65965 | error: 0.35156\n",
      "  Ended batch 029 / 034 | loss: 0.58272 | error: 0.33594\n",
      "  Ended batch 030 / 034 | loss: 0.55383 | error: 0.30469\n",
      "  Ended batch 031 / 034 | loss: 0.60651 | error: 0.34375\n",
      "  Ended batch 032 / 034 | loss: 0.70797 | error: 0.39844\n",
      "  Ended batch 033 / 034 | loss: 0.70000 | error: 0.36719\n",
      "  Ended batch 034 / 034 | loss: 0.70160 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 069 sec | loss: 0.61511 | error: 0.19913\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.8_model.pt.\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.66766 | error: 0.37500\n",
      "  Ended batch 002 / 034 | loss: 0.61068 | error: 0.29688\n",
      "  Ended batch 003 / 034 | loss: 0.65599 | error: 0.37500\n",
      "  Ended batch 004 / 034 | loss: 0.62796 | error: 0.34375\n",
      "  Ended batch 005 / 034 | loss: 0.57311 | error: 0.31250\n",
      "  Ended batch 006 / 034 | loss: 0.46058 | error: 0.25781\n",
      "  Ended batch 007 / 034 | loss: 0.72990 | error: 0.42969\n",
      "  Ended batch 008 / 034 | loss: 0.54801 | error: 0.31250\n",
      "  Ended batch 009 / 034 | loss: 0.65156 | error: 0.35938\n",
      "  Ended batch 010 / 034 | loss: 0.63349 | error: 0.35156\n",
      "  Ended batch 011 / 034 | loss: 0.45945 | error: 0.24219\n",
      "  Ended batch 012 / 034 | loss: 0.69543 | error: 0.41406\n",
      "  Ended batch 013 / 034 | loss: 0.58850 | error: 0.28906\n",
      "  Ended batch 014 / 034 | loss: 0.65985 | error: 0.33594\n",
      "  Ended batch 015 / 034 | loss: 0.62132 | error: 0.35156\n",
      "  Ended batch 016 / 034 | loss: 0.59857 | error: 0.32812\n",
      "  Ended batch 017 / 034 | loss: 0.64591 | error: 0.35938\n",
      "  Ended batch 018 / 034 | loss: 0.61541 | error: 0.34375\n",
      "  Ended batch 019 / 034 | loss: 0.55193 | error: 0.28906\n",
      "  Ended batch 020 / 034 | loss: 0.68100 | error: 0.35938\n",
      "  Ended batch 021 / 034 | loss: 0.64034 | error: 0.35938\n",
      "  Ended batch 022 / 034 | loss: 0.61788 | error: 0.32031\n",
      "  Ended batch 023 / 034 | loss: 0.61941 | error: 0.34375\n",
      "  Ended batch 024 / 034 | loss: 0.67749 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.67192 | error: 0.39062\n",
      "  Ended batch 026 / 034 | loss: 0.65727 | error: 0.37500\n",
      "  Ended batch 027 / 034 | loss: 0.58046 | error: 0.32031\n",
      "  Ended batch 028 / 034 | loss: 0.63368 | error: 0.37500\n",
      "  Ended batch 029 / 034 | loss: 0.58028 | error: 0.34375\n",
      "  Ended batch 030 / 034 | loss: 0.54987 | error: 0.32031\n",
      "  Ended batch 031 / 034 | loss: 0.61641 | error: 0.35156\n",
      "  Ended batch 032 / 034 | loss: 0.71344 | error: 0.39844\n",
      "  Ended batch 033 / 034 | loss: 0.70234 | error: 0.36719\n",
      "  Ended batch 034 / 034 | loss: 0.69465 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 077 sec | loss: 0.62974 | error: 0.20499\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.68562 | error: 0.36719\n",
      "  Ended batch 002 / 034 | loss: 0.59007 | error: 0.26562\n",
      "  Ended batch 003 / 034 | loss: 0.66387 | error: 0.36719\n",
      "  Ended batch 004 / 034 | loss: 0.63375 | error: 0.34375\n",
      "  Ended batch 005 / 034 | loss: 0.56138 | error: 0.30469\n",
      "  Ended batch 006 / 034 | loss: 0.46587 | error: 0.25781\n",
      "  Ended batch 007 / 034 | loss: 0.74532 | error: 0.43750\n",
      "  Ended batch 008 / 034 | loss: 0.60185 | error: 0.35156\n",
      "  Ended batch 009 / 034 | loss: 0.65518 | error: 0.35938\n",
      "  Ended batch 010 / 034 | loss: 0.62509 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.45862 | error: 0.24219\n",
      "  Ended batch 012 / 034 | loss: 0.70173 | error: 0.42188\n",
      "  Ended batch 013 / 034 | loss: 0.59501 | error: 0.28906\n",
      "  Ended batch 014 / 034 | loss: 0.67697 | error: 0.32812\n",
      "  Ended batch 015 / 034 | loss: 0.60026 | error: 0.33594\n",
      "  Ended batch 016 / 034 | loss: 0.58776 | error: 0.32031\n",
      "  Ended batch 017 / 034 | loss: 0.65861 | error: 0.36719\n",
      "  Ended batch 018 / 034 | loss: 0.61780 | error: 0.34375\n",
      "  Ended batch 019 / 034 | loss: 0.55167 | error: 0.28906\n",
      "  Ended batch 020 / 034 | loss: 0.67623 | error: 0.35156\n",
      "  Ended batch 021 / 034 | loss: 0.65002 | error: 0.35938\n",
      "  Ended batch 022 / 034 | loss: 0.62906 | error: 0.32812\n",
      "  Ended batch 023 / 034 | loss: 0.59207 | error: 0.32812\n",
      "  Ended batch 024 / 034 | loss: 0.68840 | error: 0.39844\n",
      "  Ended batch 025 / 034 | loss: 0.65659 | error: 0.37500\n",
      "  Ended batch 026 / 034 | loss: 0.65946 | error: 0.37500\n",
      "  Ended batch 027 / 034 | loss: 0.58820 | error: 0.33594\n",
      "  Ended batch 028 / 034 | loss: 0.63267 | error: 0.35938\n",
      "  Ended batch 029 / 034 | loss: 0.58334 | error: 0.34375\n",
      "  Ended batch 030 / 034 | loss: 0.58800 | error: 0.35156\n",
      "  Ended batch 031 / 034 | loss: 0.62215 | error: 0.35938\n",
      "  Ended batch 032 / 034 | loss: 0.71224 | error: 0.40625\n",
      "  Ended batch 033 / 034 | loss: 0.69058 | error: 0.35938\n",
      "  Ended batch 034 / 034 | loss: 0.69733 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 071 sec | loss: 0.61752 | error: 0.19913\n",
      "Starting epoch 006 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.65257 | error: 0.35938\n",
      "  Ended batch 002 / 034 | loss: 0.58962 | error: 0.28906\n",
      "  Ended batch 003 / 034 | loss: 0.66122 | error: 0.37500\n",
      "  Ended batch 004 / 034 | loss: 0.64369 | error: 0.36719\n",
      "  Ended batch 005 / 034 | loss: 0.60663 | error: 0.35938\n",
      "  Ended batch 006 / 034 | loss: 0.48763 | error: 0.28125\n",
      "  Ended batch 007 / 034 | loss: 0.70846 | error: 0.41406\n",
      "  Ended batch 008 / 034 | loss: 0.54418 | error: 0.31250\n",
      "  Ended batch 009 / 034 | loss: 0.64243 | error: 0.35156\n",
      "  Ended batch 010 / 034 | loss: 0.63022 | error: 0.34375\n",
      "  Ended batch 011 / 034 | loss: 0.45937 | error: 0.22656\n",
      "  Ended batch 012 / 034 | loss: 0.69249 | error: 0.41406\n",
      "  Ended batch 013 / 034 | loss: 0.59089 | error: 0.29688\n",
      "  Ended batch 014 / 034 | loss: 0.67305 | error: 0.33594\n",
      "  Ended batch 015 / 034 | loss: 0.63086 | error: 0.35156\n",
      "  Ended batch 016 / 034 | loss: 0.59477 | error: 0.32031\n",
      "  Ended batch 017 / 034 | loss: 0.64904 | error: 0.35938\n",
      "  Ended batch 018 / 034 | loss: 0.61504 | error: 0.34375\n",
      "  Ended batch 019 / 034 | loss: 0.56146 | error: 0.28125\n",
      "  Ended batch 020 / 034 | loss: 0.66876 | error: 0.35156\n",
      "  Ended batch 021 / 034 | loss: 0.64951 | error: 0.35156\n",
      "  Ended batch 022 / 034 | loss: 0.62516 | error: 0.32812\n",
      "  Ended batch 023 / 034 | loss: 0.60224 | error: 0.33594\n",
      "  Ended batch 024 / 034 | loss: 0.68247 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.66238 | error: 0.38281\n",
      "  Ended batch 026 / 034 | loss: 0.66804 | error: 0.38281\n",
      "  Ended batch 027 / 034 | loss: 0.58471 | error: 0.31250\n",
      "  Ended batch 028 / 034 | loss: 0.63838 | error: 0.34375\n",
      "  Ended batch 029 / 034 | loss: 0.58179 | error: 0.31250\n",
      "  Ended batch 030 / 034 | loss: 0.53967 | error: 0.28125\n",
      "  Ended batch 031 / 034 | loss: 0.60681 | error: 0.34375\n",
      "  Ended batch 032 / 034 | loss: 0.70840 | error: 0.39844\n",
      "  Ended batch 033 / 034 | loss: 0.69805 | error: 0.35938\n",
      "  Ended batch 034 / 034 | loss: 0.71011 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 006 / 016 | time: 072 sec | loss: 0.61598 | error: 0.19978\n",
      "Starting epoch 007 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.68487 | error: 0.38281\n",
      "  Ended batch 002 / 034 | loss: 0.60196 | error: 0.32812\n",
      "  Ended batch 003 / 034 | loss: 0.65054 | error: 0.35938\n",
      "  Ended batch 004 / 034 | loss: 0.62629 | error: 0.33594\n",
      "  Ended batch 005 / 034 | loss: 0.55017 | error: 0.27344\n",
      "  Ended batch 006 / 034 | loss: 0.45173 | error: 0.23438\n",
      "  Ended batch 007 / 034 | loss: 0.69026 | error: 0.39844\n",
      "  Ended batch 008 / 034 | loss: 0.54993 | error: 0.31250\n",
      "  Ended batch 009 / 034 | loss: 0.65324 | error: 0.35938\n",
      "  Ended batch 010 / 034 | loss: 0.62073 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.43117 | error: 0.22656\n",
      "  Ended batch 012 / 034 | loss: 0.67033 | error: 0.38281\n",
      "  Ended batch 013 / 034 | loss: 0.59108 | error: 0.29688\n",
      "  Ended batch 014 / 034 | loss: 0.67516 | error: 0.33594\n",
      "  Ended batch 015 / 034 | loss: 0.59983 | error: 0.33594\n",
      "  Ended batch 016 / 034 | loss: 0.58720 | error: 0.32031\n",
      "  Ended batch 017 / 034 | loss: 0.64632 | error: 0.35938\n",
      "  Ended batch 018 / 034 | loss: 0.61104 | error: 0.34375\n",
      "  Ended batch 019 / 034 | loss: 0.55067 | error: 0.28906\n",
      "  Ended batch 020 / 034 | loss: 0.69261 | error: 0.36719\n",
      "  Ended batch 021 / 034 | loss: 0.64227 | error: 0.36719\n",
      "  Ended batch 022 / 034 | loss: 0.60933 | error: 0.32812\n",
      "  Ended batch 023 / 034 | loss: 0.62298 | error: 0.35156\n",
      "  Ended batch 024 / 034 | loss: 0.67723 | error: 0.39062\n",
      "  Ended batch 025 / 034 | loss: 0.65511 | error: 0.37500\n",
      "  Ended batch 026 / 034 | loss: 0.66900 | error: 0.38281\n",
      "  Ended batch 027 / 034 | loss: 0.57921 | error: 0.32031\n",
      "  Ended batch 028 / 034 | loss: 0.65154 | error: 0.38281\n",
      "  Ended batch 029 / 034 | loss: 0.58363 | error: 0.34375\n",
      "  Ended batch 030 / 034 | loss: 0.55882 | error: 0.32812\n",
      "  Ended batch 031 / 034 | loss: 0.60805 | error: 0.34375\n",
      "  Ended batch 032 / 034 | loss: 0.70357 | error: 0.39844\n",
      "  Ended batch 033 / 034 | loss: 0.70473 | error: 0.36719\n",
      "  Ended batch 034 / 034 | loss: 0.68716 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 007 / 016 | time: 107 sec | loss: 0.64745 | error: 0.21280\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 9.04183711608251 minutes (542.5102269649506 seconds).\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.8_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_real_0.8_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/strategic_approx_model.pt.\n",
      "---------- Calculating results ----------\n",
      "Evaluating using epsilon: 0.8\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0903, -0.2800, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "         -0.0746,  0.0482, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919,  0.1656,\n",
      "          0.3650,  0.3795, -0.0873, -0.1903, -0.0747, -0.0523, -0.0763],\n",
      "        [-0.0710, -0.0306, -0.0791, -0.0876, -0.0431, -0.0708, -0.0919, -0.1246,\n",
      "          0.0652, -0.1662, -0.0495,  0.3771, -0.0304, -0.0473, -0.0763]])\n",
      "Evaluating using epsilon: 0.8\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0916, -0.2730, -0.0805, -0.0891, -0.0430, -0.0718, -0.0928, -0.1225,\n",
      "         -0.0726,  0.0461, -0.0898, -0.1911, -0.0753, -0.0527, -0.0778],\n",
      "        [-0.0745, -0.0119, -0.0830, -0.0916, -0.0428, -0.0736, -0.0944,  0.1713,\n",
      "          0.3704,  0.3740, -0.0938, -0.1923, -0.0762, -0.0533, -0.0802],\n",
      "        [-0.0726, -0.0220, -0.0809, -0.0895, -0.0430, -0.0721, -0.0930, -0.1220,\n",
      "          0.0677, -0.1687, -0.0525,  0.3761, -0.0311, -0.0478, -0.0781]])\n",
      "Evaluating using epsilon: 0.8\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.0944, -0.2659, -0.0813, -0.0904, -0.0435, -0.0732, -0.0938, -0.1215,\n",
      "         -0.0720,  0.0453, -0.0929, -0.1904, -0.0757, -0.0516, -0.0790],\n",
      "        [-0.2391,  0.5487, -0.1722, -0.2029, -0.0568, -0.1705, -0.1728,  0.2918,\n",
      "          0.4718,  0.2641, -0.3154, -0.1966, -0.1131, -0.0217, -0.1859],\n",
      "        [-0.0872,  0.0253, -0.0881, -0.0987, -0.0444, -0.0804, -0.0997, -0.1124,\n",
      "          0.0755, -0.1773, -0.0716,  0.3765, -0.0341, -0.0444, -0.0869]])\n",
      "Evaluating using epsilon: 0.8\n",
      "Evaluating on X_opt=\n",
      "tensor([[-0.1270,  0.0698, -0.2355, -0.2366, -0.2258, -0.1863, -0.1448, -0.1568,\n",
      "         -0.0952, -0.0167, -0.2774, -0.1906, -0.0650,  0.0814, -0.1276],\n",
      "        [-0.0951,  0.2711, -0.2040, -0.2063, -0.1902, -0.1613, -0.1296,  0.1316,\n",
      "          0.3408,  0.3241, -0.2544, -0.1912, -0.0672,  0.0535, -0.1202],\n",
      "        [-0.0913,  0.2567, -0.1946, -0.1972, -0.1796, -0.1538, -0.1250, -0.1591,\n",
      "          0.0399, -0.2188, -0.2097,  0.3760, -0.0236,  0.0502, -0.1180]])\n",
      "------------------------- 0.95 -------------------------\n",
      "---------- Training non-strategically with epsilon=0.95 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98923 | error: 0.43750\n",
      "  Ended batch 002 / 034 | loss: 0.83193 | error: 0.39844\n",
      "  Ended batch 003 / 034 | loss: 0.75601 | error: 0.38281\n",
      "  Ended batch 004 / 034 | loss: 0.62267 | error: 0.32031\n",
      "  Ended batch 005 / 034 | loss: 0.53573 | error: 0.26562\n",
      "  Ended batch 006 / 034 | loss: 0.43920 | error: 0.17188\n",
      "  Ended batch 007 / 034 | loss: 0.55319 | error: 0.24219\n",
      "  Ended batch 008 / 034 | loss: 0.43821 | error: 0.17188\n",
      "  Ended batch 009 / 034 | loss: 0.48477 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.44141 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.34122 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.39537 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.46204 | error: 0.17969\n",
      "  Ended batch 014 / 034 | loss: 0.61706 | error: 0.21094\n",
      "  Ended batch 015 / 034 | loss: 0.40735 | error: 0.15625\n",
      "  Ended batch 016 / 034 | loss: 0.44818 | error: 0.19531\n",
      "  Ended batch 017 / 034 | loss: 0.42789 | error: 0.17969\n",
      "  Ended batch 018 / 034 | loss: 0.45081 | error: 0.14062\n",
      "  Ended batch 019 / 034 | loss: 0.40077 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.67522 | error: 0.20312\n",
      "  Ended batch 021 / 034 | loss: 0.44442 | error: 0.17188\n",
      "  Ended batch 022 / 034 | loss: 0.49886 | error: 0.20312\n",
      "  Ended batch 023 / 034 | loss: 0.43399 | error: 0.16406\n",
      "  Ended batch 024 / 034 | loss: 0.42774 | error: 0.17188\n",
      "  Ended batch 025 / 034 | loss: 0.49798 | error: 0.17969\n",
      "  Ended batch 026 / 034 | loss: 0.48518 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34755 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.36209 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30258 | error: 0.14062\n",
      "  Ended batch 030 / 034 | loss: 0.27834 | error: 0.09375\n",
      "  Ended batch 031 / 034 | loss: 0.31573 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.46953 | error: 0.21094\n",
      "  Ended batch 033 / 034 | loss: 0.44510 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.35380 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 000 sec | loss: 0.45466 | error: 0.21103\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/non_strategic_0.95_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.40293 | error: 0.16406\n",
      "  Ended batch 002 / 034 | loss: 0.39698 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39748 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.33817 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31394 | error: 0.13281\n",
      "  Ended batch 006 / 034 | loss: 0.26311 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.40746 | error: 0.21094\n",
      "  Ended batch 008 / 034 | loss: 0.28619 | error: 0.12500\n",
      "  Ended batch 009 / 034 | loss: 0.42815 | error: 0.21875\n",
      "  Ended batch 010 / 034 | loss: 0.37308 | error: 0.14062\n",
      "  Ended batch 011 / 034 | loss: 0.24522 | error: 0.10938\n",
      "  Ended batch 012 / 034 | loss: 0.35200 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.38305 | error: 0.16406\n",
      "  Ended batch 014 / 034 | loss: 0.48793 | error: 0.18750\n",
      "  Ended batch 015 / 034 | loss: 0.29911 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37009 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39094 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.39047 | error: 0.16406\n",
      "  Ended batch 019 / 034 | loss: 0.33128 | error: 0.14844\n",
      "  Ended batch 020 / 034 | loss: 0.62716 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.39453 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.42599 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.40997 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38743 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.49943 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.46154 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34475 | error: 0.14844\n",
      "  Ended batch 028 / 034 | loss: 0.35799 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30337 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29895 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31889 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44511 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40606 | error: 0.16406\n",
      "  Ended batch 034 / 034 | loss: 0.41604 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 000 sec | loss: 0.44767 | error: 0.21363\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37960 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.38054 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39798 | error: 0.17188\n",
      "  Ended batch 004 / 034 | loss: 0.32809 | error: 0.15625\n",
      "  Ended batch 005 / 034 | loss: 0.31529 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26527 | error: 0.10938\n",
      "  Ended batch 007 / 034 | loss: 0.39605 | error: 0.20312\n",
      "  Ended batch 008 / 034 | loss: 0.29298 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42259 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.37166 | error: 0.12500\n",
      "  Ended batch 011 / 034 | loss: 0.24852 | error: 0.11719\n",
      "  Ended batch 012 / 034 | loss: 0.34251 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37322 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47350 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31297 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.36932 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.39082 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38728 | error: 0.15625\n",
      "  Ended batch 019 / 034 | loss: 0.33095 | error: 0.13281\n",
      "  Ended batch 020 / 034 | loss: 0.63589 | error: 0.24219\n",
      "  Ended batch 021 / 034 | loss: 0.39093 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41775 | error: 0.18750\n",
      "  Ended batch 023 / 034 | loss: 0.40417 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.38116 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48604 | error: 0.21875\n",
      "  Ended batch 026 / 034 | loss: 0.45087 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34605 | error: 0.13281\n",
      "  Ended batch 028 / 034 | loss: 0.36658 | error: 0.14844\n",
      "  Ended batch 029 / 034 | loss: 0.30254 | error: 0.10938\n",
      "  Ended batch 030 / 034 | loss: 0.30504 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31964 | error: 0.12500\n",
      "  Ended batch 032 / 034 | loss: 0.44560 | error: 0.20312\n",
      "  Ended batch 033 / 034 | loss: 0.40693 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.43542 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 000 sec | loss: 0.44874 | error: 0.21168\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37193 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37114 | error: 0.17188\n",
      "  Ended batch 003 / 034 | loss: 0.39974 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31597 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31782 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26514 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.39098 | error: 0.18750\n",
      "  Ended batch 008 / 034 | loss: 0.28986 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42376 | error: 0.19531\n",
      "  Ended batch 010 / 034 | loss: 0.36756 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24499 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34379 | error: 0.15625\n",
      "  Ended batch 013 / 034 | loss: 0.37043 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47858 | error: 0.20312\n",
      "  Ended batch 015 / 034 | loss: 0.30862 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37446 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38862 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38593 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32783 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64187 | error: 0.25000\n",
      "  Ended batch 021 / 034 | loss: 0.38988 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41519 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39861 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37473 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48543 | error: 0.21094\n",
      "  Ended batch 026 / 034 | loss: 0.44854 | error: 0.21094\n",
      "  Ended batch 027 / 034 | loss: 0.34043 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36239 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.29949 | error: 0.11719\n",
      "  Ended batch 030 / 034 | loss: 0.29681 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31391 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45120 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40857 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42124 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 004 / 016 | time: 000 sec | loss: 0.44502 | error: 0.21103\n",
      "Starting epoch 005 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.37063 | error: 0.17969\n",
      "  Ended batch 002 / 034 | loss: 0.37067 | error: 0.16406\n",
      "  Ended batch 003 / 034 | loss: 0.39355 | error: 0.16406\n",
      "  Ended batch 004 / 034 | loss: 0.31063 | error: 0.13281\n",
      "  Ended batch 005 / 034 | loss: 0.31461 | error: 0.14062\n",
      "  Ended batch 006 / 034 | loss: 0.26898 | error: 0.11719\n",
      "  Ended batch 007 / 034 | loss: 0.38740 | error: 0.19531\n",
      "  Ended batch 008 / 034 | loss: 0.28760 | error: 0.10938\n",
      "  Ended batch 009 / 034 | loss: 0.42075 | error: 0.20312\n",
      "  Ended batch 010 / 034 | loss: 0.36434 | error: 0.11719\n",
      "  Ended batch 011 / 034 | loss: 0.24144 | error: 0.10156\n",
      "  Ended batch 012 / 034 | loss: 0.34192 | error: 0.14844\n",
      "  Ended batch 013 / 034 | loss: 0.36529 | error: 0.15625\n",
      "  Ended batch 014 / 034 | loss: 0.47503 | error: 0.19531\n",
      "  Ended batch 015 / 034 | loss: 0.31060 | error: 0.10938\n",
      "  Ended batch 016 / 034 | loss: 0.37830 | error: 0.15625\n",
      "  Ended batch 017 / 034 | loss: 0.38680 | error: 0.17188\n",
      "  Ended batch 018 / 034 | loss: 0.38255 | error: 0.14844\n",
      "  Ended batch 019 / 034 | loss: 0.32599 | error: 0.14062\n",
      "  Ended batch 020 / 034 | loss: 0.64071 | error: 0.25781\n",
      "  Ended batch 021 / 034 | loss: 0.38804 | error: 0.14844\n",
      "  Ended batch 022 / 034 | loss: 0.41321 | error: 0.19531\n",
      "  Ended batch 023 / 034 | loss: 0.39709 | error: 0.17188\n",
      "  Ended batch 024 / 034 | loss: 0.37160 | error: 0.18750\n",
      "  Ended batch 025 / 034 | loss: 0.48514 | error: 0.20312\n",
      "  Ended batch 026 / 034 | loss: 0.44696 | error: 0.21875\n",
      "  Ended batch 027 / 034 | loss: 0.34037 | error: 0.14062\n",
      "  Ended batch 028 / 034 | loss: 0.36098 | error: 0.15625\n",
      "  Ended batch 029 / 034 | loss: 0.30023 | error: 0.12500\n",
      "  Ended batch 030 / 034 | loss: 0.29524 | error: 0.10938\n",
      "  Ended batch 031 / 034 | loss: 0.31327 | error: 0.11719\n",
      "  Ended batch 032 / 034 | loss: 0.45363 | error: 0.18750\n",
      "  Ended batch 033 / 034 | loss: 0.40813 | error: 0.15625\n",
      "  Ended batch 034 / 034 | loss: 0.42413 | error: 0.22727\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 005 / 016 | time: 000 sec | loss: 0.44367 | error: 0.21168\n",
      "Ending training due to 4 consecutive epochs without improvement in validation accuracy.\n",
      "Total training time: 0.752974271774292 seconds.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.95_model.pt.\n",
      "Model loaded from ./Results/vanilla_vs_hardt_orig/non_strategic_0.95_model.pt.\n",
      "---------- Training strategically with epsilon=0.95 ----------\n",
      "Starting epoch 001 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.98297 | error: 0.44531\n",
      "  Ended batch 002 / 034 | loss: 0.84786 | error: 0.45312\n",
      "  Ended batch 003 / 034 | loss: 0.94935 | error: 0.47656\n",
      "  Ended batch 004 / 034 | loss: 0.87280 | error: 0.45312\n",
      "  Ended batch 005 / 034 | loss: 0.80891 | error: 0.45312\n",
      "  Ended batch 006 / 034 | loss: 0.66434 | error: 0.35938\n",
      "  Ended batch 007 / 034 | loss: 0.84484 | error: 0.49219\n",
      "  Ended batch 008 / 034 | loss: 0.71054 | error: 0.25781\n",
      "  Ended batch 009 / 034 | loss: 0.75958 | error: 0.28125\n",
      "  Ended batch 010 / 034 | loss: 0.70638 | error: 0.19531\n",
      "  Ended batch 011 / 034 | loss: 0.60139 | error: 0.14844\n",
      "  Ended batch 012 / 034 | loss: 0.70670 | error: 0.27344\n",
      "  Ended batch 013 / 034 | loss: 0.63157 | error: 0.23438\n",
      "  Ended batch 014 / 034 | loss: 0.66079 | error: 0.28125\n",
      "  Ended batch 015 / 034 | loss: 0.62992 | error: 0.33594\n",
      "  Ended batch 016 / 034 | loss: 0.59443 | error: 0.32812\n",
      "  Ended batch 017 / 034 | loss: 0.63984 | error: 0.33594\n",
      "  Ended batch 018 / 034 | loss: 0.60748 | error: 0.32812\n",
      "  Ended batch 019 / 034 | loss: 0.51712 | error: 0.25781\n",
      "  Ended batch 020 / 034 | loss: 0.71388 | error: 0.35156\n",
      "  Ended batch 021 / 034 | loss: 0.55061 | error: 0.25781\n",
      "  Ended batch 022 / 034 | loss: 0.62013 | error: 0.28125\n",
      "  Ended batch 023 / 034 | loss: 0.53293 | error: 0.25000\n",
      "  Ended batch 024 / 034 | loss: 0.61323 | error: 0.29688\n",
      "  Ended batch 025 / 034 | loss: 0.61985 | error: 0.30469\n",
      "  Ended batch 026 / 034 | loss: 0.59069 | error: 0.28906\n",
      "  Ended batch 027 / 034 | loss: 0.49761 | error: 0.25781\n",
      "  Ended batch 028 / 034 | loss: 0.53919 | error: 0.26562\n",
      "  Ended batch 029 / 034 | loss: 0.43641 | error: 0.22656\n",
      "  Ended batch 030 / 034 | loss: 0.40639 | error: 0.21094\n",
      "  Ended batch 031 / 034 | loss: 0.49385 | error: 0.25781\n",
      "  Ended batch 032 / 034 | loss: 0.67151 | error: 0.34375\n",
      "  Ended batch 033 / 034 | loss: 0.61885 | error: 0.29688\n",
      "  Ended batch 034 / 034 | loss: 0.60651 | error: 0.31818\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 001 / 016 | time: 087 sec | loss: 0.70875 | error: 0.23428\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.95_model.pt.\n",
      "Starting epoch 002 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.62930 | error: 0.32812\n",
      "  Ended batch 002 / 034 | loss: 0.42678 | error: 0.20312\n",
      "  Ended batch 003 / 034 | loss: 0.57584 | error: 0.30469\n",
      "  Ended batch 004 / 034 | loss: 0.56618 | error: 0.29688\n",
      "  Ended batch 005 / 034 | loss: 0.44346 | error: 0.22656\n",
      "  Ended batch 006 / 034 | loss: 0.45075 | error: 0.25000\n",
      "  Ended batch 007 / 034 | loss: 0.70801 | error: 0.39062\n",
      "  Ended batch 008 / 034 | loss: 0.46521 | error: 0.25000\n",
      "  Ended batch 009 / 034 | loss: 0.58724 | error: 0.31250\n",
      "  Ended batch 010 / 034 | loss: 0.45408 | error: 0.21094\n",
      "  Ended batch 011 / 034 | loss: 0.37707 | error: 0.18750\n",
      "  Ended batch 012 / 034 | loss: 0.54296 | error: 0.28906\n",
      "  Ended batch 013 / 034 | loss: 0.49210 | error: 0.23438\n",
      "  Ended batch 014 / 034 | loss: 0.61135 | error: 0.29688\n",
      "  Ended batch 015 / 034 | loss: 0.56004 | error: 0.30469\n",
      "  Ended batch 016 / 034 | loss: 0.49962 | error: 0.25781\n",
      "  Ended batch 017 / 034 | loss: 0.54776 | error: 0.28906\n",
      "  Ended batch 018 / 034 | loss: 0.51922 | error: 0.25781\n",
      "  Ended batch 019 / 034 | loss: 0.47873 | error: 0.23438\n",
      "  Ended batch 020 / 034 | loss: 0.70085 | error: 0.35156\n",
      "  Ended batch 021 / 034 | loss: 0.57338 | error: 0.29688\n",
      "  Ended batch 022 / 034 | loss: 0.50506 | error: 0.25000\n",
      "  Ended batch 023 / 034 | loss: 0.51937 | error: 0.27344\n",
      "  Ended batch 024 / 034 | loss: 0.62247 | error: 0.33594\n",
      "  Ended batch 025 / 034 | loss: 0.65532 | error: 0.35156\n",
      "  Ended batch 026 / 034 | loss: 0.62829 | error: 0.34375\n",
      "  Ended batch 027 / 034 | loss: 0.57501 | error: 0.32812\n",
      "  Ended batch 028 / 034 | loss: 0.60709 | error: 0.33594\n",
      "  Ended batch 029 / 034 | loss: 0.53559 | error: 0.29688\n",
      "  Ended batch 030 / 034 | loss: 0.45589 | error: 0.23438\n",
      "  Ended batch 031 / 034 | loss: 0.56882 | error: 0.32031\n",
      "  Ended batch 032 / 034 | loss: 0.67846 | error: 0.37500\n",
      "  Ended batch 033 / 034 | loss: 0.66544 | error: 0.34375\n",
      "  Ended batch 034 / 034 | loss: 0.72356 | error: 0.40909\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 002 / 016 | time: 090 sec | loss: 0.60726 | error: 0.19522\n",
      "Validation accuracy improved.\n",
      "Model saved to ./Results/vanilla_vs_hardt_orig/strategic_real_0.95_model.pt.\n",
      "Starting epoch 003 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.64749 | error: 0.36719\n",
      "  Ended batch 002 / 034 | loss: 0.57041 | error: 0.31250\n",
      "  Ended batch 003 / 034 | loss: 0.68019 | error: 0.39062\n",
      "  Ended batch 004 / 034 | loss: 0.62979 | error: 0.35938\n",
      "  Ended batch 005 / 034 | loss: 0.54077 | error: 0.28125\n",
      "  Ended batch 006 / 034 | loss: 0.44313 | error: 0.24219\n",
      "  Ended batch 007 / 034 | loss: 0.64972 | error: 0.38281\n",
      "  Ended batch 008 / 034 | loss: 0.51896 | error: 0.29688\n",
      "  Ended batch 009 / 034 | loss: 0.65632 | error: 0.36719\n",
      "  Ended batch 010 / 034 | loss: 0.61105 | error: 0.33594\n",
      "  Ended batch 011 / 034 | loss: 0.44448 | error: 0.24219\n",
      "  Ended batch 012 / 034 | loss: 0.69368 | error: 0.41406\n",
      "  Ended batch 013 / 034 | loss: 0.58084 | error: 0.30469\n",
      "  Ended batch 014 / 034 | loss: 0.61324 | error: 0.30469\n",
      "  Ended batch 015 / 034 | loss: 0.60116 | error: 0.34375\n",
      "  Ended batch 016 / 034 | loss: 0.58006 | error: 0.32031\n",
      "  Ended batch 017 / 034 | loss: 0.60425 | error: 0.34375\n",
      "  Ended batch 018 / 034 | loss: 0.63003 | error: 0.35156\n",
      "  Ended batch 019 / 034 | loss: 0.53854 | error: 0.28125\n",
      "  Ended batch 020 / 034 | loss: 0.66492 | error: 0.35156\n",
      "  Ended batch 021 / 034 | loss: 0.64100 | error: 0.35938\n",
      "  Ended batch 022 / 034 | loss: 0.60344 | error: 0.31250\n",
      "  Ended batch 023 / 034 | loss: 0.57316 | error: 0.32031\n",
      "  Ended batch 024 / 034 | loss: 0.66590 | error: 0.38281\n",
      "  Ended batch 025 / 034 | loss: 0.63260 | error: 0.35938\n",
      "  Ended batch 026 / 034 | loss: 0.66797 | error: 0.35938\n",
      "  Ended batch 027 / 034 | loss: 0.55874 | error: 0.30469\n",
      "  Ended batch 028 / 034 | loss: 0.60526 | error: 0.35156\n",
      "  Ended batch 029 / 034 | loss: 0.56331 | error: 0.32812\n",
      "  Ended batch 030 / 034 | loss: 0.51468 | error: 0.29688\n",
      "  Ended batch 031 / 034 | loss: 0.59308 | error: 0.34375\n",
      "  Ended batch 032 / 034 | loss: 0.68139 | error: 0.38281\n",
      "  Ended batch 033 / 034 | loss: 0.67845 | error: 0.33594\n",
      "  Ended batch 034 / 034 | loss: 0.68719 | error: 0.45455\n",
      "  Finished training step, calculating validation loss and accuracy.\n",
      "Ended epoch 003 / 016 | time: 074 sec | loss: 0.61881 | error: 0.20173\n",
      "Starting epoch 004 / 016.\n",
      "  Ended batch 001 / 034 | loss: 0.63939 | error: 0.29688\n",
      "  Ended batch 002 / 034 | loss: 0.55800 | error: 0.25000\n",
      "  Ended batch 003 / 034 | loss: 0.64994 | error: 0.36719\n",
      "  Ended batch 004 / 034 | loss: 0.62760 | error: 0.35156\n",
      "  Ended batch 005 / 034 | loss: 0.54708 | error: 0.31250\n",
      "  Ended batch 006 / 034 | loss: 0.46545 | error: 0.26562\n",
      "  Ended batch 007 / 034 | loss: 0.75394 | error: 0.44531\n",
      "  Ended batch 008 / 034 | loss: 0.53681 | error: 0.31250\n",
      "  Ended batch 009 / 034 | loss: 0.63653 | error: 0.35156\n",
      "  Ended batch 010 / 034 | loss: 0.59438 | error: 0.30469\n",
      "  Ended batch 011 / 034 | loss: 0.44171 | error: 0.20312\n",
      "  Ended batch 012 / 034 | loss: 0.63697 | error: 0.36719\n",
      "  Ended batch 013 / 034 | loss: 0.56370 | error: 0.27344\n",
      "  Ended batch 014 / 034 | loss: 0.64550 | error: 0.31250\n",
      "  Ended batch 015 / 034 | loss: 0.62093 | error: 0.35156\n",
      "  Ended batch 016 / 034 | loss: 0.60143 | error: 0.32812\n",
      "  Ended batch 017 / 034 | loss: 0.64750 | error: 0.35938\n",
      "  Ended batch 023 / 034 | loss: 0.58248 | error: 0.32031\n",
      "  Ended batch 024 / 034 | loss: 0.64708 | error: 0.36719\n",
      "  Ended batch 025 / 034 | loss: 0.68748 | error: 0.39062\n",
      "  Ended batch 026 / 034 | loss: 0.65599 | error: 0.37500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-bb6bc277937d>\", line 40, in <module>\n",
      "    strategic_model_real.fit(X, Y, Xval, Yval, opt_class=torch.optim.Adam, opt_kwargs={\"lr\": 2e-1}, epochs=epochs, verbose=\"batches\", path=PATH, model_name=model_name)\n",
      "  File \"C:\\Users\\mmakhlevich\\ModularStrategicClassification-develop\\lib\\StrategicModel.py\", line 357, in fit\n",
      "    Xbatch_opt, Ybatch_pred = self.forward(Xbatch, requires_grad=True)\n",
      "  File \"C:\\Users\\mmakhlevich\\ModularStrategicClassification-develop\\lib\\StrategicModel.py\", line 200, in forward\n",
      "    X_opt = self.optimize_X(X, requires_grad=requires_grad)\n",
      "  File \"C:\\Users\\mmakhlevich\\ModularStrategicClassification-develop\\lib\\StrategicModel.py\", line 238, in optimize_X\n",
      "    requires_grad=requires_grad)\n",
      "  File \"C:\\Users\\mmakhlevich\\ModularStrategicClassification-develop\\lib\\ResponseMapping.py\", line 184, in optimize_X\n",
      "    for i in range(len(X_batches))]\n",
      "  File \"C:\\Users\\mmakhlevich\\ModularStrategicClassification-develop\\lib\\ResponseMapping.py\", line 184, in <listcomp>\n",
      "    for i in range(len(X_batches))]\n",
      "  File \"C:\\Users\\mmakhlevich\\ModularStrategicClassification-develop\\lib\\ResponseMapping.py\", line 155, in solve_differential_optimization_problem\n",
      "    return self.CCP_layer(X, w, b, nonlinear_score_batched, f_der)[0]\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\cvxpylayers\\torch\\cvxpylayer.py\", line 152, in forward\n",
      "    sol = f(*params)\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\cvxpylayers\\torch\\cvxpylayer.py\", line 276, in forward\n",
      "    keep_zeros=True)\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\cvxpy\\reductions\\dcp2cone\\cone_matrix_stuffing.py\", line 176, in apply_parameters\n",
      "    self.c, param_vec, self.x.size, with_offset=True)\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\cvxpy\\cvxcore\\python\\canonInterface.py\", line 233, in get_matrix_from_tensor\n",
      "    param_vec = scipy.sparse.csc_matrix(param_vec[:, None])\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\scipy\\sparse\\compressed.py\", line 85, in __init__\n",
      "    self._set_self(self.__class__(coo_matrix(arg1, dtype=dtype)))\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\scipy\\sparse\\coo.py\", line 190, in __init__\n",
      "    self.data = M[self.row, self.col]\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\inspect.py\", line 1464, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 182, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\torch\\_fx\\graph_module.py\", line 27, in patched_getline\n",
      "    return _orig_getlines(*args, **kwargs)\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\tokenize.py\", line 449, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\tokenize.py\", line 418, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"C:\\Users\\mmakhlevich\\Anaconda3\\envs\\SCMP_setup\\lib\\tokenize.py\", line 376, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-bb6bc277937d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mstrategic_model_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStrategicModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"linear\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_const_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"v\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"epsilon\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"scale\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.35\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_slope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrategic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mstrategic_model_real\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"lr\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m2e-1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"batches\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ModularStrategicClassification-develop\\lib\\StrategicModel.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, Xval, Yval, opt_class, opt_kwargs, shuffle, epochs, epochs_without_improvement_cap, verbose, save_progress, path, model_name)\u001b[0m\n\u001b[0;32m    356\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m                 \u001b[0mXbatch_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYbatch_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYbatch_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXbatch_opt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ModularStrategicClassification-develop\\lib\\StrategicModel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, requires_grad)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0minit_ccp_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mX_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mccp_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0minit_ccp_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ModularStrategicClassification-develop\\lib\\StrategicModel.py\u001b[0m in \u001b[0;36moptimize_X\u001b[1;34m(self, X, requires_grad)\u001b[0m\n\u001b[0;32m    237\u001b[0m         X_opt_linear = self.response_mapping.optimize_X(X[:, self.linear_indices], self.w, self.b, nonlinear_score,\n\u001b[1;32m--> 238\u001b[1;33m                                                         requires_grad=requires_grad)\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ModularStrategicClassification-develop\\lib\\ResponseMapping.py\u001b[0m in \u001b[0;36moptimize_X\u001b[1;34m(self, X, w, b, nonlinear_score, requires_grad, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m                                                                           X_opt_batches[i], **kwargs)\n\u001b[1;32m--> 184\u001b[1;33m                              for i in range(len(X_batches))]\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ModularStrategicClassification-develop\\lib\\ResponseMapping.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    183\u001b[0m                                                                           X_opt_batches[i], **kwargs)\n\u001b[1;32m--> 184\u001b[1;33m                              for i in range(len(X_batches))]\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ModularStrategicClassification-develop\\lib\\ResponseMapping.py\u001b[0m in \u001b[0;36msolve_differential_optimization_problem\u001b[1;34m(self, X, w, b, nonlinear_score, X_opt, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[0mnonlinear_score_batched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnonlinear_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCCP_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonlinear_score_batched\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_der\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\cvxpylayers\\torch\\cvxpylayer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, solver_args, *params)\u001b[0m\n\u001b[0;32m    151\u001b[0m         )\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0msol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\cvxpylayers\\torch\\cvxpylayer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, *params)\u001b[0m\n\u001b[0;32m    275\u001b[0m                     \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_numpy_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m                     keep_zeros=True)\n\u001b[0m\u001b[0;32m    277\u001b[0m                 \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mneg_A\u001b[0m  \u001b[1;31m# cvxpy canonicalizes -A\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\cvxpy\\reductions\\dcp2cone\\cone_matrix_stuffing.py\u001b[0m in \u001b[0;36mapply_parameters\u001b[1;34m(self, id_to_param_value, zero_offset, keep_zeros)\u001b[0m\n\u001b[0;32m    175\u001b[0m         c, d = canonInterface.get_matrix_from_tensor(\n\u001b[1;32m--> 176\u001b[1;33m             self.c, param_vec, self.x.size, with_offset=True)\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\cvxpy\\cvxcore\\python\\canonInterface.py\u001b[0m in \u001b[0;36mget_matrix_from_tensor\u001b[1;34m(problem_data_tensor, param_vec, var_length, nonzero_rows, with_offset, problem_data_index)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[0mparam_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m         \u001b[0mflat_problem_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproblem_data_tensor\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mparam_vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_canonical_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2043\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 2047\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   2048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1436\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1334\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1336\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m             )\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m-> 1193\u001b[1;33m                                                                tb_offset)\n\u001b[0m\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[1;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\SCMP_setup\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"epsilons\": [],\n",
    "    \"benchmark\": [],\n",
    "    \"SERM\": [],\n",
    "    \"blind\": [],\n",
    "    \"Hardt\": []\n",
    "}\n",
    "\n",
    "print(f\"---------- Training Hardt et al's model (strategic with eps={small_eps}) ----------\")\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "model_name = \"strategic_approx\"\n",
    "strategic_model_approx = StrategicModel(x_dim, batch_size, cost_fn=\"linear\", cost_const_kwargs={\"v\": v, \"epsilon\": small_eps, \"scale\":0.35}, eval_slope = 4, strategic=True)\n",
    "strategic_model_approx.fit(X, Y, Xval, Yval, opt_class=torch.optim.Adam, opt_kwargs={\"lr\": 2e-1}, epochs=epochs, verbose=\"batches\", path=PATH, model_name=model_name)\n",
    "\n",
    "for eps in epsilons:\n",
    "    print(f\"------------------------- {eps} -------------------------\")\n",
    "    \n",
    "    # Non-strategic classification\n",
    "    print(f\"---------- Training non-strategically with epsilon={eps} ----------\")\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    model_name = f\"non_strategic_{eps}\"\n",
    "    non_strategic_model = StrategicModel(x_dim, batch_size, cost_fn=\"linear\", cost_const_kwargs={\"v\": v, \"epsilon\": eps, \"scale\":0.35}, eval_slope = 4, strategic=False)\n",
    "    non_strategic_model.fit(X, Y, Xval, Yval, opt_class=torch.optim.Adam, opt_kwargs={\"lr\": 2e-1}, epochs=epochs, verbose=\"batches\", path=PATH, model_name=model_name)\n",
    "    \n",
    "    non_strategic_model = StrategicModel(x_dim, batch_size, cost_fn=\"linear\", cost_const_kwargs={\"v\": v, \"epsilon\": eps, \"scale\":0.35}, eval_slope = 4, strategic=False)\n",
    "    non_strategic_model.load_model(PATH, model_name)\n",
    "    non_strategic_model.normalize_parameters()\n",
    "    \n",
    "    # Strategic classification\n",
    "    print(f\"---------- Training strategically with epsilon={eps} ----------\")\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    model_name = f\"strategic_real_{eps}\"\n",
    "    strategic_model_real = StrategicModel(x_dim, batch_size, cost_fn=\"linear\", cost_const_kwargs={\"v\": v, \"epsilon\": eps, \"scale\":0.35}, eval_slope = 4, strategic=True)\n",
    "    strategic_model_real.fit(X, Y, Xval, Yval, opt_class=torch.optim.Adam, opt_kwargs={\"lr\": 2e-1}, epochs=epochs, verbose=\"batches\", path=PATH, model_name=model_name)\n",
    "    \n",
    "    strategic_model_real = StrategicModel(x_dim, batch_size, cost_fn=\"linear\", cost_const_kwargs={\"v\": v, \"epsilon\": eps, \"scale\":0.35}, eval_slope = 4, strategic=True)\n",
    "    strategic_model_real.load_model(PATH, model_name)\n",
    "    \n",
    "    # Approximate strategic classification (set evaluation epsilon to eps)\n",
    "    model_name = \"strategic_approx\"\n",
    "    strategic_model_approx = StrategicModel(x_dim, batch_size, cost_fn=\"linear\", cost_const_kwargs={\"v\": v, \"epsilon\": eps, \"scale\":0.35}, eval_slope = 4, strategic=True)\n",
    "    strategic_model_approx.load_model(PATH, model_name)\n",
    "    \n",
    "    # Calculate results\n",
    "    print(\"---------- Calculating results ----------\")\n",
    "    results[\"epsilons\"].append(eps)\n",
    "    # Non-strategic model & non-strategic data - Benchmark\n",
    "    results[\"benchmark\"].append(non_strategic_model.evaluate(Xtest, Ytest, strategic_data=False))\n",
    "    # Approx strategic model & strategic data - Hardt et al\n",
    "    results[\"Hardt\"].append(strategic_model_approx.evaluate(Xtest, Ytest, strategic_data=True))\n",
    "    # Real strategic model & strategic data - SERM\n",
    "    results[\"SERM\"].append(strategic_model_real.evaluate(Xtest, Ytest, strategic_data=True))\n",
    "    # Non-strategic model & strategic data - Blind\n",
    "    results[\"blind\"].append(non_strategic_model.evaluate(Xtest, Ytest, strategic_data=True))\n",
    "    pd.DataFrame(results).to_csv(f\"{PATH}/results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Presentation.show_vanilla_vs_hardt_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "results = pd.read_csv(\"Results/vanilla_vs_hardt_orig/results.csv\")\n",
    "epsilons = results[\"epsilons\"].to_list()\n",
    "benchmark = results[\"benchmark\"].to_list()\n",
    "SERM = results[\"SERM\"].to_list()\n",
    "blind = results[\"blind\"].to_list()\n",
    "Hardt = results[\"Hardt\"].to_list()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "_ = fig.suptitle(\"Linear model on spam data\")\n",
    "\n",
    "_ = ax.set_xlabel(\"Mixing parameter\")\n",
    "_ = ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "_ = ax.set_ylabel(\"accuracy\")\n",
    "_ = ax.set_ylim([0.46, 0.84])\n",
    "\n",
    "_ = ax.plot(epsilons, benchmark, label=\"Benchmark\", linestyle=\"--\", color=\"tab:gray\")\n",
    "_ = ax.plot(epsilons, SERM, \"-x\", label=\"SERM\")\n",
    "_ = ax.plot(epsilons, blind, \"-x\", label=\"Blind\")\n",
    "_ = ax.plot(epsilons, Hardt, \"-x\", label=\"Hardt et al. (2016)\")\n",
    "\n",
    "lines, labels = ax.get_legend_handles_labels()    \n",
    "_ = fig.legend(lines, labels, loc=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
